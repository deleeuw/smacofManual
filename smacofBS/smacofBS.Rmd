---
title: |
    | Smacof at 50: A Manual
    | Part 2: Smacof with B-Spline Transformation
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started February 21 2024, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 4
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 4
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: TBD
editor_options: 
  markdown: 
    wrap: 72
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(splines, quietly = TRUE))
suppressPackageStartupMessages(library(splines2, quietly = TRUE))
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(smacofBS, quietly = TRUE))
```

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/smacofCode>.

\sectionbreak


# Smacof Algorithm

## First Phase: Update Configuration

### Introduction to Majorization

Majorization, these days better known as MM (@lange_16), is a general
approach for the construction of minimization algorithms. There is also
minorization, which leads to maximization algorithms, which explains the
MM acronym: minorization for maximization and majorization for
minimization.

Before the MM principle was formulated as a general approach to
algorithm construction there were some important predecessors. Major
classes of MM algorithms avant la lettre were the *EM Algorithm* for
maximum likelihood estimation of @dempster_laird_rubin_77, the *Smacof
Algorithm* for MDS of @deleeuw_C_77, the *Generalized Weiszfeldt Method*
of @vosz_eckhardt_80, and the *Quadratic Approximation Method* of
@boehning_lindsay_88. The first formulation of the general majorization
principle seems to be @deleeuw_C_94c.

Let's start with a brief introduction to majorization. Minimize a real
valued function $\sigma$ over $x\in\mathbb{S}$, where $\mathbb{S}$ is
some subset of $\mathbb{R}^n$. There are obvious extensions of
majorization to functions defined on more general spaces, with values in
any partially ordered set, but we do not need that level of generality
in this manual. Also majorization applied to $\sigma$ is minorization
applied to $-\sigma$, so concentrating on majorization-minimization and
ignoring minorization-maximization causes no loss of generality

Suppose there is a real-valued function $\eta$ on
$\mathbb{S}\otimes\mathbb{S}$ such that

```{=tex}
\begin{align}
\sigma(x)&\leq\eta(x,y)\qquad\forall x,y\in\mathbb{S},(\#eq:maj1)\\
\sigma(x)&=\eta(x,x)\qquad\forall x\in\mathbb{S}.(\#eq:maj2)
\end{align}
```
The function $\eta$ is called a *majorization scheme* for $\sigma$ on
$S$. A majorization scheme is *strict* if $\sigma(x)<\eta(x,y)$ for all
$x,y\in S$ withj $x\not=y$.

Define \begin{equation}
x^{(k+1)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\eta(x,x^{(k)}),
(\#eq:majalg)
\end{equation} assuming that $\eta(\bullet,y)$ attains its (not
necessarily unique) minimum over $x\in\mathbb{S}$ for each $y$. If
$x^{(k)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\eta(x,x^{(k)})$ 
then we stop.

By majorization property \@ref(eq:maj1) 
\begin{equation}
\sigma(x^{(k+1)})\leq\eta(x^{(k+1)},x^{(k)}).
\end{equation}
Because we did not stop update rule \@ref(eq:majalg) implies 
\begin{equation}
\eta(x^{(k+1)},x^{(k)})<\eta(x^{(k)},x^{(k)}).
\end{equation} 
and finally by majorization property \@ref(eq:maj1)
\begin{equation}
\eta(x^{(k)},x^{(k)})=\sigma(x^{(k)}).
\end{equation}

If the minimum in \@ref(eq:majalg) is attained for a unique $x$ then
$\eta(x^{(k+1)},x^{(k)})<\eta(x^{(k)},x^{(k)})$. If the majorization
scheme is strict then $\sigma(x^{(k+1)})<\eta(x^{(k+1)},x^{(k)})$. Under
either of these two additional conditions
$\sigma(x^{(k+1)})<\sigma(x^{(k)})$, which means that the majorization
algorithm is a monotone descent algorithm, and if $\sigma$ is bounded
below on $\mathbb{S}$ the sequence $\sigma(x^{(k)})$ converges.

Note that we only use the order relation to prove convergence of the
sequence of function values. To prove convergence of the $x^{(k)}$ we
need stronger compactness and continuity assumptions to apply the
general theory of @zangwill_69a. For such a proof the argmin in update
formula \@ref(eq:majalg) can be generalized to
$x^{(k+1)}=\phi(x^{(k)})$, where $\phi$ maps $\mathbb{S}$ into
$\mathbb{S}$ such that $\eta(\phi(x),x)\leq\sigma(x)$ for all $x$.

We give a small illustration in which we minimize $\sigma$ with
$\sigma(x)=\sqrt{x}-\log{x}$ over $x>0$. Obviously we do not need
majorization here, because solving $\mathcal{D}\sigma(x)=0$ immediately
gives $x=4$ as the solution we are looking for.

To arrive at this solution using majorization we start with
\begin{equation}
\sqrt{x}\leq\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}},
(\#eq:sqrtmaj)
\end{equation} which is true because a differentiable concave function
such as the square root is majorized by its tangent everywhere.
Inequality \@ref(eq:sqrtmaj) implies \begin{equation}
\sigma(x)\leq\eta(x,y):=\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}}-\log{x}.
(\#eq:examplemaj)
\end{equation} Note that $\eta(\bullet,y)$ is convex in its first
argument for each $y$. We have $\mathcal{D}_1\eta(x,y)=0$ if and only if
$x=2\sqrt{y}$ and thus the majorization algorithm is \begin{equation}
x^{(k+1)}=2\sqrt{x^{(k)}}
(\#eq:examplealg)
\end{equation} The sequence $x^{(k)}$ converges monotonically to the
fixed point $x=2\sqrt{x}$, i.e. to $x=4$. If $x^{(0)}<4$ the sequence is
increasing, if $x^{(0)}<4$ it is decreasing. Also, by l'Hôpital,
\begin{equation}
\lim_{x\rightarrow 4}\frac{2\sqrt{x}-4}{x-4}=\frac12
(\#eq:hopi1)
\end{equation} and thus convergence to the minimizer is linear with
asymptotic convergence rate $\frac12$. By another application of
l'Hôpital \begin{equation}
\lim_{x\rightarrow 4}\frac{\sigma(2\sqrt{x)})-\sigma(4)}{\sigma(x)-\sigma(4)}=\frac14,
(\#eq:hopi2)
\end{equation} and convergence to the minimum is linear with asymptotic
convergence rate $\frac14$. Linear convergence to the minimizer is
typical for majorization algorithms, as is the twice-as-fast linear
convergence to the minimum value.

This small example is also of interest, because we minimize a *DC
function*, the difference of two convex functions. In our example the
convex functions are minus the square root and minus the logarithm.
Algorithms for minimizing DC functions define other important subclasses
of MM algorithms, the *DC Algorithm* of Tao Pham Dinh (see @lethi_tao_18
for a recent overview), the *Concave-Convex Procedure* of
@yuille_rangarajan_03, and the *Half-Quadratic Method* of Donald Geman
(see @nikolova_ng_05 for a recent overview). For each of these methods
there is a huge literature, with surprisingly little non-overlapping
literatures. The first phase of the smacof algorithm, in which we
improve the configuration for given disparities, is DC, concave-convex,
and half-quadratic.

In the table below we show convergence of \@ref(eq:examplealg) starting
at $x=1.5$. The first column show how far $x^{(k)}$ deviates from the
minimizer (i.e. from 4), the second shows how far$\sigma(x^{(k)})$
deviates from the minimum (i.e. from $2-\log 4$). We clearly see the
convergence rates $\frac12$ and $\frac14$ in action.

```{r majiter, echo = FALSE}
x <- 1.5
f <- sqrt(x) - log(x)
f0 <- 2 - log(4)
for (i in  1:15) {
  cat("itel ", formatC(i, digits = 0, width = 2, format = "d"),
      formatC(4 - x, digits = 10, format = "f"),
      formatC(f - f0, digits = 10, format = "f"),
      "\n")
  x <- 2 * sqrt(x)
  f <- sqrt(x) - log(x)
}
```

The first three iterations are shown in the figure below. The vertical
lines indicate the value of $x$, function is in red, and the first three
majorizations are in blue.

```{r majplot, fig.align = "center", echo = FALSE}
x <- 100:500/100
y <- sqrt(x) - log(x)
plot(x, y, type = "l", lwd = 3, col = "RED")
g <- function(x, y) {
return(sqrt(y)+ (x - y) / (2 * sqrt(y)) - log(x))
}
x1 <- 1.5
abline(v = x1)
z1 <- g(x, x1)
lines(x, z1, col = "BLUE")
x2 <- 2 * sqrt(x1)
abline(v = x2)
z2 <- g(x, x2)
lines(x, z2, col = "BLUE")
x3 <- 2 * sqrt(x2)
abline(v = x3)
z3 <- g(x, x3)
lines(x, z3, col = "BLUE")
```

### Majorizing Stress

## Second Phase: Update Transformation

### Spline Basis Details

*Splines* of degree $d$ on a closed interval $[a,b]$ are piecewise polynomials of degree $d$. The endpoints of the interval are the *boundary knots*. In the interval there are a number of *inner knots*. For smacof we suppose the inner knots are distinct. There is a polynomial piece between all consecutive
pairs of knots. Although the pieces can be parts of different polynomials
splines are required to have a certain degree of smoothness. In fact
at the interior knots a spline has $d - 1$ continuous derivatives. A
spline of degree zero is a step function, stepping to a different level
at each knot. A spline of degree one is piecewise linear, where the 
line segments are joined continuously at the inner knots. A spline of
degree two is piecewise quadratic and continuously differentiable at the
knots. And so on. There is no limit on the number of inner knots and
on the degree of the spline, although the number of interior knots
must be greater than or equal to the degree minus one. The flexibility
of the spline (as opposed to the rigidity of a polynomial on $[a,b]$
of the same degree) comes from the number and placement of the interior knots, not so much from the degree of the spline.

### B-splines

### Berstein PolynomiaLS


```{r bsplinekable, echo = FALSE}
x <- matrix(c(0:4, 1:5, rep(4,5), c(6, 8, 10, 12, 14), 5:9), 5, 5)
kable(x, format = "pipe", digits = 0, col.names = c("degree", "order", "ninner", "nknots", "span"), align = 'c', caption = "B spline parameters")
```

$$
\sum_i B_{i,k}(x)=1
$$

M-splines

$$
M_{i,k}(x)=\frac{k+1}{t_{i+k+1}-t_i}B_{i,k}(X)
$$ then $$
\int M_{i,k}(x)dx=1
$$

I-splines $$
I_{i,k+1}(z)=\int_{-\infty}^zM_{i,k}(x)dx
$$

When is a B-spline increasing ? $$
\mathcal{D}B_{i,k}(x)=
$$ Thus if $$
\mathcal{D}\sum_{i=1}^{d+m}\alpha_iB_{i,k}(x)=
$$

It is sufficient that $\alpha_i\leq\alpha_{i+1}$

Integral, I-splines

### Ordinal MDS

### Interval and Ratio MDS

Additive constant

### Cyclic Coordinate Decent

In the non-linear least squares (NNLS) problem the data are an
$n\times p$ matrix $X$, a vector $y$ with $n$ elements, and a positive
semi-definite diagonal matrix $W$. We want to minimize $$
\sigma(\beta):=\frac12(X\beta-y)'W(X\beta-y)
$$ over $\beta\geq 0$. In data analysis and statistics the problem is
often solved by *active set methods*, implemented in R for example by
NNLS (@mullen_vanstokkum_23) and FNNLS (@bro_dejong_97). Active set
methods are finitely convergent dual methods. While iterating the
intermediate solutions are not feasible (i.e. non-negative). In fact in
dual methods we reach feasibility and optimality at the same time. Also
the number of iterations, although theoretically finite, can be very
large.

In each smacof iteration we need an NNLS solution. Especially in the
early iterations the solution does not have to be very precise. Also the
solution from the previous NNLS problem will generally provide a very
good starting value for the next iteration (each NNLS problem has a "hot
start"). And finally, we would like all internediate solutions to be
feasible. These considerations have lead us to using *cyclic coordinate
descent* (CCD).

Suppose the current best feasible solution in CCD iteration $k$ is
$\beta^{(k)}$. The next CCD iteration changes each of the $p$
coordinates of $\beta^{(k)}$ in turn, maintaining feasibility, while
keeping the other $p-1$ coordinates fixed at their current values. Thus
within a CCD iteration $k$ we create intermediate solutions
$\beta^{(k,1)},\cdots,\beta^{(k,p)}$, where each of the intermediate
solutions $\beta^{(k,r)}$ differs from the previous one
$\beta^{(k,r-1)}$ in a single coordinate. For consistency we define
$\beta^{(k,0)}:=\beta^{(k)}$. After the iteration is finished we set
$\beta^{(k+1)}=\beta^{(k,p)}$.

Note that in smacof each iteration modifies the coordinates in the order
$1,\cdots,p$, which explains why the method is called "cyclic". There
are variations of CCD in which the order within an iteration is random
or greedy (choose the coordinate which gives the largest improvement) or
zig-zag $1,\cdots,p,p-1,\cdots,1$. We have not tried out these
alternatives in smacf, but we may in the future.

The effect of changing a single coordinate on the loss function is $$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$ where $e_j$ is the unit vector corresponding with the coordinate we
are changing, $g(\beta):=\mathcal{D}\sigma(\beta)=X'Wr(\beta)$ is the
gradient at $\beta$, and $r(\beta):=X\beta-y$ is the residual. Also
$S:=X'WX$. Note that if $s_{jj}=0$ then also $g_j(\beta)=0$ and thus
$\sigma(\beta+\epsilon e_j)=\sigma(\beta)$. In each CCD cycle we simply
skip updating coordinate $j$.

If $s_{jj}>0$ then $\sigma(\beta+\epsilon e_j)$ is a strictly convex
quadratic in $\epsilon$, which we must minimize under the constraint
$\beta_j+\epsilon\geq 0$ or $\epsilon\geq-\beta_j$. Define
$\hat\epsilon$ to be the solution of this constrained minimization
problem.

The quadratic ... has its minimum at $$
\tilde\epsilon=-\frac{g_j(\beta)}{s_{jj}}
$$ If $\beta+\tilde\epsilon$ is feasible then it is the update we are
looking for. Thus $\hat\epsilon=\tilde\epsilon$. If
$\beta+\tilde\epsilon<0$ then the contrained minimum is attained at the
boundary, i.e. $\hat\epsilon=-\beta_j$ and the updated $\beta_j$ is
zero. Thus, in summary, $\hat\epsilon=\max(\tilde\epsilon,-\beta_j)$.

One of the nice things about CCD is that $$
r(\hat\beta)=r(\beta)+\hat\epsilon x_j
$$ $$
g(\hat\beta)=g(\beta)+\hat\epsilon s_j
$$

It follows that $\hat\epsilon=0$ if and only if either $\beta_j=0$ and
$g_j(\beta)\geq 0$ or if $g_j(beta)=0$ and $beta_j>0$.

If $g_j(\beta)<0$ then $\tilde\epsilon>0$, and thus $\hat\epsilon>0$ and
$\sigma(\hat\beta)<\sigma(\beta)$. Thus we must have $g_j(\beta)\geq 0$.

If $\beta_j>0$ and $g_j(\beta)\not=0$ then there is an $\epsilon$ such
that $\sigma(\beta+\epsilon e_j)<\sigma(\beta)$. Thus if $\beta_j>0$ we
must have $g_j(\beta)=0$.

In summary at the minimum of $\sigma$ over $\beta\geq 0$ we must have
$\beta_j\geq 0$, $g_j(\beta)\geq 0$, and $\beta_jg_j(\beta)=0$ for all
$j$ (*complementary slackness*).

$$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$ where $S:=X'WX$.

Now suppose we minimize $\sigma$ over $\beta\geq 0$.

Our best solution so far is $\beta^{(k)}\geq 0$. Minimize
$\sigma(\beta^{(k)}+\epsilon e_1)$ over $\epsilon$ on the condition that
$\beta^{(k)}_1+\epsilon\geq 0$ or $\epsilon\leq-\beta^{(k)}_1$. If
$s_{11}=0$ then also $g_1(\beta)=0$ and we set
$\beta^{(k+1,1)}=\beta^{(k,1)}$. If $s_{11}>0$ we compute $$
\tilde\epsilon=-g_1(\beta)/s_{11}
$$ If $$
\beta^{(k)}_1+\tilde\epsilon\geq 0
$$ then $$
\beta^{(k+1,1)}=\beta^{(k)}_1+\tilde\epsilon
$$ If $$
\beta^{(k)}_1+\tilde\epsilon<0
$$ we set $$
\beta^{(k+1,1)}=0.
$$

# Smacof Program

### Front-end

The front-end for both smacofBS and smacofRC is written in R. The
analysis is started in the user's working directory by the command
smacofBS(foo) or smacofRC(foo), where foo is a user-chosen name (without
quotes).

Two text files need to be present in the working directory. The first is
fooParameters.txt, where of course you substitute the user-chosen name
for foo. The second file is fooDelta.txt, which has the dissimilarities
below the diagonal in row-major order.

The parameter file has key-value format. Here is an example.

```{r parfile, eval = FALSE}
nobj   9
ndim   3
init   2
width   10
precision   6
haveweights   0
itmax  1000
epsi  10
verbose  1
ditmax  5
depsi  6
dverbose  0
kitmax  5
kepsi  6
kverbose  0
degree  3
haveknots  3
ninner  5
ordinal  1
anchor 0
intercept  1
```

The parameter file is read first, using the R function read.table().
There is one key-value pair on each line, at the start of the line. The order of the lines does not matter. There can be additional comments or other text on
each line after the value field, as long as that text is space-separated
from the value field. Additional key-value lines with non-existing
parameters can be added at will.

Values of the parameters are put the local environment using R function
assign(), which means they are available to R throughout the smacof run.
Of course if we choose smacofRC the front-end needs to pass them to C
using C(), but they will be available again for the back-end. 

The Delta file, and any subsequent optional input files, are read with
the R function scan(). Values are separated by spaces. They can be on a
single line, or laid out as a lower-triangular matrix, or whatever. The
function scan() only stops reading if it reaches the end-of-file.

We'll now discuss the parameters one by one. Note that all parameters
are integers. The first two are obvious: *nobj* is the number of objects
and *ndim* the number of dimensions. These two parameters have no
default or recommended values, because they deyermined by the data. All
other parameters in our example parameter file are set to reasonable
values in our example parameter file. But the whole idea is to
experiment with various combinations of parameter values, so
"reasonable" is weaker than "recommended" and "recommended" is weaker
than "default".

The *init* parameter can have values 1, 2, or 3. If *init* equals 1 the
program reads an initial configuration from the file fooXinit.txt in the
working directory. The file has *nobj* \* *ndim* numbers, the initial
configuration, in row-major format. If *init* = 2 then the classical
Torgerson initial estimate will be computed. If *init* = 3 a random
initial estimate will be used.

*width* and *precision* are parameters for the output of the values of
stress during iterations.

*haveweights* is either zero or one. If zero there are no weights, which
is equivalent to all weights equal to one. If one then we will read a
file fooWeights.txt, which has the lower-diagonal $\frac12 n(n-1)$
weights in row-major order.

As explained in previous sections there are three iterative running in
smacof. There are two inner iterations: one for the configuration for
fixed disparities, and one for the disparities for fixed configuration.
The two inner iteration loops are nested in one outer iteration loop.
Each of the iterations has three parameters: one for the maximum number
of iterations, one for the stopping criterion, and one for the verbosity
of the iteration output. For the outer loop the parameters are *itmax*,
*ieps*, and *verbose*. For the inner configuration loop they are
*kitmax*, *keps*, and *kverbose*. And the inner transformation loop they
are *ditmax*, *deps*, and *dverbose*. If the verbose parameter is one,
then each iteration prints out the stress before and the stress after
update. If verbose is zero, nothing is printed. The stopping parameters
check if the change in stress in an iteration is less than epsilon,
where epsilon is 10\^-ieps, 10\^-keps, or 10\^-deps.

The final five parameters are used to define the nature of the spline
space for the transformations. *degree* is the degree of the piecewise
polynomials. The *haveknots* parameter can be 0, 1, 2, or 3. If it is
zero, there are no inner knots and we use the Bernstein polynomial
basis. If *haveknots* is one, the inner knots are read in from
fooKnots.txt in the usual way. If *haveknots* is two the knots are
equally spaced between zero and one, and if it is three the knots are
equally spaced on the precentile scale (so that the number of data
points between knots is approximately the same). The *ninner* parameter
determines the number of knots in the case that *haveknots* is either
two or three. If *haveknots* is zero, then *ninner* should be zero, if
*haveknots* is one it should be equal to the number of knots in
fooKnots.txt.

The three final spline parameters are *ordinal*, *anchor*, and *intercept*. If *ordinal* is one the fitted spline is constrained to be monotone. If *intercept* is zero then the first spline coefficient is constrained to be zero (which
means the first column is deleted from the basis). This means that the spline
is constrained to be zero at the lower boundary knot. If *intercept* is one
there is no such constraint, and the spline can be anything at the lower boundary
(subject to monotonicity of *ordinal* is one). If *anchor* is one then the
boundary knots are set to zero and the maximum dissimilarity, if *anchor* is
zero the boundary knots are the minimum and maximum dissimilarity. Thus if *intercept* is zero and  *anchor* is one the spline goes through the origin.

The computations in the frontend are straightforward. We first transform
the dissimilarities linearly so that the smallest becomes zero and the
largest becomes one. This is not strictly necessary but it makes the
spline computations slightly easier.

Initial Estimates for $X$ Spline Basis

### Engine

ALS First Phase Second Phase

### Back-end

The back-end consists of a number of R functions that have the list
returned by smacofBS or smacofRC as an argument. They can be used to 
make plots, compute derivatives, convert matrices to an easily
printable format, do sensitivity analysis, and so on. The philosophy
is that in the backend the main computing is finished and we
just create different representations of the results.

#### Plotting

There are two main plot functions in the backend: smacofShepardPlot()
and smacofConfigurationPlot(). A smacofShepardPlot has the
dissimilarities (un-normalized) on the horizontal axes and
it has the distances and the disparities on the vertical axis.
It draws the spline, and shows where the fitted disparities are
on the spline. It also plots the (delta, dist) pairs as points,
to show how far they deviate from the spline. Optionally 
smacofShepardPlot() can draw vertical lines at the inner knots
(argument knotlines = TRUE), and optionally it can connect 
the (delta, dhat) points on the spline to the (delta, dist)
points with lines (argument fitlines = TRUE). 

It must be emphasized that smacofShepardPlot() draws the spline
over the whole interval, which is either (deltamin, deltamax)
if anchor = 0 and (0, deltamax) if anchor = 1. It does
this by recomputing the spline at a large number of
uniformly spaced points in the interval, where the
number of points is given by the smacofShepardPlot()
parameter resolution. Thus we do not use only the
data points (delta,dhat) and then let the R plot function 
interpolate linearly. That can be misleading. It is
especially misleading if degree is zero (step function)
or if there are consecutive inner knots with no data values
between the knots. Degree zero is handled by the special 
purpose step function plotting routine smacofPlotStepFunction(),
which makes sure the spline is drawn as a horizontal segment
from one knot to the next knot. In addition smacofShepardPlot()
can set some base R plot parameters such as col, cex, lwd, and pch
(see the R documentation).

The smacofConfigurationPlot() function is much simpler than
smacofShepardPlot(). It sets pch, col, and cex. It uses
the smacofBS/RC labels parameter to decide how to label
the points in the configuration. If labels = 1 it reads
a character vector of labels from fooLabels.txt, where
foo is of course the name of the run. If labels = 2
the points are numbered, if labels = 3 plotting uses
the pch symbol for all points. If the dimension $p$
is larger than two, smacofConfigurationPlot() uses
the parameters dim1 and dim2 to select the dimensions
to plot.

#### Writing

#### Checking

#### Derivatives

#### Sensitivity

Perturbation regions 

Parametric Bootstrap 

Jacknife

# Examples


## ekman

The @ekman_54 color circle similarities have been used in many, if not most, multidimensional scaling textbooks and review articles. See, for example,
@borg_groenen_05, p 63-68. Its popularity is due, no doubt,
to its astonishing good two-dimensional fit and its easy interpretability.

In brief, Ekman used what he called the *Method of similarity analysis*. The stimuli
were fourteen color filters, transmitting light of wave lengths 434 $m\mu$ mp to 674 $m\mu$. The subjects were 31 students with normal color vision. They judged
the similarity between all 91 pairs of colors on a five-point rating scale
with categories 0-4. The similarity judgments were then averaged over students
and the average was divided by four to create similarties $s_{ij}$ between zero 
and one. For MDS purposes we set $\delta_{ij}=1-s_{ij}$.

```{r ekmanresults, echo = FALSE}
source("../../smacofExamples/smacofBS/ekman/ekmanMetric.R")
source("../../smacofExamples/smacofBS/ekman/ekman3Metric.R")
source("../../smacofExamples/smacofBS/ekman/ekman0301012.R")
source("../../smacofExamples/smacofBS/ekman/ekman5331012.R")
source("../../smacofExamples/smacofBS/ekman/ekman50021012.R")
source("../../smacofExamples/smacofBS/ekman/ekman50031012.R")
```

We start with a metric analysis in two dimensions. In all analyses in this section of the manual we start with the Torgerson initial configuration, and we iterate until the change in stress from one outer iteration to the next is less than or equal to 1e-10. 

After `r ekmanMetric$itel` iterations we find a minimum stress of `r ekmanMetric$snew`. The Shepardplot in the left panel of figure \@ref(fig:ekmanmetric) shows a reasonably
good fit, but strongly suggests making a non-linear transformation to improve the fit.

It is known that if we use $\delta_{ij}=(1-s_{ij})^3$ then the two-dimensional solution computed by smacof is the
global minimum of stress for all dimensions $p\geq 2$ (@deleeuw_U_14b, @deleeuw_E_19e). So we repeated the metric analysis using the cubic transformation.
After `r ekman3Metric$itel` iterations we find stress `r ekman3Metric$snew` and the Shepardplot in the right panel of figure \@ref(fig:ekmanmetric). A better fit, and
the deviations from linearity are what one expects from least squares: overestimation
of the smaller dissimilarities and underestimation of the larger ones. Nevertheless,
the configurations for the two solutions in figure \@ref(fig:ekmanmetricconf) are remarkably similar, showing once again that configurations tend to be more stable than Shepardplots.


:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmanmetric) ABOUT HERE**
:::
::::

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmanmetricconf) ABOUT HERE**
:::
::::

We present four additional non-metric analyses, using the seven-digit coding
illustrated in table \@ref(tab:ekmankable). The stress values in the table cannot be compared directly with those of the metric solutions, because in the non-metric analysis the dissimilarities are normalized and thus the stress scales are different. We can compare within the table, however, and compare all Shepard plots.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT TABLE \@ref(tab:ekmankable) ABOUT HERE**
:::
::::


:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmancubic) ABOUT HERE**
:::
::::

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmanordinal) ABOUT HERE**
:::
::::

## gruijter

The ekman examples had a really good fit. We use the data from @degruijter_67
for an example with a rather bad fit in two dimensions.
Dissimilarity judgments between nine Dutch political parties were
collected in 1966 by the complete method of triads. Results were
averaged over 100 students. Averaging over a heterogeneous population
will cause regression to the mean, and thus dissimilarities will tend
to be more equal than they should be on the basis of the individual 
results. This means there should be a substantial difference in fit between
the "ratio" and the "interval" options
(i.e. between the interval parameter equal to one or zero).

```{r gruijterresults, echo = FALSE}
source("../../smacofExamples/smacofBS/gruijter/gruijter0100012.R")
source("../../smacofExamples/smacofBS/gruijter/gruijter0100102.R")
source("../../smacofExamples/smacofBS/gruijter/gruijter0301012.R")
source("../../smacofExamples/smacofBS/gruijter/gruijter0301013.R")
source("../../smacofExamples/smacofBS/gruijter/gruijter5331012.R")
source("../../smacofExamples/smacofBS/gruijter/gruijter5331013.R")
```

## morse

```{r morseresults, echo = FALSE, eval = FALSE}
source("results/morseResults.R")
```

# Tables

```{r ekmankable, echo = FALSE}
x <- matrix(c(
ekman0301012$ninner, ekman0301012$degree, ekman0301012$haveknots,
ekman0301012$ordinal, ekman0301012$intercept, ekman0301012$anchor, 
ekman0301012$ndim, ekman0301012$snew, ekman0301012$itel,
ekman5331012$ninner, ekman5331012$degree, ekman5331012$haveknots,
ekman5331012$ordinal, ekman5331012$intercept, ekman5331012$anchor, 
ekman5331012$ndim, ekman5331012$snew, ekman5331012$itel,
ekman50031012$ninner, ekman50031012$degree, ekman50031012$haveknots,
ekman50031012$ordinal, ekman50031012$intercept, ekman50031012$anchor, ekman50031012$ndim, ekman50031012$snew, ekman50031012$itel,
ekman50021012$ninner, ekman50021012$degree, ekman50021012$haveknots,
ekman50021012$ordinal, ekman50021012$intercept, ekman50021012$anchor, ekman50021012$ndim, ekman50021012$snew, ekman50021012$itel), 4, 9, byrow = TRUE)
knitr::kable(x, format = "pipe", digits = 10, col.names = c("ninner", "degree", "haveknots", "ordinal", "intercept", "anchor", "ndim", "stress", "itel"), align = 'c',
caption = "Analyses of the ekman example")
```
******
```{r gruijterkable, echo = FALSE}
x <- matrix(c(
gruijter0100012$ninner, gruijter0100012$degree, gruijter0100012$haveknots,
gruijter0100012$ordinal, gruijter0100012$intercept, gruijter0100012$anchor, gruijter0100012$ndim, gruijter0100012$snew, gruijter0100012$itel,

gruijter0100102$ninner, gruijter0100102$degree, gruijter0100102$haveknots,
gruijter0100102$ordinal, gruijter0100102$intercept, gruijter0100102$anchor, gruijter0100102$ndim, gruijter0100102$snew, gruijter0100102$itel,

gruijter0301012$ninner, gruijter0301012$degree, gruijter0301012$haveknots,
gruijter0301012$ordinal, gruijter0301012$intercept, gruijter0301012$anchor, gruijter0301012$ndim, gruijter0301012$snew, gruijter0301012$itel,

gruijter0301013$ninner, gruijter0301013$degree, gruijter0301013$haveknots,
gruijter0301013$ordinal, gruijter0301013$intercept, gruijter0301013$anchor, gruijter0301013$ndim, gruijter0301013$snew, gruijter0301013$itel,

gruijter5331012$ninner, gruijter5331012$degree, gruijter5331012$haveknots,
gruijter5331012$ordinal, gruijter5331012$intercept, gruijter5331012$anchor, gruijter5331012$ndim, gruijter5331012$snew, gruijter5331012$itel,

gruijter5331013$ninner, gruijter5331013$degree, gruijter5331013$haveknots,
gruijter5331013$ordinal, gruijter5331013$intercept, gruijter5331013$anchor, gruijter5331013$ndim, gruijter5331013$snew, gruijter5331013$itel), 6, 9, byrow = TRUE)
knitr::kable(x, format = "pipe", digits = 10, col.names = c("ninner", "degree", "haveknots", "ordinal", "intercept", "anchor", "ndim", "stress", "itel"), align = 'c', caption = "Analyses of the gruijter example")
```
******

# Figures

```{r ekmanmetric, fig.align = "center", fig.cap = "ekman example, metric", echo = FALSE}
par(pty = "s", mfrow = c(1, 2))
smacofShepardPlot(ekmanMetric, main = "Metric Analysis (1-s)", fitlines = TRUE)
smacofShepardPlot(ekman3Metric, main = "Metric Analysis (1-s)^3", fitlines = TRUE)
```


```{r ekmanmetricconf, fig.align = "center", fig.cap = "ekman example, metric", echo = FALSE}
par(pty = "s", mfrow = c(1, 2))
smacofConfigurationPlot(
  ekmanMetric,
  main = "Metric Analysis (1-s)", cex = .75)
smacofConfigurationPlot(
  ekman3Metric,
  main = "Metric Analysis (1-s)^3", cex = .75)
```

```{r ekmancubic, fig.align = "center", fig.cap = "ekman example, cubic analysis", echo = FALSE}
par(mfrow = c(1, 2))
smacofShepardPlot(ekman0301012, main = "Cubic Polynomial", fitlines = TRUE)
smacofShepardPlot(ekman5331012, main = "Cubic Spline", fitlines = TRUE, knotlines = TRUE)
```

```{r ekmanordinal, fig.align = "center", fig.cap = "ekman example, pseudo-nonmetric analysis", echo = FALSE}
par(mfrow = c(1,2))
smacofShepardPlot(ekman50031012, main = "Equal Percentile Intervals", knotlines = TRUE)
smacofShepardPlot(ekman50021012, main = "Equal Delta Intervals", knotlines = TRUE)
```

```{r gruijterlinear, fig.align = "center", fig.cap = "gruijter example, linear analysis", echo = FALSE}
par(pty = "s", mfrow = c(1,2))
smacofShepardPlot(gruijter0100012, main = "Line without intercept", fitlines = TRUE)
smacofShepardPlot(gruijter0100102, main = "Line with intercept", fitlines = TRUE)
```

```{r gruijterlinearconf, fig.align = "center", fig.cap = "gruijter example, linear analysis", echo = FALSE}
par(pty = "s", mfrow = c(1, 2))
smacofConfigurationPlot(
  gruijter0100012,
  main = "Line without intercept", cex = .75)
smacofConfigurationPlot(
  gruijter0100102,
  main = "Line with intercept", cex = .75)
```

```{r gruijtercubic, fig.align = "center", fig.cap = "gruijter example, cubic polynomial", echo = FALSE}
par(mfrow = c(1,2))
smacofShepardPlot(gruijter0301012, main = "Two dimensions", fitlines = TRUE)
smacofShepardPlot(gruijter0301013, main = "Three dimensions", fitlines = TRUE)
```

```{r gruijtersplines, fig.align = "center", fig.cap = "gruijter example, cubic spline analysis", echo = FALSE}
par(mfrow = c(1,2))
smacofShepardPlot(gruijter5331012, main = "Two dimensions", fitlines = TRUE)
smacofShepardPlot(gruijter5331013, main = "Three dimensions", fitlines = TRUE)
```


# References
