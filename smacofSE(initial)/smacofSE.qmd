---
title: |
    | Smacof at 50: A Manual
    | Part zz: smacofSE: Initial Estimates
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
---

```{r load_stuff, echo = FALSE}
library(microbenchmark)
library(RSpectra)
library(quadprog)
source("../../smacofCode/smacofSE(initial)/smacofTorgerson.R")
source("../../smacofCode/smacofSE(initial)/smacofALSCAL.R")
source("../../smacofCode/smacofSE(initial)/smacofElegant.R")
source("../../smacofCode/smacofSE(initial)/smacofMaximumSum.R")
source("../../smacofCode/smacofUT(utilities)/smacofSimpleUtilities.R")
source("../../smacofCode/smacofUT(utilities)/smacofIOUtilities.R")
source("../../smacofCode/smacofUT(utilities)/smacofDataUtilities.R")
source("../../smacofData/ekman.R")
source("../../smacofData/gruijter.R")
```

\sectionbreak 
 
# Introduction

The best way to avoid unwanted (i.e. non-global) local minima in smacof is to start the iterations with an excellent initial configuration. Even if the iterations converge to a non-global local minimum, at least we can be sure than the solution has a lower stress than the initial configuration, which was supposedly already excellent.

In this chapter we implement two fairly elaborate initial
estimates for metric smacof, which can of course also be 
used in the alternating least squares algorithm for non-linear
and non-metric smacof. Both initial estimates basically
iteratively minimize alternative loss functions, respectively called sstress and strain (see @deleeuw_heiser_C_82 for a definition and comparison of these loss functions).

The R functions implementing the minimization of these
loss functions can be interpreted (and can actually be used) as
alternative MDS methods. It may seem somewhat peculiar to
start an iterative MDS technique with an initial estimate 
computed with another iterative MDS technique. But it is
not exactly unprecedented, since traditionally MDS programs
compute their initial configurations by using classical
MDS (which uses iterative eigenvalue-eigenvector methods).

Our initial configuration techniques have a great deal of
flexibility, because we do not necessarily iterate to
convergence in the initial phase. We use the option
in monotone iterative algorithms to merely improve instead
of completely solve.

Also keep in mind that our objective is to find excellent
local minima of stress, in fact our ultimate objective
is to find the global minimum. This makes it desirable
to find as good an initial approximation as possible,
even if that requires a considerable amount of 
computation.

\sectionbreak

# Using Strain

In the standard R versions of smacof the initial configuration is chosen
using classical multidimensional scaling, i.e. the method proposed in @torgerson_58 and @gower_66.

Classical scaling is based on the Euclidean embedding theorem of @schoenberg_35, revealed to psychometricians by @young_householder_38. A real hollow symmetric matrix $\Delta$ of order $n$, with non-negative entries, is a Euclidean distance matrix if and only if the matrix
$$
B:=-\frac12 J\Delta^2J
$${#eq-defbmat}
is positive semi-definite. 
Here $\Delta^2$ is the elementwise square of $\Delta$ and $J=I-n^{-1}ee'$ is the centering matrix. Moreover if $B$ is positive semi-definite then the embedding dimension is the rank of $B$. Classical scaling computes $B$ and its eigen-decomposition. It then constructs the initial estimate $X$ for $p$-dimensional MDS by using the $p$ largest eigenvalues $\Lambda$ of $B$, and the corresponding eigenvectors in $K$,  to give $X=K\Lambda^\frac12$.

Classical scaling, as an MDS technique, has one major advantage over other forms
of MDS, but also some disadvantages. The advantage is that it finds the global minimum of the strain loss function, defined as
$$
\sigma(X):=\frac14\text{tr}\ J(\Delta^2-D^2(X))J(\Delta^2-D^2(X))J)
$${#eq-strain}
with $D^2(X)$ the squared Euclidean distances. So not only does classical scaling minimize strain, but it actually computes its global minimum, and if the ordered eigenvalues of $B$ from @eq-defbmat satisfy $\lambda_p(B)>\lambda_{p+1}(B)$ that global minimum over $p$-dimensional configurations is unique (up to rotation and translation).

To see that minimizing strain is an eigenvalue problem in disguise, note that if $X$ is column centered then
$-\frac12JD^2(X)J=XX'$ and thus $-\frac12J(\Delta^2-D^2(X))J=B-XX'$. Taking
the sum of squares on both sides of the equation, and using the fact that $J$ is idempotent, gives the alternative expression for strain
$$
\sigma(X)=\text{tr}\ (B-XX')^2.
$${#eq-cstrain}
Minimizing @eq-cstrain over $p$-dimensional configuartions is indeed classical scaling. The matrix form in @eq-strain strain was first given
by @deleeuw_heiser_C_82, although equivalent versions, using different notation, are already in @gower_66 and @mardia_78. 

The first disadvantage of classical scaling is that the $B$-matrix may only
have $q<p$ positive eigenvalues. If that is the case the global minimizer $X_p$ of strain in $p<q$ dimensions does not exist. The global minimizer in $r\leq p$
dimensions has only $q$ non-zero dimensions (and $p-q$ zero dimensions).
In practice this disadvantage is often not a very serious one, because having fewer than $p$ positive eigenvalues suggests a bad fit so that maybe MDS is not a good technique choice in the first place. Because $n$ is usually much larger than $p$ matrix $B$ is bound to have more than $p$ positive eigenvalues. If not, one can always compute an additive constant to force positive semi-definiteness of $B$ (@cailliez_83).

The second disadvantage is that classical scaling, unlike smacof, has no provision to include data weights $w_{ij}$ for each of the dissimilarities $\delta_{ij}$. 
Or, if there are weights it is unclear if they should be applied to @eq-strain or
@eq-cstrain, and no matter where they are used the two definitions of strain are no longer equivalent and the global minimum advantage of classical scaling is lost. 

In particular this disadvantage also means that there cannot be missing dissimilarities, or, more precisely, something additional has to be done if there are missing data. One obvious possibility is to use alternating least squares to minimize strain, alternating a step to impute the missing dissimilarities and a step to compute the optimal configuration. The current smacof program in R imputes the missing dissimilarities by replacing them with the average non-missing dissimilarity, which is computationally convenient but not very satisfactory. This second disadvantage also means there is no straightforward version of classical
scaling for multidimensional unfolding, in which all within-set dissimilarities
are missing.

Part of the problem is the double centering operator $J$, because it requires
complete data. This problem can be alleviated if we have one object, say the
first one, for which there are no missing data. We then put that object in the
origin of the configuration and compute $-\frac12\{\delta_{ij}^2-\delta_{1i}^2-\delta_{1j}^2\}$, which is equal 
to $x_i'x_j$ for Euclidean dissimilarities. We can then define a version of strain on the non-missing elements of $B$, but we still need a low-rank approximation of
a symmetric matrix with missing data. That is, we need low-rank symmetric matrix completion, for which there is a gigantic literature (@nguyen_kim_shim_19), although that literature mostly addresses the rectangular case.

Another disadvantage, or peculiarity, is emphasized by @deleeuw_meulman_C_86. If $\Delta$ is Euclidean then @gower_66 shows that 
$$
d_{ij}^2(X_1)\leq d_{ij}^2(X_2)\leq\cdots\leq d_{ij}^2(X_r)=\delta_{ij}^2,
$${#eq-frombelow}
where $X_p$ is the $p$-dimensional classical scaling 
solution and $r$ is the rank of $B$. Thus squared dissimilarities are 
approximated from below, which may not be the most obvious way to 
approximate. If $B$ has negative eigenvalues
then @eq-frombelow is no longer true and we have
$$
d_{ij}^2(X_1)\leq d_{ij}^2(X_2)\leq\cdots\leq d_{ij}^2(X_s)\geq\delta_{ij}^2,
$${#eq-frombelowp}
where $s$ is the number of positive eigenvalues.

Another peculiarity, discussed in the excellent
paper by @bailey_gower_90, is that strain is the sum of squares over all $n^2$ residuals, which means each off-diagonal element is used twice, each diagonal element only once. In minimizing stress
or sstress the diagonal does not play a role, minimizing over all elements below the diagonal gives the same result as minimizing over all elements.

And finally the squaring and double-centering of the dissimilarities may not
be such a good idea from a statistical point of view. Squaring will
emphasize large errors, and can easily lead to outliers. Double-centering
introduces dependencies between different observations, because the
pseudo scalar products in $B$ are linear combinations of multiple
(squared) dissimilarities.

Nevertheless the smacof project, and the code for this chapter, includes a version of classical MDS that can handle missing data (but not general weights). A similar non-metric version of strain was discussed by @trosset_98.

## Algorithm

We formulate loss as
$$
\sigma(X,\theta)=\text{tr}\ (B(\theta)-XX')^2
$${#eq-strainmissing}
where
$$
B(\theta)=-\frac12J(\Delta_0^2+\sum \theta_kE_k)J=B_0+\sum \theta_kT_k
$${#eq-btheta}
Here $\Delta_0^2$ is the matrix with squared dissimilarities, with elements equal to zero for missing data.
The $E_k$ code missing data, and are for the form $e_ie_j'+e_je_i'$. Thus they are zero, except for
elements $(i,j)$ and $(j,i)$, which are one. 

Strain @eq-strainmissing must be minimized over configurations
$X$ and over $\theta\geq 0$.

We use alternating least squares. Minimizing over $X$ for fixed $\theta$ is classical MDS. Minimizing
over $\theta$ for fixed $X$ is a non-negative linear least squares problem. 
For the first problem we use the eigs_sym function from the RSpectra package (@qiu_mei_24), for the second the quadprog package and function (@turlach_weingessel_19). Both functions in an inner iteraton have a 
cold start, which may be wasteful in later iterations, and the plan therefor is to eventually replace them with the majorization methods in smacofEigenRoutines.R
and smacofNNLS.R (both in the smacofUtilities directory in smacofCode).

If there are no missing data the program just performs classical MDS.

Finding an better $X^{(\nu+1)}$ for given $\theta^{(\nu)}$ is classical scaling. In smacofTorgerson() we find $X$ to decrease strain, not necessarily to minimize it. So we have to decrease 
$$
\sigma(X)=\text{tr}\ (B(\theta^{(\nu)})-XX')^2
$${#eq-straintheta}
The majorization method smacofSymmetricEckartYoung() is described in the chapter on utilities.

To impute the missing data define the residuals
$$
R^{(k)}:=B_0-X^{(k)}\{X^{(k)}\}'.
$${#eq-strainres}
Then
$$
\sigma(\theta)=\text{tr}\ (R^{(\nu)}+\sum\theta_sT_s)^2
$${#eq-strainimpute}
where the $T_s$ are matrices of the form $-\frac12JE_{ij}J$. 
Note $R$ is doubly-centered and thus $-\frac12\text{tr}\ JE_{ij}JR=-r_{ij}$.
Consequently
$$
\sigma(\theta)=\text{tr}\ \{R^{(\nu)}\}^2-2\theta'r^{(\nu)}+\theta'V\theta,
$${#eq-strainexpand}
where $V$ has elements $\text{tr}\ T_sT_t$. After some additional computation we find the following convenient expression for the elements of $V$.
$$
\frac14\text{tr}\ JE_{ij}JJE_{kl}J=
\begin{cases}
\frac12-\frac{1}{n}+\frac{1}{n^2}&\text{two indices in common},\\
-\frac{1}{2n}+\frac{1}{n^2}&\text{one index in common},\\
\frac{1}{n^2}&\text{no indices in common}.
\end{cases}
$${#eq-strainvmat}
Minimizing @eq-strainexpand
over $\theta\geq 0$ is straightforward quadratic programming.

The utilities chapter has the majorization routine smacofNonnegativeQP() that find a $\theta^{(\nu)}$ that decreases the loss in @eq-strainexpand over $\theta\geq 0$. For the majorization we need an upper bound for the largest eigenvalue
of $V$.

\sectionbreak

# Maximum Sum

In some early reports (@deleeuw_R_68e, @deleeuw_R_68g) I proposed using what I call the "maximum sum principle", not just for metric MDS, but for various other metric and nonmetric techniques as well. Around the same time @guttman_68 used a similar initial configuration for his MDS algorithms. 

In metric MDS the maximum sum method maximizes
$$
\rho(X):=\mathop{\sum\sum}_{1<i\leq j<n}w_{ij}\delta_{ij}^2d_{ij}^2(X)
$${#eq-rhodef}
over $X$ in some compact set of configurations. General considerations seem to suggest that $\rho$ will tend to be large if $\Delta$ and $D(X)$ are numerically similar, or at least similarly ordered. The actual normalization
constraint on $X$ is not specified, and different constraints will lead
to different solutions.

Now $\rho(X)=\text{tr}\ X'BX$ where
$B$ is  defined as
$$
B:=\mathop{\sum\sum}_{1<i\leq j<n} w_{ij}\delta_{ij}^2A_{ij},
$${#eq-bdef}
i.e. $B$ has off-diagonal elements $-w_{ij}\delta_{ij}^2$, with the diagonal filled in such a way that rows and columns add up to zero. It follows that $B$ is symmetric, doubly-centered, and positive semi-definite. 

But no matter how simple and attractive $\rho$ is, we still have to decide how to bound $X$ and how to introduce multi-dimensionality. A naive 
choice would be requiring $\text{tr}\ X'X=1$,  but that gives a solution $X$ of rank one with all
columns equal to the eigenvector corresponding with the largest eigenvalue of $B$. @deleeuw_R_70b suggests maximizing $\rho$ over $X'X=I$, which leads 
to choosing $X$ as the eigenvectors corresponding with the $p$ largest
eigenvalues of $B$. But that result is of limited usefulness, because the
MDS problem does not specify anywhere that the configuration $X$ must be 
orthonormal. But since the maximum sum solution is only to be used as
an initial configuration this may not be that serious.

@guttman_68 says his initial configuration maximizes 
$$
\lambda:=\sum_{s=1}^p\frac{x_s'Bx_s}{x_s'x_s}
$${#eq-guttmaxsum}
But that cannot be correct. In the first place it would mean that the $x_s$ can be scaled independently and arbitrarily, in the second place the maximum of $\lambda$ is just $p$ times the largest eigenvalue of $B$ and all $x_s$ are proportional to the corresponding eigenvector. Thus Guttman's derivation is wrong. 

Both De Leeuw and Guttman seem to arrive, in somewhat mysterious ways, at the "solution" $X=K\Lambda^\frac12$, where $K$ and $\Lambda$ are the $p$ dominant eigenvectors and eigenvalues of $B$.
An ad hoc justification of this normalization interprets it as a two step optimization over $X=K\Psi$ with $K'K=I$ and $\Psi$ diagonal. First maximize $\rho=\text{tr}\ K'BK$ over $K'K=I$. Then maximize $\rho=\text{tr}\ \Psi K'BK\Psi=\text{tr}\ \Lambda\Psi^2$ over $\text{tr}\ \Psi^4=1$, with $\Lambda$ again the $p$ largest eigenvalues. This gives $\Psi=\Lambda^\frac12$. But note that if $p=n-1$ then $K\Lambda K'$ reproduces $B$, and
$$
d_{ij}^2(X)=(e_i-e_j)'B(e_i-e_j)=\sum_{k=1}^nw_{ik}\delta_{ik}^2+\sum_{k=1}^nw_{jk}\delta_{jk}^2+2w_{ij}\delta_{ij}^2,
$${#eq-maxsumrep}
which does not reproduce the squared dissimilarities. Thus the maximum sum method gives at best a quick and dirty initial configuration

\sectionbreak

# Using Sstress

## Elegant

The maximum sum method is related to the problem of minimizing the MDS loss function sstress, defined by @takane_young_deleeuw_A_77 as
$$
\sigma(X)=\mathop{\sum\sum}_{1<i\leq j<n} w_{ij}(\delta_{ij}^2-d_{ij}^2(X))^2.
$${#eq-sstressdef}
For notational convenience only, assume the sum of squares of the squared dissimilarities is equal to one. Of course normalizing dissimilarities does
not change the minimization problem.

If we expand @eq-sstressdef we find
$$
\sigma(X)=1-2\rho(X)+\eta^2(X) ,
$${#eq-sstressexp}
with $\rho$ defines as in @eq-rhodef, and with
$$
\eta^2(X):=\mathop{\sum\sum}_{1<i\leq j<n} w_{ij}d_{ij}^4(X).
$${#eq-stresseta}
Now, by homogeneity of the distance function, 
$$
\min_X\sigma(X)=\min_{\eta^2(X)=1}\min_{\lambda}\{1-2\lambda\rho(X)+\lambda^2\}=1-\max_{\eta^2(X)=1}\rho^2(X).
$${#eq-innermin}
Thus minimizing sstress is equivalent to maximizing $\rho$ over all $X$ in the compact set $\eta^2(X)=1$. In that sense minimizing
sstress is a maximum sum method.
But, unlike the maximum sum approach of Guttman and de Leeuw, this normalization of $X$ does not lead to a simple eigenvalue-eigenvector problem. It can be solved, however, by majorization, which means, in thus case, iteratively solving a sequence of related eigenvalue-eigenvector problems. 

If we use sstress to approximate stress there is a more or less rational way to choose the weights in sstress. This seems a good thing
since we use sstress solutions as initial configurations for minimizing
stress.
\begin{align}
\sigma(X)&=\mathop{\sum\sum}_{1<i\leq j<n}w_{ij}(\delta_{ij}-d_{ij}(X))^2\notag\\
&=\mathop{\sum\sum}_{1<i\leq j<n}\frac{w_{ij}}{(\delta_{ij}+d_{ij}(X))^2}
(\delta_{ij}^2-d_{ij}^2(X))^2\notag\\&\approx\frac14\mathop{\sum\sum}_{1<i\leq j<n} \frac{w_{ij}}{\delta_{ij}^2}
(\delta_{ij}^2-d_{ij}^2(X))^2.
\end{align}

The complicated history of this majorization algorithm, which early on was called ELEGANT, starts with @deleeuw_U_75b, @takane_77, and 
@browne_87. You may notice that in the references @deleeuw_U_75b is not linked to a pdf,
because the paper seems to be irretrievably lost. It hangs on to its existence because it is discussed, referenced, and used, by Takane and Browne. See @deleeuw_groenen_pietersz_E_16m and @takane_16 for more on the history. The original derivation in @deleeuw_U_75b used augmentation
(@deleeuw_C_94c), which leads to unsharp majorization and to the slow convergence
mentioned by De Leeuw, Browne, and Takane as a major disadvantage of ELEGANT. In @deleeuw_groenen_pietersz_E_16m a sharper majorization is used to improve the asymptotic convergence rate of the original algorithm by a large factor $n$.
The modified ELEGANT is much faster than the original version (@deleeuw_E_16o
).

Now for the actual algorithm. Since it handles general non-negative weights, including zeroes, there is no fitting step as in our missing data generalization of strain.

Define $A_{ij}$ as usual, $C:=XX'$, $a_{ij}:=\text{vec}(A_{ij})$, and $c=\text{vec}(C)$. Also use $A$ for the $n^2\times\frac12n(n-1)$ matrix
that has the $a_{ij}$ with $i<j$ as columns, and $W$ for the diagonal matrix
with the $w_{ij}$ for $i<j$. Then $d_{ij}^2(X)=\text{tr}\ A_{ij}C=a_{ij}'c$, and thus
$$
\sigma(C)=1-2b'c+c'Hc,
$${#eq-elegantc}
with $b=\text{vec}(B)$, with $B$ from @eq-bdef, and with
$$
H:=AWA'.
$${#eq-hdef}
There are two alternative expressions for the $n^2\times n^2$ matrix $H$ which are worth mentioning. The first one uses Kronecker products. It is
$$
H=\mathop{\sum\sum}_{1<i\leq j<n} w_{ij}(A_{ij}\otimes A_{ij}).
$${#eq-halt1}
The second expression represents $H$ as an $n\times n$ block-matrix with elements (blocks) that are themselves $n\times n$ matrices. The $(i, j)$ off-diagonal block of $H$ is
$$
\{H\}_{ij}=-w_{ij}A_{ij},
$${#eq-halt2}
while for the diagonal blocks
$$
\{H_{ii}\}=\sum_{j\not= i} w_{ij}A_{ij}.
$${#eq-halt3}

We now apply the standard majorization method for quadratic optimization.
Let $\mu$ be an upper bound for the largest eigenvalue $\lambda_+$ of $H$.  Write $C=\tilde C+(C-\tilde C)$.
Then
\begin{multline}
\sigma(c)\leq\sigma(\tilde c)-2 (b-H\tilde c)'(c-\tilde c)+\mu(c-\tilde c)'(c-\tilde c)=\\\sigma(\tilde c)+\mu\|c-(\tilde c+\mu^{-1}H\tilde c-b))\|^2-\mu^{-1}\|H\tilde c-b\|^2.
\end{multline}{#eq-elegantmaj}
Thus we find $c^{(\nu+1)}$ by minimizing
$$
\omega(c):=\|c-(c^{(\nu)}+\mu^{-1}(Hc^{(\nu)}-b))\|^2
$${#eq-elegantstep}
over $c$. Now
$$
Hc=\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}a_{ij}a_{ij}' c=\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}d_{ij}^2(c)a_{ij}.
$${#eq-hvecmat}
Using @eq-hvecmat we can translate back to matrices 
$$
\omega(C)=\text{tr}\ (C-(C^{(\nu)}+\mu^{-1}R(C^{(\nu)}))^2
$${#eq-omegamat}
where 
$$
R(C^{(\nu)}):=\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}(d_{ij}^2(C^{(\nu)})-\delta_{ij}^2)
$${#eq-relegant}
This is a low-rank symmetric matrix approximation problem, which we have encountered before, and which we can solve by partial eigen-decomposition.

Various choices of $\mu\geq\lambda_+$ are possible. The simplest one is the 
trace of $H$, which is equal to four times the sum of the weights.The
original version of ELEGANT used this trace bound. A sharper bound, which involves only slightly more work, is the maximum row sum of the absolute values of the elements of $H$. This is equal to four times the maximum row sum of the weights $W$. The best choice for $\mu$, of course, is $\mu=\lambda_+$, which requires more computation in the general case. For really large examples
matrices of order $O(n^2)$ may be prohibitively large, and the maximum absolute
value bound can be computed without actually computing and storing $H$.

If all weights are equal to one matters simplify. @tbl-bounds 
gives the three bounds for various values of $n$.
```{r vtable, echo = FALSE}
#| label: tbl-bounds
k <- c(2, 4, 8, 16, 32, 64, 128)
x <- matrix(0, 7, 4)
x[, 1] <- k
x[, 2] <- 2 * k * ( k - 1)
x[, 3] <- 4 * (k - 1)
x[, 4] <- 2 * k
knitr::kable(x, format = "pipe", digits = 5, col.names = c("n", "trc", "abs", "eig"), caption = "Eigenvalue Bounds")
```
The trace bound is $2n(n-1)$, the maximum row sum bound is $4(n-1)$. @deleeuw_groenen_pietersz_E_16m show that if all weights are one the maximum eigenvalue is $2n$. In the unfolding situation, with a rectangular $n\times m$ matrix of dissmilarities and all
$nm$ weights equal to one, the trace bound is $4nm$. The absolute value bound 
is $4\max(n,m)$ and the maximum eigenvalue is $n+m+2$. 

The largest eigenvalue of $H$, which has order $n^2$, is also the largest eigenvalue of the matrix with elements $W^\frac12A'AW^\frac12$, which is a non-negative matrix of order $\frac12n(n-1)$. For general weights we use
the algorithm of @markham_68 to compute the largest eigenvalue (the Perron-Frobenius root) of this matrix. In balanced situations, where the weights are not too different, the absolute value bound is about twice the largest eigenvalue, and it is unclear if the additional iterative computation of the largest eigenvalue is warranted. Moreover since the iterations approximate 
the eigenvalue from below we need precise computations, otherwise we
do not have an appropriate majorization. 

## ALSCAL

In ALSCAL we use cyclic coordinate descent to minimize sstress. The alternating least squares algorithm changes one coordinate at a time, keeping the other $np-1$ coordinates fixed at their current values. Each cycle changes all coordinates in this way, thus generating a decreasing and convergent sequence of sstress values. 

The original ALSCAL algorithm (@takane_young_deleeuw_A_77) used alternating
least squares, but  with $n$ blocks of size $p$, using a safeguarded version of Newton's method in each block subproblem. Even at the time (almost 50 years ago now) I pushed for using $np$ blocks of size one, but I lst out initially. But pretty soon afterwards Doug Carroll and
others pointed out that the original algorithm has the problem that the
Newton method may converge to a block local minimum (@takane_young_deleeuw_A_77, page 61-63). So in subsequent versions of ALSCAL in SAS and SPSS (@young_takane_lewyckyj_78b) coordinate descent
was used.

Formally changing a single coordinate is
$$
X=Y+\theta e_ke_s',
$${#eq-xchange}
which makes $d_{ij}^2(X)$ the following quadratic function of $\theta$. 
$$
d_{ij}^2(\theta)=
d_{ij}^2(Y)+2\theta (y_{is}-y_{js})(\delta^{ik}-\delta^{jk})+\theta^2(\delta^{ik}+\delta^{jk}),
$${#eq-dchange}
Note there are superscripted 
Kronecker deltas such as $\delta^{ik}$ and subscripted dissimilarity deltas such as $\delta_{ij}$.

Define the residual 
$$
r_{ij}(Y):=d_{ij}^2(Y)-\delta_{ij}^2.
$${#eq-sstressresidual}
Then
$$
\sigma(\theta)=
\sum_{i=1}^n\sum_{j=1}^nw_{ij}(r_{ij}(Y)+2\theta(y_{is}-y_{js})(\delta^{ik}-\delta^{jk})+\theta^2(\delta^{ik}-\delta^{jk})^2)^2
$${#eq-sstresstheta}
Now expand @eq-sstresstheta. This was also done undoubtedly by
@young_takane_lewyckyj_78b, and explicitly for R version in @deleeuw_U_06i, with a mistake in his formula for $a_3(X)$ on page 3. 
We use the fact that if $i\not= j$ then
\begin{subequations}
\begin{align}
(\delta^{ik}-\delta^{jk})^2&=\delta^{ik}+\delta^{jk},\\
(\delta^{ik}-\delta^{jk})(\delta^{ik}+\delta^{jk})&=\delta^{ik}-\delta^{jk},\\
(\delta^{ik}+\delta^{jk})^2&=\delta^{ik}+\delta^{jk}.
\end{align}
\end{subequations}
Expanding gives
$$
\sigma(\theta)=\sigma(Y)+\sigma_1(Y)\theta+\sigma_2(Y)\theta^2+\sigma_3(Y)\theta^3+\sigma_4(Y)\theta^4,
$${#eq-sigmaexpansion}
with
\begin{subequations}
\begin{align}
\sigma_1(Y)&=8\sum_{j=1}^nw_{kj}r_{kj}(Y)(y_{ks}-y_{js}),\\
\sigma_2(Y)&=4\sum_{j=1}^nw_{kj}r_{kj}(Y)+
8\sum_{j=1}^nw_{kj}(y_{ks}-y_{js})^2,\\
\sigma_3(Y)&=8\sum_{j=1}^nw_{kj}(y_{ks}-y_{js}),\\
\sigma_4(Y)&=2\sum_{j=1}^nw_{kj}.
\end{align}
\end{subequations}
This non=negative bowl-shaped quartic is minimized to find the optimum coordinate replacement of $x_{ks}$ in @eq-xchange. Then go to the next
coordinate, and so on. Convergence is tested after each cycle.
  
\sectionbreak

# Example

We use the data from @ekman_54, which has a very good MDS fit in two dimensions,
with apparently very few local minima.

Comparisons of the various ELEGANT options are done with the microbenchmark package (@mersmann_23), using 100 runs and the default iteration options. 
First we look at the unweighted case.  varies the three bounds
on the largest eigenvalue and the initial configuration (zero is random, one
is maximum sum).

```{r mbelegant1, echo = FALSE, cache = TRUE}
ekmat <- as.matrix(1 - ekman)
microbenchmark(
  smacofElegant(ekmat, bnd = 0, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 0, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 1, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 1, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 2, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 2, xold = 1, itmax = 100000, verbose = FALSE))
```


As expected, bound 2 (the eigenvalue) is about twice as fast as bound 1 (maximum
absolute row sum). The original ELEGANT, with bound 0, is indeed very slow. The maximum sum initial configuration gives another speedup of around two. 

Next, we do the same comparison with weights $w_{ij}=\delta_{ij}^{-1}$. 

```{r mbelegant2, echo = FALSE, cache = TRUE}
ekmat <- as.matrix(1 - ekman)
wmat <- 1 / (ekmat + diag(14)) - diag(14)
microbenchmark(
  smacofElegant(ekmat, wght = wmat, bnd = 0, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, wght = wmat, bnd = 0, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, wght = wmat, bnd = 1, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, wght = wmat, bnd = 1, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, wght = wmat, bnd = 2, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, wght = wmat, bnd = 2, xold = 1, itmax = 100000, verbose = FALSE))
```

Using these reciprocal weights slows down everything, but the ranking of the
various options remains the same, except for the fact that the maximum sum
initial configuration does not give much of an improvement. Although
bound equal to two now involves one-time computation of the largest
eigenvalue of $H$ it still is twice as fast as bound equal to one. But
remember that for larger examples 

 

```{r mbelegan3, echo = FALSE, cache = TRUE}
grmat <- as.matrix(gruijter)
microbenchmark(
  smacofElegant(grmat, bnd = 0, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(grmat, bnd = 0, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(grmat, bnd = 1, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(grmat, bnd = 1, xold = 1, itmax = 100000, verbose = FALSE),
  smacofElegant(grmat, bnd = 2, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(grmat, bnd = 2, xold = 1, itmax = 100000, verbose = FALSE))
```

We also used the Ekman data for a comparison of unweighted ELEGANT, with the eigenvalue bound, and unweighted ALSCAL. If we used a random initial $x$ then microbenchmark gives a median time of 19.56022 milliseconds for ALSCAL versus 11.65208 for ELEGANT. Using the maximum sum initial $x$ changes this to 15.611836
and 7.121864.


```{r mbelegant4, echo = FALSE, cache = TRUE}
microbenchmark(
  smacofElegant(ekmat, bnd = 2, xold = 0, itmax = 100000, verbose = FALSE),
  smacofElegant(ekmat, bnd = 2, xold = 1, itmax = 100000, verbose = FALSE),
  smacofALSCAL(ekmat, x = 0, itmax = 100000, verbose = FALSE),
  smacofALSCAL(ekmat, x = 1, itmax = 100000, verbose = FALSE))
```

\sectionbreak


# References
