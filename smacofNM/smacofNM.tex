% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
    \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{yfonts}
\usepackage{bm}
\usepackage{titlesec}
\usepackage{kbordermatrix}


\newtcolorbox{greybox}{
  colback=white,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
\newcommand{\sectionbreak}{\clearpage}

 
\newcommand{\ds}[4]{\sum_{{#1}=1}^{#3}\sum_{{#2}=1}^{#4}}
\newcommand{\us}[3]{\mathop{\sum\sum}_{1\leq{#2}<{#1}\leq{#3}}}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\amin}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\amax}[1]{\mathop{\text{argmax}}_{#1}}

\newcommand{\ci}{\perp\!\!\!\perp}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newcommand{\eps}{\epsilon}
\newcommand{\lbd}{\lambda}
\newcommand{\alp}{\alpha}
\newcommand{\df}{=:}
\newcommand{\am}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\ls}[2]{\mathop{\sum\sum}_{#1}^{#2}}
\newcommand{\ijs}{\mathop{\sum\sum}_{1\leq i<j\leq n}}
\newcommand{\jis}{\mathop{\sum\sum}_{1\leq j<i\leq n}}
\newcommand{\sij}{\sum_{i=1}^n\sum_{j=1}^n}
	
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdfauthor={Jan de Leeuw - University of California Los Angeles},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Smacof at 50: A Manual\\
Part 2: Non-metric Smacof}
\author{Jan de Leeuw - University of California Los Angeles}
\date{Started March 30 2024, Version of April 12, 2024}

\begin{document}
\maketitle
\begin{abstract}
TBD
\end{abstract}

{
\setcounter{tocdepth}{4}
\tableofcontents
}
\textbf{Note:} This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The code files can be found at
\url{https://github.com/deleeuw/smacofCode}, the manual files at
\url{https://github.com/deleeuw/smacofManual}, and the example files
at \url{https://github.com/deleeuw/smacofExamples}.

\sectionbreak

\section{Introduction}\label{introduction}

pick and rank

\section{Loss Function}\label{loss-function}

We start with a general non-metric MDS problem. The formulas for the general case are simpler than those for the various special cases implemented
in smacofNM.

The loss function is
\begin{equation}
\sigma(X,\Delta_1,\cdots,\Delta_s):=\sum_{r=1}^R\sigma_r(X,\Delta_r),
\label{eq:stressdef}
\end{equation}
with
\begin{equation}
\sigma_r(X,\Delta_r):=\sum_{i=1}^n\sum_{j=1}^n w_{ijr}(\delta_{ijr}-d_{ij}(X))^2.
\label{eq:rstressdef}
\end{equation}
As usual, the symbol \(:=\) is used for definitions.

The \emph{weights} \(W_r=\{w_{ijr}\}\) are known non-negative
numbers and \(D(X):=\{d_{ij}(X)\}\) is a matrix of Euclidean distances
between the rows of the \(n\times p\) \emph{configuration} \(X=\{x_{is}\}\), which are interpreted as \(n\) points in \(\mathbb{R}^p\).

Loss function \eqref{eq:stressdef} must be minimized over \emph{configurations} \(X\) and over the \(K\) matrices of \emph{disparities} \(\Delta_r=\{\delta_{ijr}\}\). The minimization problem has some constraints, both on \(X\) and on the \(\Delta_r\).
We require that \(X\in\mathcal{X}\subseteq\mathbb{R}^{n\times p}\). Here \(\mathcal{X}\) is the set of \emph{column-centered} (columns add up to zero) \(n\times p\) matrices that are \emph{normalized} by
\begin{equation}
\sum_{i=1}^n\sum_{j=1}^n w_{ij}^\star d_{ij}^2(X))=1,
\label{eq:xscale}
\end{equation}
where
\begin{equation}
w_{ij}^\star:=\sum_{r=1}^R w_{ijr}.
\label{eq:wstardef}
\end{equation}

The \emph{disparities} \(\Delta_r=\{\delta_{ijr}\}\) are required to satisfy \(\Delta_r\in\mathcal{C}_r\). The \(\mathcal{C}_r\) are polyhedral convex cones, which are subcones of the cone of non-negative matrices. Each of the cones is defined by partial orders \(\leq_r\) over the elements of the \(\Delta_r\). In general, neither the (known) weight matrices \(W_r\) nor the (unknown) disparity matrices \(\Delta_r\) need to be symmetric and/or hollow (i.e.~have zero diagonal).

The \emph{data} of the MDS problem are the weights \(W_r\) and the cones \(\mathcal{C}_r\). Each pair \((W_r,\mathcal{C}_r)\) is called a \emph{slice} of the data. The \emph{unknowns} or \emph{parameters} of the problem are \(X\in\mathcal{X}\) and the \(\Delta_r\in\mathcal{C}_r\).

\section{Algorithm}\label{algorithm}

\subsection{Initial Configuration}\label{initial-configuration}

In smacofBS and smacofAC the default initial configuration is the
classical Torgerson metric MDS solution. That is not available for
smacofNM, because there are no numerical dissimilarities. In
De Leeuw (1970) (section 5.1) and De Leeuw (1974) (chapter 4) a different eigenvalue-eigenvector based initial solution is proposed. We discuss a somewhat modernized version here. It is sometime known as the
\emph{maximum sum method}.

Let us interpret our data as an order over a number of pairs of pairs. For purposes of the initial configuration triads, propellors, and rank orders are also coded as pairs of pairs.
Consider
\begin{equation}
\omega(X):=\mathop{\sum\sum}_{(i,j)\prec(k,l)}(d_{ij}^2(X)-d_{kl}^2(X)),
\label{eq:omegadef}
\end{equation}
where \((i,j)\prec(k,l)\) means that we want \(d_{ij}(X)\leq d_{kl}(X)\).
We want all terms in \(\omega(X)\) to be positive, but for purposes of
the initial configuration we relax this to wanting their sum to be
large. This explains the ``maximum sum'' label.

The sum \eqref{eq:omegadef} can be written as the quadratic form
\(\omega(X)=\text{tr}\ X'A^\star X,\)
with
\begin{equation}
A^\star:=\left\{\sum_{(i,j)}\sum_{(k,l)}(A_{ij}-A_{kl})\right\} 
\label{eq:astardef}
\end{equation}
Note that \(A^\star\) is symmetric and doubly-centered. Moreover
its trace is zero, and consequently it has one zero, some negative,
and some positive eigenvalues.

Because \(X\) is column centered we have
\(\omega(X)=\text{tr}\ X'(A^\star + \theta J)X\) where \(J\) is the
centering matrix \(I-n^{-1}ee'\) and \(\theta\) is arbitrary. For
the non-zero eigenvalues we have
\(\lambda_s(A^\star + \theta J))=\lambda_s(A^\star) + \theta,\)
and thus for \(\theta\geq-\lambda_{\text{min}}(A^\star)\) the matrix \(A^\star + \theta J\) is positive semi-definite.

Of course maximizing \(\omega\) over all \(X\) does not make sense, because
by making \(X\) larger we make \(\omega\) larger. Thus the supremum over all
\(X\) is \(+\infty\) and the maximum is not attained. We need some
kind of normalization. The obvious choice is \(\text{tr}\ X'X=1\), but
unfortunately that does not work. It gives a rank-one solution
with all columns of \(X\) equal to the eigenvector corresponding with the
dominant eigenvalue of \(A^\star\). Instead we use \(\text{tr}\ (X'X)^2=1\).
This gives the solution \(X=K\Lambda^\frac12\) with \(\Lambda\) the largest \(p\) eigenvalues of \(A^\star\) (assumed to be non-negative) and \(K\) the
corresponding normalized eigenvectors. This is our version of
the Torgerson initial solution for non-metric MDS.

We have been deliberately vague about what to do if the number of
positive eigenvalues of \(A^\star\) is less than \(p\), which is of
course a problem the Torgerson metric MDS solution has as well. In the
program we simply choose \(\theta\) equal to \(-\lambda_p(A^\star)\)
if \(\lambda_p(A^\star)<0\). We expect the problem to be rare, and
the actual choice of \(\theta\) to be fairly inconsequential.

\subsection{The Iterations}\label{the-iterations}

As in other smacof implementations our minimization method is based on the \emph{alternating least squares} principle. Start
iteration \(\nu=0\) with the initial normalized configuration \(X^{(0)}\). We do not need an initial \((\Delta_1^{(0)},\cdots,\Delta_1^{(0)})\). Then in each iteration \(\nu\) we improve the configuration and the disparities by solving two subproblems. They are
\begin{align}
\text{Step  }\nu.1:\quad&\text{For }k=1,\cdots,K\text{ set } \Delta_r^{(\nu+1)}=\mathop{\text{argmin}}_{\Delta_r\in\mathcal{C}_r}\sigma_r(X^{(\nu)},\Delta_r),\label{eq:alsprob1}\\
\text{Step  }\nu.2:\quad&\text{select }X^{(\nu+1)}\text{ from }\mathop{\text{argmin}}_{X\in\mathcal{X}}\sigma(X,\Delta_1^{(\nu+1)},\cdots,\Delta_s^{(k+1)}).\label{eq:alsprob2}
\end{align}

\subsection{First Subproblem}\label{first-subproblem}

In formulating the first subproblem \eqref{eq:alsprob1} we have used the fact that the projections on the cones \(\mathcal{C}_r\) can be carried out separately for each \(k\). We also know that the projections exist and are unique.

\subsection{Second Subproblem}\label{second-subproblem}

To minimize loss over \(X\in\mathcal{X}\) for the current best value of the \(\Delta_r\). This subproblem is simplified by using the least squares partitioning
\begin{equation}
\sigma(X,\Delta_1,\cdots,\Delta_s)=\sum_{r=1}^R\sum_{i=1}^n\sum_{j=1}^nw_{ijr}(\delta_{ijr}-\delta_{ij}^\star)^2+\sum_{i=1}^n\sum_{j=1}^nw_{ij}^\star(\delta_{ij}^\star-d_{ij}(X))^2,
\label{eq:stresspart}
\end{equation}
where
\begin{equation}
\delta_{ij}^\star=\frac{\sum_{r=1}^R w_{ijr}\delta_{ijr}}{\sum_{r=1}^R w_{ijr}}.
\label{eq:deltastardef}
\end{equation}

Minimizing over \(X\) can be done by minimizing the second term on the right in
\eqref{eq:stresspart}, which is a standard metric smacof problem. But unfortunately there is no closed form solution for this problem, and the
minimizer must be computed by an infinite iterative process. Of course
we do not want to have an infinite iterative process with the infinite alternating least square process, and thus we truncate the minimizations
in the second subproblem. Although the resulting algorithm is not
strictly of the form \eqref{eq:alsprob1}-\eqref{eq:alsprob2} any more, it does
decrease the loss function in each iteration and consequently
produces a stable and convergent algorithm. Since the smacof iterations
use majorization (or MM), our overall algorithm combines alternating
least squares and majorization.

\section{Special Cases}\label{special-cases}

\subsection{Slice-Independence}\label{slice-independence}

We will assume throughout that \(w_{ijr}=w_{ij}\epsilon_{ijr}\), where
\(\epsilon_{ijr}=1\) is either zero or one. If \(\epsilon_{ijr}\) is one, we say that pair \((i,j)\) \emph{participates} in slice \(r\). Thus \(\epsilon_{ijr}=0\) for all pairs that do not particpate. We refer to the assumption \(w_{ijr}=w_{ij}\epsilon_{ijr}\) on the weights as the \emph{slice-independent} case.

To see the consequences of slice-independence for our equations
we define
\begin{equation}
\mathcal{I}_r:=\{(i, j)\mid \epsilon_{ijr}= 1\}
\label{eq:irdef}
\end{equation}
so that
\begin{equation}
\sigma_r(X,\Delta_1,\cdots,\Delta_s)=\sum_{(i,j)\in\mathcal{I}_r} w_{ij}(\delta_{ijr}-d_{ij}(X))^2
\label{eq:stressdefred}
\end{equation}

Equation \eqref{eq:wstardef} gives \(w_{ij}^\star=w_{ij}\epsilon_{ij}^\star,\)
where \(\epsilon_{ij}^\star\) is the number of times pair \((i,j)\) occurs in
the \(R\) slices. A set of slices is \emph{balanced} if all \(\epsilon_{ij}^\star\) are equal. Also, from \eqref{eq:deltastardef},
\begin{equation}
\delta_{ij}^\star=\frac{\sum_{r=1}^R \epsilon_{ijr}\delta_{ijr}}{\sum_{r=1}^R \epsilon_{ijr}}
\label{eq:deltastarsimp}
\end{equation}
which does not depend on the \(w_{ij}\).

It follows that in our computations we have to deal with various
different sets of weights. There are the \(w_{ijr}\), the \(w_{ij}^\star\),
the \(w_{ij}\), and the \(\epsilon_{ij}^\star\). In the first ALS subproblem
where we minimize over the \(\Delta_r\) for fixed \(X\) we use
equation \eqref{eq:stressdefred}, i.e.~we use the \(w_{ij}\). If we minimize over \(X\) for fixed \(\Delta_r\) we use \eqref{eq:stresspart}, which means we
use \(w_{ij}^\star=w_{ij}\epsilon_{ij}^\star\) and \(\delta_{ij}^\star\)
given by \eqref{eq:deltastarsimp}. Of course this all simplifies dramatically
if \(w_{ij}=1\) for all pairs \((i,j)\) (the \emph{unweighted} case).

\subsection{Rank Order}\label{rank-order}

Ordinary non-metric multidimensional scaling is the special case in which
there is only one slice (\(R=1\)) and the cone \(\mathcal{C}\) is the isotone cone (possibly with provisions for ties). We have \(\epsilon_{ij}=\epsilon_{ij}^\star=1\) for all
\((i,j)\), and thus \(w_{ij}^\star=w_{ij}\). There is only a single sets of weights we have to deal with, same as in smacofBS and smacofAC.

Finding the optimum \(\Delta\) for given \(X\) is a single monotone regression
problem, possibly using the primary or secondary approach to ties (De Leeuw (1977)).

\subsection{Paired Comparisons}\label{paired-comparisons}

THe paired comparison method of data collection is the simplest and
the most basic one of the cartwheel methods.

Positive Orthant / Absolute Value / Pairwise

De Leeuw (1970)
De Leeuw (2018)
Hartmann (1979)
Guttman (1969)
Johnson (1973)

Suppose datum \(r\) says that that \((i,j)\prec(k,l)\). In the slice independent case \(w_{ijr}=w_{ij}\) and \(w_{klr}=w_{kl}\)
can be non-zero and all other elements of \(W_r\) are zero.

Use \(w_{(i,j)_r}\)
Thus
\[
w_{ij}(\delta_{ijr}-d_{ij})^2+w_{kl}(\delta_{klr}-d_{kl})^2
\]
Must be minimized over \(\delta_{ijr}\leq\delta_{klr}\). If \(d_{ij}\leq d_{kl}\)
then \(\hat d_{ijr}=d_{ij}\) and \(\hat d_{klr}=d_{kl}\), and otherwise
\[
\hat d_{ijr}=\hat d_{klr}=\frac{w_{ij}d_{ij}+w_{kl}d_{kl}}{w_{ij}+w_{kl}}
\]
Thus

\[w_{ij}(\hat d_{ijr}-d_{ij})^2+w_{kl}(\hat d_{klr}-d_{kl})^2\]
is zero if the order of \(d_{ij}\) and \(d_{kl}\) is the same as the order in the data
and

\[
\frac{w_{ij}w_{kl}}{w_{ij}+w_{kl}}(d_{ij}-d_{kl})^2
\]
Also

\[w_{ij}(\hat d_{ijr}-d_{ij})^2+w_{kl}(\hat d_{klr}-d_{kl})^2=\]
\[
w_{ij}\hat d_{ijr}^2+w_{kl}\hat d_{klr}^2+\\
-2w_{ij}\hat d_{ijr}d_{ij}-2w_{kl}\hat d_{klr}d_{kl}+
\]

So far we have only considered the forced-choice situation in which
the subject has to choose one of the two pairs. If we allow for the alternative that \((i,j)\) and \((k,l)\) are equally similar then we can choose between two different approaches. In the \emph{primary approach} we incur no loss for this pair, no matter what \(d_{ij}(X)\) and \(d_{kl}(X)\) are. In the \emph{secondary approach} we require that \(\delta_{ijr}=\delta_{klr}\) and consequently we use \ldots{} and add to the loss if
\(d_{ij}X)\not= d_{kl}(X)\).

\subsection{Triads}\label{triads}

We have implemented three different versions of the method of triads, in which stimuli are presented three at a time,
at the corners of an equilateral triangle, as in
\ldots{}

In the first one we present all
\(\binom{n}{3}=\frac16 n(n-1)(n-2)\approx\frac16n^3\)
triples of stimuli and we ask the subject to rank the
three similarities between them. More precisely we ask for the two pairs with the largest and smallest similarity,
and we interpret the responses as giving us a rank order.
Coombs (1954) calls this the \emph{method of similarities},
and Torgerson (1958) calls it the \emph{complete method of triads}.

\subsection{Propellors}\label{propellors}

The second method was first proposed by Richardson (1938),
as the \emph{method of triadic combinations}. Every triad is presented three times using a layout that is slightly dfferent from the complete method of triads.
\ldots{}
We ask the subject which one of the top stimuli is most similar to the bottom stimulus. This requires
\(n\binom{n-1}{2}=\frac12n(n-1)(n-2)\approx\frac12n^3\)
presentations for a complete set. Since there is only
one comparision involved, this is a special case of
the paired comparisons method, in which the pairs always
have exactly one stimulus in common. Coombs (1954) call this the \emph{method of propellors} because we have only drawn lines from the bottom stimulus (the ``hub'') to the two stimuli at the top.

It is clear that the judments by the subjects in the each presentation of the complete method of triads can also be
coded as three paired comparisons. We could then us the
smacof method for paired comparison for the data thus
generated. But this ignores the information that stimuli were presented in triples. It also has the problem that
if we code the data as paired comparisons then we could also code only use only two pairs of pairs per presentation and still have the same information.

Of course if the number of stimuli is at all large
then the number of triads is too large for a subject to handle, even if they are highly motivated, paid, or
undergraduate. In this case we could use either
a random sample from the set of all triads, or a
balanced design, as in Levelt, Van De Geer, and Plomp (1966).

\subsection{Conditional Rank Orders -- Klingberg}\label{conditional-rank-orders-klingberg}

\section{Data Collection Programs}\label{data-collection-programs}

\subsection{Data Format}\label{data-format}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-coombs_54}
Coombs, C. H. 1954. {``A Method for the Study of Interstimulus Similarity.''} \emph{Psychometrika} 19: 183--94.

\bibitem[\citeproctext]{ref-deleeuw_R_70a}
De Leeuw, J. 1970. {``The Positive Orthant Method for Nonmetric Multidimensional Scaling.''} Research Report 001-70. Leiden, The Netherlands: Department of Data Theory FSW/RUL.

\bibitem[\citeproctext]{ref-deleeuw_B_74}
---------. 1974. \emph{Canonical Analysis of Categorical Data}. Leiden, The Netherlands: Psychological Institute, Leiden University.

\bibitem[\citeproctext]{ref-deleeuw_A_77}
---------. 1977. {``Correctness of Kruskal's Algorithms for Monotone Regression with Ties.''} \emph{Psychometrika} 42: 141--44.

\bibitem[\citeproctext]{ref-deleeuw_E_18d}
---------. 2018. {``{The Positive Orthant Method }.''} 2018.

\bibitem[\citeproctext]{ref-guttman_69}
Guttman, L. 1969. {``{Smallest Space Analysis by the Absolute Value Principle}.''} In \emph{{Proceedings of the XIX International Congress of Psychology, London}}.

\bibitem[\citeproctext]{ref-hartmann_79}
Hartmann, W. 1979. \emph{{Geometrische Modelle zur Analyse empirischer Data}}. Akademie Verlag.

\bibitem[\citeproctext]{ref-johnson_73}
Johnson, R. M. 1973. {``{Pairwise Nonmetric Multidimensional Scaling}.''} \emph{Psychometrika} 38 (12--18).

\bibitem[\citeproctext]{ref-levelt_vandegeer_plomp_66}
Levelt, W. J. M., J. P. Van De Geer, and R. Plomp. 1966. {``{Triadic Comparions of Musical Intervals}.''} \emph{British Journal of Mathematical and Statistical Psychology} 19: 163--79.

\bibitem[\citeproctext]{ref-richardson_38}
Richardson, M. W. 1938. {``Multidimensional Psychophysics.''} \emph{Psychological Bulletin} 35: 659--60.

\bibitem[\citeproctext]{ref-torgerson_58}
Torgerson, W. S. 1958. \emph{{Theory and Methods of Scaling}}. New York: Wiley.

\end{CSLReferences}

\end{document}
