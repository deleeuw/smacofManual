---
title: |
    | Smacof at 50: A Manual
    | Part 5: Unfolding in Smacof
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started December 12 2022, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex 
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 3
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 3
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: TBD
---
```{r loadpackages, echo = FALSE}
#suppressPackageStartupMessages (library (foo, quietly = TRUE))
```

```{r load code, echo = FALSE}
#dyn.load("foo.so")
#source("janUtil.R")
```


**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw> in the repositories smacofCode, smacofManual,
and smacofExamples.

# Introduction

In Multidimensional Unfolding (MDU) the objects of an MDS problem are partitioned into two sets.
There is a set of $n$ row-objects and a set of 
$m$ column-objects, and a corresponding $n\times p$ row-configuration $X$ and $m\times p$ column-configuration
$Y$. We minimize stress defined as
\begin{equation}
\sigma(X,Y):=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(\delta_{ij}-d(x_i,y_j)^2.
(\#eq:ufstress)
\end{equation}
over both $X$ and $Y$. Here
\begin{equation}
d(x_i,y_j):=\sqrt{(x_i-y_j)'(x_i-y_j)}
(\#eq:ufdist)
\end{equation}
Thus the within-set
dissimilarities are missing, or ignored even if they are available, and only the between-set dissimilarities are fitted by between-set distances. 
 
If we define $Z$ as
\begin{equation}
Z:=
\kbordermatrix{
\mbox{\ }&p\\
n&X\\
m&Y
}
(\#eq:ufzdef)
\end{equation}
and $U$ as
\begin{equation}
U:=
\kbordermatrix{
&n&m\\
n&0&W\\
m&W'&0
}
(\#eq:ufudef)
\end{equation}
then we can also write
\begin{equation}
\sigma(Z)=\sum_{i=1}^{n+m}\sum_{j=1}^{n+m}u_{ij}(\delta_{ij}-d_{ij}(Z))^2.
(\#eq:ufzstress)
\end{equation}

Data Preferences Type A and Type B Conditional

The unfolding model for preference judgments is often attributed to @coombs_50,
with further developments by Coombs and his co-workers reviewed in @coombs_64.
After this path-breaking work the digital computer took over, and minimization of loss function
\@ref(eq:ufstress) and its variations was started by 
@roskam_68 and @kruskal_carroll_69.

In this manual we are not interested in MDU as a psychological theory, as a model for
preference judgments. We merely are interested in mapping off-diagonal dissimilarity
relations into low-dimensional Euclidean space, i.e. in making a picture of the
data. In some cases (distance completion, distances with errors, spatial basis)

Challenges: degeneracy

# Loss function

## Metric

## Non-linear

## Non-metric

## Constraints

# smacofUF

## Initial Configuration

$$
\sigma(C)=\sigma(\tilde C+(C-\tilde C))=\sum_{i=1}^n\sum_{j=1}^m((\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)-\text{tr}\ A_{ij}(C-\tilde C))^2
$$
$$
\sigma(C)=\sigma(\tilde C)-2\sum_{i=1}^n\sum_{j=1}^m(\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)\text{tr}\ A_{ij}(C-\tilde C)+\sum_{i=1}^n\sum_{j=1}^m\{\text{tr}\ A_{ij}(C-\tilde C)\}^2
$$

From @deleeuw_groenen_pietersz_U_06
$$
\sum_{i=1}^n\sum_{j=1}^m\{\text{tr}\ A_{ij}(C-\tilde C)\}^2\leq (n+m+2)\text{tr}\ (C-\tilde C)^2
$$
Define 
$$
B(\tilde C):=\frac{1}{n+m+2}\sum_{i=1}^n\sum_{j=1}^m(\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)A_{ij}
$$
So we minimize
$$
-2\ \text{tr}\ B(\tilde C)C+\text{tr}\ C^2-2\text{tr}\ C\tilde C=\\
\text{tr}\ (C-\{\tilde C + B(\tilde C)\})^2
$$

## Constraints

### Centroid Restriction

$$
Z=\begin{bmatrix}
X\\Y
\end{bmatrix}=
\begin{bmatrix}
I\\
D^{-1}G'
\end{bmatrix}X=HX
$$
Minimize $\text{tr}\ (\overline{Z}-HX)'V(\overline{Z}-HX)$. If there are
no further restrictions on $X$ the minimum is attained at $\hat X=(H'VH)^+H'V\overline{Z}$.
Otherwise write $X=\hat X+(X-\hat X)$. Then
$$
\text{tr}\ (\overline{Z}-H\hat X-H(X-\hat X))'V(\overline{Z}-H\hat X-H(X-\hat X))=\\
\text{tr}\ (\overline{Z}-H\hat X)'V(\overline{Z}-H\hat X)+
\text{tr}\ (X-\hat X)'H'VH(X-\hat X)
$$
and we must minimize $\text{tr}\ (X-\hat X)'H'VH(X-\hat X)$ for example over $X'H'VHX=I$.
That is maximizing $H'VH\hat X=H'VHXM$ with $M$ a symmetric matrix of Lagrange multipliers.
Thus $M^2=\hat X'H'VH\hat X$ and $X=\hat X(\hat X'H'VH\hat X)^{-\frac12}$. 

Over $\text{tr}\ X'H'VHX=1$ we get $H'VH(X-\hat X)=\lambda H'VHX$ or 
$X=(1-\lambda)H'VHX=H'VH\hat X$, wich means $X$ is proportional to $\hat X$ and we just have to normalize $\hat X$ to find $X$.

The constraint $X'V_{11}X=I$ is more difficult to deal with 
$$
\text{tr}\ (X-\hat X)'H'VH(X-\hat X)=
$$

Oblique Procrustus

### Linearly Restricted Unfolding

#### External Unfolding

$$
\begin{bmatrix}
X\\Y
\end{bmatrix}=
\begin{bmatrix}
R&0\\
0&S
\end{bmatrix}
\begin{bmatrix}
P\\
Q
\end{bmatrix}
$$
### Normalization Restrictions

$X'V_{11}X=I$.

### Rank-one Restriction

$Y=za'$

Minimize
$$
2\ \text{tr}\ (za'-\overline{Y})'V_{21}(X-\overline{X})+
\text{tr}\ (za'-\overline{Y})'V_{22}(za'-\overline{Y})
$$
Removing irrelevant terms
$$
2\ z'\{V_{21}(X-\overline{X})-V_{22}\overline{Y}\}a+
a'a.z'V_{22}z
$$
Let $H=-V_{21}(X-\overline{X})-V_{22}\overline{Y}$. Minimize using $z'V_{22}z=1$.
$$
a=H'z=\{\overline{Y}'V_{22}-(X-\overline{X})'V_{12})\}z
$$

# Examples 


## Roskam

## Breakfast

## Gold

## Indicator matrix / Matrices


# References
