---
title: |
    | Smacof at 50: A Manual
    | Part x: Non-linear smacof with power functions
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started May 14, 2024, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex 
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 3
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 3
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
---
```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(numDeriv, quietly = TRUE))
```

```{r load code, echo = FALSE}
source("gruijter.R")
source("wish.R")
source("smacofPO.R")
data(ekman, package = "smacof")
ekman <- 1 - ekman
data(morse, package = "smacof")
```


**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The various files can be found at
<https://github.com/deleeuw> in the smacofPO directory of the repositories smacofCode, smacofManual, and smacofExamples. 

# Introduction

In least squares MDS we minimize *stress*, defined as 
\begin{equation}
\sigma(X, r):=\frac12\sum\sum w_{ij}(\hat d_{ij}-d_{ij}(X))^2.
(\#eq:stressdef)
\end{equation}
over the *configurations* $X\in\mathfrak{X}\subseteq\mathbb{R}^{n\times p}$ and over the *disparities* $\hat D\in\mathfrak{D}\subseteq\mathbb{R}^{n\times n}$.
(The symbol $:=$ is used for definitions). Assume, without loss of generality,
that the *weights* $w_{ij}$ add up to one. The double summations in the 
definion of stress are always over the elements below the diagonal of the
symmetric matrices $\hat D$ and $D$.

In metric MDS the set of disparities $\mathfrak{D}$ is the singleton $\{\Delta\}$, with $\Delta$ the
observed *dissimilarities*. In non-metric MDS $\mathfrak{D}$ is the set of all
monotone transformations of $\Delta$, and in non-linear MDS it is the set of all
monotone polynomial or splinical transformations. There are some less familiar alternatives. In *additive constant* MDS $\mathfrak{D}$ is the set of all
$\hat D$ of the form $\Delta+\alpha(E-I)$, where $I$ is the identity and
$E$ has all elements equal to one. In *interval* MDS we require
$\Delta_-\leq\hat D\leq\Delta_+$ elementwise, where $\Delta_-$ and 
$\Delta_+$ are two given matrices of *disparity bounds*.

In this chapter we study and implement another set $\mathfrak{D}$,
the set of all $\Delta^r$, the elementwise powers of the dissimilarities.
This definition has some advantages and some disadvantages. Polynomials are often critisized as suitable for approximation because of their rigitidy. The values of
a polynomial in an interval, however small, determine the shape of the
polynomial on the whole real line. This is one of the reasons for the
popularity of splines, which are piecewise polynomials joined with a certain degree of smoothness at the knots. Splines are also popular because of their generality: 
polynomials on an interval are splines without interior knots, while step functions splines of degree zero. 

The set of all monotone functions for $\mathfrak{D}$,
as in the original non-metric proposals of @kruskal_64a and @guttman_68,
provides a great deal of flexibility. As the case of non-metric unfolding
shows there can be too much flexibility, leading to perfect but trivial
solutions of the MDS problem.

In terms of flexibility power MDS studied in this paper performs badly. There
is only one single parameter that completely determines the shape of the function on the non-negative real line. But this rigidity can also be seen as an advantage. If the
power function fits the data well then it will presumably be quite stable
under small perturbations of the data. There are other advantages. Power functions $x^r$ have some
nice properties: they always start at the origin and they are monotone,
either increasing or decreasing depending on the values of $x$ and $r$. Moreover for positive powers they are convex, for
negative powers they are concave. In psychophysics power functions are
prominent because of the work of @stevens_57 and @luce_59. And, perhaps most
importantly, in many cases non-metric and non-linear MDS compute optimal
transformations that look a lot like power functions, with some irregularities
that are maybe mostly due to measurement error. Verbally describing what these
optimal transformations look like often amounts to "they look like a power
function with positive exponent of about two".

# Loss Function

So let us now define stress as
\begin{equation}
\sigma(X, r):=\frac12\sum\sum w_{ij}(\delta_{ij}^r-d_{ij}(X))^2.
(\#eq:rstressdef)
\end{equation}
and consider the problem of minimizing thus stress over both configurations
$X$ and powers $r$. Throughout the chapter we follow the convention
that $0^0=1$.

The algorithm we will use is *alternating least squares (ALS)*, i.e. we alternate minimization over $X$ for the
current bext value of $r$ and minimization over $r$ for the
current best value of $X$. In this chapter we will only
consider the second *optimal scaling* phase of the ALS process, computing the optimal $r$ for given $X$, because minimizing over $X$ for fixed $r$ is a standard metric MDS problem.

Minimizing \@ref(eq:rstressdef) differs from the more familiar forms of non-linear and non-metric scaling because the optimal scaling is not positively homogeneous. The set of matrices $\mathfrak{D}=\{\hat D\mid\hat D=\Delta^r\}$ does not define a cone, let alone a convex cone. It is also worth noting that the matrix $E-I$, with all off-diagonal disparities equal to one, is in $\mathfrak{D}$ (it corresponds with $r=0$).

Minimizing \@ref(eq:rstressdef) over $r$ for given $X$ is similar to two other MDS problems. Historically the first problem is to find the Minkowski power metric that best fits a set of dissimilarities or disparities. We minimize
\begin{equation}
\sigma(X,r):=\frac12\sum\sum w_{ij}(\delta_{ij}-d_{ij}^{\{r\}}(X))^2,
(\#eq:mstressdef)
\end{equation}
with
\begin{equation}
d_{ij}^{\{r\}}(X)=\{\sum|x_{is}-x_{js}|^r\}^{1/r}.
(\#eq:minkovski)
\end{equation}
This particular problem has mainly been used in 
comparing minimum stress for the city block metric ($r=1$) and the Euclidean metric ($r=2$).

A second similar problem is minimization of a form of power stress
defined by
\begin{equation}
\sigma(X,r):=\frac12\sum\sum w_{ij}(\delta_{ij}-d_{ij}^r(X))^2.
(\#eq:pstressdef)
\end{equation}
Minimizing loss function for various values of $r$ \@ref(eq:pstressdef) has been studied by @groenen_deleeuw_U_10, @deleeuw_U_14c, @deleeuw_groenen_mair_E_16a, @deleeuw_groenen_mair_E_16h. For both power stress
and Minkovski stress mostly the minimization over $X$ for fixed
values of the power $r$ have been considered. Minimization over $r$ 
is addressed, if at all, by comparing the minimum values of stress
over $X$ for different values of $r$ and then choosing or guessing the
$r$ corresponding with the smallest value of minimum stress. See, for example,
figure 18 in @kruskal_64a. 

We can formalize this search strategy using the *marginal function*
\begin{equation}
\sigma_\star(r):=\frac12\min_X\sum\sum w_{ij}(\delta_{ij}^r-d_{ij}(X))^2.
(\#eq:marginal)
\end{equation}
Also define, for later use,
\begin{equation}
X(r):=\mathop{\text{argmin}}_X\sigma(X, r)=\{X\mid\sigma(X,r) = \sigma_\star(r)\}.
(\#eq:argmin)
\end{equation}
The idea of the search strategy is to compute the value of the marginal function at a number of values of $r$, and then interpolate to approximate the minimum over $r$. There is nothing wrong with this, but it is somewhat ad-hoc and potentially rather expensive. It also supposes, of course, that
in computing the marginal function the global minimum over $X$
for given $r$ has been found.

Zero and infinity, the extreme values of $r$, are of special interest.
For $r=0$ the situation is clear.
\begin{equation}
\sigma(X, 0):=\frac12\sum\sum w_{ij}(\hat\delta_{ij}-d_{ij}(X))^2.
(\#eq:rstressnull)
\end{equation}
with $\hat\delta_{ij}=1$. Computing $\sigma_star(0)$, i.e. minimizing $\sigma(X, 0)$ over $X$, means
fitting $p$-dimensional distances to the distance matrix of an
$(n-1)$-dimensional regular simplex. This problem has been
studied, in a different context, by @deleeuw_stoop_A_84.
They compute $\sigma_\star(0)$ and the corresponding configurations
$X(0)$ for various values of the number of objects $n$
and the number of dimensions $p$. For $n\leq 8$ the optimal
configuration has its points equally spaced on a circle, for 
$n>8$ points are equally spaced on two or more concentric
circles. Of course the minimum is far from unique, because
we can permute the points on the circles however we want
without changing stress.

If $r\rightarrow+\infty$ limit behavior depends on $\Delta$.





# Theory



## Derivatives of stress

If $f(r)=x^r$ then 
\begin{subequations}
\begin{align}
\mathcal{D}f(r)&=x^r\log x,(\#eq:der1)\\
\mathcal{D}^2f(r)&=x^r(\log x)^2(\#eq:der2).
\end{align}
\end{subequations}

It follows that

* if $x < 1$ then $f$ is decreasing,
* if $x > 1$ then $f$ in increasing,
* if $x = 1$ then $f$ is constant,
* $f$ is convex.

Now define
\begin{subequations}
\begin{align}
\eta^2(r)&:=\sum\sum w_{ij}\{\delta_{ij}^r\}^2,\\
\rho(r)&:=\sum\sum w_{ij}d_{ij}(X)\delta_{ij}^r,\\
\omega^2&:=\sum\sum w_{ij}d_{ij}^2(X),
\end{align}
\end{subequations}
so that
\begin{equation}
\sigma(r)=\frac12\{\eta^2(r)-2\rho(r)+\omega^2\}.
\end{equation}
Now

* both $\eta^2$ and $\rho$ are convex,
* if $\delta_{ij}\leq 1$ for all $(i,j)$ then both
$\eta^2$ and $\rho$ are non-increasing,
* if $\delta_{ij}\geq 1$ for all $(i,j)$ then both
$\eta^2$ and $\rho$ are non-decreasing.

Using equation \@ref(eq:der1) the first derivative of stress is
\begin{equation}
\mathcal{D}\sigma(r)=\sum\sum w_{ij}\delta_{ij}^r\log\delta_{ij}(\delta_{ij}^r-d_{ij}(X)),
(\#eq:first)
\end{equation}
and using \@ref(eq:der2)  the second derivative is
\begin{equation}
\mathcal{D}^2\sigma(r)=\sum\sum w_{ij}\delta_{ij}^r(\log\delta_{ij})^2(2\delta_{ij}^r-d_{ij}(X))
(\#eq:second)
\end{equation}
If either $\delta_{ij}\leq 1$ for all $(i,j)$ or
$\delta_{ij}\geq 1$ for all $(i,j)$ then all
quantities $w_{ij}\delta_{ij}^r\log\delta_{ij}$
have the same sign, and we see that $\mathcal{D}\sigma(r)\geq 0$
if
$$\frac{\sum\sum w_{ij}\delta_{ij}^r|\log\delta_{ij}|\delta_{ij}^r}
{\sum\sum w_{ij}\delta_{ij}^r|\log\delta_{ij}|d_{ij}^2(X)}\geq 1.
$$
Without any further conditions we have $\mathcal{D}\sigma(r)\geq 0$
if
$$\frac{\sum\sum w_{ij}\delta_{ij}^r(\log\delta_{ij})^2\delta_{ij}^r}
{\sum\sum w_{ij}\delta_{ij}^r(\log\delta_{ij})^2d_{ij}^2(X)}\geq\frac12.
$$

In a decent fit we will have for all or most $(i,j)$  
\begin{equation}
\frac{d_{ij}(X)}{\delta_{ij}^r}\leq 2,
(\#eq:decent)
\end{equation}
and thus $\mathcal{D}^2\sigma(r)\geq 0$.

In an excellent fit $\delta_{ij}^r\approx d_{ij}(X)$ and
\begin{equation}
\mathcal{D}^2\sigma(r)\approx\sum\sum w_{ij}(\delta_{ij}^r\log\delta_{ij})^2,
(\#eq:excellent)
\end{equation}
which is obviously non-negative, and can be used in a Gauss-Newton approximation
of stress.

Because of some examples we will discuss later on in this paper the derivatives
at $r=0$ are of special interest. First

$$
\mathcal{D}\sigma(0)=\sum\sum w_{ij}\log\delta_{ij}(1-d_{ij}(X)),
$$
and thus $\mathcal{D}\sigma(0)=0$ if
$$
\frac{\sum\sum w_{ij}\log\delta_{ij}d_{ij}(X)}{\sum\sum w_{ij}\log\delta_{ij}}=1.
$$
Also
$$
\mathcal{D}^2\sigma(0)=\sum\sum w_{ij}(\log\delta_{ij})^2(2-d_{ij}(X)),
$$
and thus $\mathcal{D}^2\sigma(0)\geq 0$ if
$$
\frac{\sum\sum w_{ij}(\log\delta_{ij})^2d_{ij}(X)}{\sum\sum w_{ij}(\log\delta_{ij})^2}\leq 2.
$$

## Marginal Function

Continuous

Directional derivative:

$$
\mathfrak{D}\sigma_\star(r):=\lim_{\epsilon\downarrow 0}\frac{\sigma_\star(r+\epsilon)-\sigma_\star(r)}{\epsilon}=
$$
$$
\mathfrak{D}\sigma_\star(r)=\sum\sum w_{ij}\delta_{ij}^r\log\delta_{ij}(\delta_{ij}^r-d_{ij}(X(r)))
$$

Again, the directional derivative at zero is
$$
\mathfrak{D}\sigma_\star(0)=\sum\sum w_{ij}\log\delta_{ij}(1-d_{ij}(X(0)))
$$
where $X(0)$ is now the metric MDS solution if all dissimilarities are equal to one. This configuration has been studied in detail by @deleeuw_stoop_A_84, where it is shown that for small $n$ we find $n$ points equally spaced on a circle, while for larger $n$ it becomes points equally spaced on several concentric circles.

# Algorithm

We'll use the R function optimize() to find the
optimal power $r$ for fixed $X$. Using optimize() is safe, but somewhat brute force and probably not efficient. We don't use information from previous 
iterations, so every iteration has a "cold start". Given the convexity
properties of the loss function we could probably
use a lightly safeguarded Newton method for efficiency. Also, our algorithm uses only a single Guttman transform per major iteration. Performing more Guttman iterations between upgrades of $r$ may also improve performance.

# Examples

## Artificial

We start with an aritificial example in which perfect fit is possible. Configuration $X$ consists of 10 points equally spaced on a circle. Define dissimilarities as $\delta_{ij}=d_{ij}^2(X)$. Note that for antipodal
points $\delta_{ij}$ is as large as four.
 
```{r}
s <- seq(0, 2 * pi, length = 11)
x <- cbind(sin(s), cos(s))[1:10, ]
delta <- dist(x) ^ 2
harti <- smacofPO(as.matrix(delta), itmax = 1000, verbose = FALSE)
```

Convergence in  `r harti$itel` iterations to stress `r harti$s` and power `r harti$r`. smacofPO finds the square root, the inverse of the square.

Next define $\delta_{ij}=\sqrt{d_{ij}(X)}$.

```{r harti, echo = FALSE}
s <- seq(0, 2 * pi, length = 11)
x <- cbind(sin(s), cos(s))[1:10, ]
delta <- sqrt(dist(x))
harti <- smacofPO(as.matrix(delta), itmax = 1000, verbose = FALSE)
```

Convergence in  `r harti$itel` iterations to stress `r harti$s` and power `r harti$r`. smacofPO finds the squares, the inverse of the square root.

## @ekman_54

```{r}
hzero <- smacofPO(as.matrix(ekman), interval = c(0, 0), itmax = 1000, eps = 1e-15, verbose = FALSE)
```

Stress at $r=0$ is `r sum(((as.matrix(ekman)) - 1) ^ 2)` and the right derivative of the marginal function at zero is `r sum(log(as.matrix(ekman) + diag(14)) * ( 1 - hzero$d))`. The largest $\delta_{ij}$ is `r max(ekman)` and the smallest
`r min(ekman)`.

```{r ekman, echo = FALSE}
hekman <- smacofPO(as.matrix(ekman), itmax = 1000, verbose = TRUE)
ekv <- as.vector(ekman)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hekman$d))[ord]
epv <- as.vector(as.dist(hekman$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hekman$itel` iterations to stress `r hekman$s` and power `r hekman$r`.

```{r deeper0, echo = FALSE, cache = TRUE}
r <- seq(0, 4, length = 100)
s <- rep(0, 100)
for (i in 1:100) {
if (i == 1) {
  xe <- NULL
} 
h <- smacofPO(as.matrix(ekman), xe = xe , interval = c(r[i],r[i]), itmax = 10000, verbose = FALSE)
s[i] <- h$stress
xe <- h$x
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

## @degruijter_67

```{r}
hzero <- smacofPO(1 - diag(9), interval = c(0, 0), eps = 1e-15, itmax = 10000, verbose = FALSE)
```

Stress at $r=0$ is `r sum(((as.matrix(gruijter)) - 1) ^ 2)` and the right derivative of the marginal function at zero is `r sum(log(as.matrix(gruijter) + diag(9)) * ( 1 - hzero$d))`. The largest $\delta_{ij}$ is `r max(gruijter)` and the smallest `r min(gruijter)`.


`r min(as.dist(hzero$d))`

### One

```{r ggruijter, echo = FALSE}
gmax <- max(gruijter)
gmin <- min(gruijter)
ggruijter <- (gruijter - gmin) / (gmax - gmin)
print(sum(log(as.matrix(ggruijter) + diag(9)) * (1 - hzero$d)))
hgruijter <- smacofPO(as.matrix(ggruijter), itmax = 1000, verbose = FALSE)
ekv <- as.vector(ggruijter)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hgruijter$d))[ord]
epv <- as.vector(as.dist(hgruijter$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hgruijter$itel` iterations to stress `r hgruijter$s` and power `r hgruijter$r`.

```{r godeeper, echo = FALSE, cache = TRUE}
r <- seq(1.0, 1.5, length = 99)
s <- rep(0, 99)
for (i in 1:99) {
h <- smacofPO(as.matrix(ggruijter), interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

```{r goyetdeeper, echo = FALSE, cache = TRUE}
r <- seq(1.2, 1.3, length = 99)
s <- rep(0, 99)
for (i in 1:99) {
h <- smacofPO(as.matrix(ggruijter), interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

### Two

```{r mgruijter, echo = FALSE}
gmax <- max(gruijter)
mgruijter <- gruijter / gmax
hgruijter <- smacofPO(as.matrix(mgruijter), itmax = 10000, verbose = FALSE)
print(sum(log(as.matrix(mgruijter) + diag(9)) * (1 - hzero$d)))
ekv <- as.vector(mgruijter)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hgruijter$d))[ord]
epv <- as.vector(as.dist(hgruijter$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hgruijter$itel` iterations to stress `r hgruijter$s` and power `r hgruijter$r`.

```{r evendeeper, echo = FALSE, cache = TRUE}
r <- seq(2.6, 2.7, length = 99)
s <- rep(0, 99)
for (i in 1:99) {
if (i == 1) {
  xe <- NULL
}
h <- smacofPO(as.matrix(mgruijter), xe = xe, interval = c(r[i],r[i]), itmax = 10000, verbose = FALSE)
s[i] <- h$stress
xe <-h$x
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

### Three

```{r hgruijter, echo = FALSE}
hgruijter <- smacofPO(as.matrix(gruijter), itmax = 10000, eps = 1e-15, verbose = FALSE)
print(sum(log(as.matrix(gruijter) + diag(9)) * (1 - hzero$d)))
ekv <- as.vector(gruijter)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hgruijter$d))[ord]
epv <- as.vector(as.dist(hgruijter$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hgruijter$itel` iterations to stress `r hgruijter$s` and power `r hgruijter$r`.



## Deeper

```{r deeper, echo = FALSE, cache = TRUE}
r <- seq(0, 2, length = 100)
s <- rep(0, 100)
for (i in 1:100) {
if (i == 1) {
  xe <- NULL
}
h <- smacofPO(as.matrix(gruijter), xe = xe, interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
xe <- h$x
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

```{r deeper2, echo = FALSE, cache = TRUE}
r <- seq(0, .01, length = 100)
s <- rep(0, 100)
for (i in 1:100) {
if (i == 1) {
xe <- NULL
}
h <- smacofPO(as.matrix(gruijter), xe = xe, interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
xe <- h$x
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

## @wish_71

```{r}
hzero <- smacofPO(1 - diag(12), interval = c(0, 0), verbose = FALSE)
```

Stress at $r=0$ is `r sum(((as.matrix(wish)) - 1) ^ 2)` and the right derivative of the marginal function at zero is `r sum(log(as.matrix(wish) + diag(12)) * ( 1 - hzero$d))`. The largest $\delta_{ij}$ is `r max(wish)` and the smallest `r min(wish)`.

`r min(as.dist(hzero$d))`


```{r wish, echo = FALSE}
gmin <- min(wish)
gmax <- max(wish)
gwish <- (wish - gmin) / (gmax - gmin)
hwish <- smacofPO(as.matrix(gwish), itmax = 1000, verbose = FALSE)
ekv <- as.vector(gwish)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hwish$d))[ord]
epv <- as.vector(as.dist(hwish$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hwish$itel` iterations to stress `r hwish$s` and power `r hwish$r`.


```{r mwish, echo = FALSE}
gmax <- max(wish)
mwish <- wish / gmax
hwish <- smacofPO(as.matrix(mwish), itmax = 1000, verbose = FALSE)
ekv <- as.vector(mwish)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hwish$d))[ord]
epv <- as.vector(as.dist(hwish$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hwish$itel` iterations to stress `r hwish$s` and power `r hwish$r`.


```{r hwish, echo = FALSE}
hwish <- smacofPO(as.matrix(wish), itmax = 10000, verbose = FALSE)
ekv <- as.vector(mwish)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hwish$d))[ord]
epv <- as.vector(as.dist(hwish$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hwish$itel` iterations to stress `r hwish$s` and power `r hwish$r`.

```{r deeper3, echo = FALSE, cache = TRUE}
r <- seq(0, 4, length = 100)
s <- rep(0, 100)
for (i in 1:100) {
h <- smacofPO(as.matrix(wish), interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

## @rothkopf_57

```{r}
hzero <- smacofPO(1 - diag(36), xe = matrix(rnorm(72), 36, 2), interval = c(0, 0), verbose = FALSE, itmax = 10000)
hone <- smacofPO(as.matrix(morse), xe = NULL, interval = c(1, 1), verbose = FALSE, itmax = 10000)
```

Stress at $r=0$ is `r sum(((as.matrix(morse)) - 1) ^ 2)` and the right derivative of the marginal function at zero is `r sum(log(as.matrix(morse) + diag(36)) * ( 1 - hzero$d))`. The largest $\delta_{ij}$ is `r max(morse)` and the smallest `r min(morse)`. Stress at $r=1$ is `r hone$stress`.




```{r morse, echo = FALSE}
hmorse <- smacofPO(as.matrix(morse), interval = c(0, 10), itmax = 1000, verbose = FALSE)
ekv <- as.vector(morse)
ord <- order(ekv)
ekv <- ekv[ord]
dev <- as.vector(as.dist(hmorse$d))[ord]
epv <- as.vector(as.dist(hmorse$e))[ord]
plot(ekv, dev, col = "BLUE", pch = 16, xlab = "delta", ylab = "dist and dhat")
lines(ekv, epv, col = "RED", lwd = 2)
```

Convergence in  `r hmorse$itel` iterations to stress `r hmorse$s` and power `r hmorse$r`.

```{r deeper5, echo = FALSE, cache = TRUE}
r <- seq(0, 8, length = 100)
s <- rep(0, 100)
for (i in 1:100) {
if (i == 1) {
xe <- NULL
}
h <- smacofPO(as.matrix(morse), xe = xe, interval = c(r[i],r[i]), itmax = 10000, 
  verbose = FALSE)
s[i] <- h$stress
xe <- h$x
}
plot(r, s, type = "l", lwd = 2, col = "RED", ylab = "stress")
```

# Code

```{r eval = FALSE}
smacofPO <-
  function(delta,
           interval = c(0, 4),
           xold = NULL,
           itmax = 1000,
           eps = 1e-10,
           verbose = TRUE) {
    nobj <- nrow(delta)
    dd <- delta ^ 2
    rd <- rowSums(dd) / nobj
    sd <- mean(delta)
    ce <- -.5 * (dd - outer(rd, rd) + sd)
    ee <- eigen(ce)
    xe <- ee$vectors[, 1:2] %*% diag(sqrt(ee$values[1:2]))
    de <- as.matrix(dist(xe))
    if (interval[1] == interval[2]) {
      r <- interval[1]
      fixed <- TRUE
    } else {
      r <- (interval[1] + interval[2]) / 2
    }
    g <- function(r, delta, de) {
      return(sum(((delta ^ r) - de) ^ 2))
    }
    ep <- delta ^ r
    sold <- sum((ep - de) ^ 2)
    itel <- 1
    repeat {
      b <- -ep / (de + diag(nobj))
      diag(b) <- -rowSums(b)
      xe <- (b %*% xe) / nobj
      de <- as.matrix(dist(xe))
      smid <- sum((ep - de) ^ 2)
      if (!fixed) {
        r <- optimize(g, interval = interval, delta = delta, de = de)$minimum
      }
      ep <- delta ^ r
      snew <- sum((ep - de) ^ 2)
      if (verbose) {
        cat(
          "itel ",
          formatC(itel, format = "d"),
          "sold ",
          formatC(sold, digits = 6, format = "f"),
          "smid ",
          formatC(smid, digits = 6, format = "f"),
          "snew ",
          formatC(snew, digits = 6, format = "f"),
          "pow  ",
          formatC(r, digits = 6, format = "f"),
          "\n"
        )
      }
      if (((sold - snew) < 1e-10) || (itel == itmax)) {
        break
      }
      itel <- itel + 1
      sold <- snew
    }
    return(list(
      x = xe,
      d = de,
      e = ep,
      r = r,
      itel = itel,
      stress = snew
    ))
  }
```
