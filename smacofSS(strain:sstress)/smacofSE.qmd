---
title: |
    | Smacof at 50: A Manual
    | Part zz: smacofSE: Initial Estimates
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
---

```{r load_stuff, echo = FALSE}
library(RSpectra)
library(quadprog)
source("../../smacofCode/smacofSE(initial)/smacofTorgerson.R")
```

\sectionbreak 
 
# Introduction

The best way to avoid unwanted (i.e. non-global) local minima in smacof is to start the iterations with an excellent initial configuration. Even if the iterations converge to a non-global local minimum, at least we can be sure than the solution has a lower stress than the initial configuration, which was supposedly already excellent.

In this chapter we implement two fairly elaborate initial
estimates for metric smacof, which can of course also be 
used in the alternating least squares algorithm for non-linear
and non-metric smacof. Both initial estimates basically
iteratively minimize alternative loss functions, respectively called sstress and strain (see @deleeuw_heiser_C_82 for a definition and comparison of these loss functions).

The R functions implementing the minimization of these
loss functions can be interpreted (and can actually be used) as
alternative MDS methods. It may seem somewhat peculiar to
start an iterative MDS technique with an initial estimate 
computed with another iterative MDS technique. But it is
not exactly unprecedented, since traditionally MDS programs
compute their initial configurations by using classical
MDS (which uses iterative eigenvalue-eigenvector methods).

Our initial configuration techniques have a great deal of
flexibility, because we do not necessarily iterate to
convergence in the initial phase. We use the option
in monotone iterative algorithms to merely improve instead
of completely solve.

Also keep in mind that our objective is to find excellent
local minima of stress, in fact our ultimate objective
is to find the global minimum. This makes it desirable
to find as good an initial approximation as possible,
even if that requires a considerable amount of 
computation.

\sectionbreak


In the standard R versions of smacof the initial configuration is chosen
using classical multidimensional scaling, i.e. the method proposed in @torgerson_58 and @gower_66.

Classical scaling is based on the Euclidean embedding theorem of @schoenberg_35, revealed to psychometricians by @young_householder_38. A real hollow symmetric matrix $\Delta$ of order $n$, with non-negative entries, is a Euclidean distance matrix if and only if the matrix
$$
B:=-\frac12 J\Delta^2J
$${#eq-defbmat}
is positive semi-definite. 
Here $\Delta^2$ is the elementwise square of $\Delta$ and $J=I-n^{-1}ee'$ is the centering matrix. Moreover if $B$ is positive semi-definite then the embedding dimension is the rank of $B$. Classical scaling computes $B$ and its eigen-decomposition. It then constructs the initial estimate $X$ for $p$-dimensional MDS by using the $p$ largest eigenvalues $\Lambda$ of $B$, and the corresponding eigenvectors in $K$,  to give $X=K\Lambda^\frac12$.

Classical scaling, as an MDS technique, has one major advantage over other forms
of MDS, but also some disadvantages. The advantage is that it finds the global minimum of the strain loss function, defined as
$$
\sigma(X):=\frac14\text{tr}\ J(\Delta^2-D^2(X))J(\Delta^2-D^2(X))J)
$${#eq-strain}
with $D^2(X)$ the squared Euclidean distances. So not only does classical scaling minimize strain, but it actually computes its global minimum, and if the ordered eigenvalues of $B$ from @eq-defbmat satisfy $\lambda_p(B)>\lambda_{p+1}(B)$ that global minimum over $p$-dimensional configurations is unique (up to rotation and translation).

To see that minimizing strain is an eigenvalue problem in disguise, note that if $X$ is column centered then
$-\frac12JD^2(X)J=XX'$ and thus $-\frac12J(\Delta^2-D^2(X))J=B-XX'$. Taking
the sum of squares on both sides of the equation, and using the fact that $J$ is idempotent, gives the alternative expression for strain
$$
\sigma(X)=\text{tr}\ (B-XX')^2.
$${#eq-cstrain}
Minimizing @eq-cstrain over $p$-dimensional configuartions is indeed classical scaling. The matrix form in @eq-strain strain was first given
by @deleeuw_heiser_C_82, although equivalent versions, using different notation, are already in @gower_66 and @mardia_78. 

The first disadvantage of classical scaling is that the $B$-matrix may only
have $q<p$ positive eigenvalues. If that is the case the global minimizer $X_p$ of strain in $p<q$ dimensions does not exist. The global minimizer in $r\leq p$
dimensions has only $q$ non-zero dimensions (and $p-q$ zero dimensions).
In practice this disadvantage is often not a very serious one, because having fewer than $p$ positive eigenvalues suggests a bad fit so that maybe MDS is not a good technique choice in the first place. Because $n$ is usually much larger than $p$ matrix $B$ is bound to have more than $p$ positive eigenvalues. If not, one can always compute an additive constant to force positive semi-definiteness of $B$ (@cailliez_83).

The second disadvantage is that classical scaling, unlike smacof, has no provision to include data weights $w_{ij}$ for each of the dissimilarities $\delta_{ij}$. 
Or, if there are weights it is unclear if they should be applied to @eq-strain or
@eq-cstrain, and no matter where they are used the two definitions of strain are no longer equivalent and the global minimum advantage of classical scaling is lost. 

In particular this disadvantage also means that there cannot be missing dissimilarities, or, more precisely, something additional has to be done if there are missing data. One obvious possibility is to use alternating least squares to minimize strain, alternating a step to impute the missing dissimilarities and a step to compute the optimal configuration. The current smacof program in R imputes the missing dissimilarities by replacing them with the average non-missing dissimilarity, which is computationally convenient but not very satisfactory. This second disadvantage also means there is no straightforward version of classical
scaling for multidimensional unfolding, in which all within-set dissimilarities
are missing.

Part of the problem is the double centering operator $J$, because it requires
complete data. This problem can be alleviated if we have one object, say the
first one, for which there are no missing data. We then put that object in the
origin of the configuration and compute $-\frac12\{\delta_{ij}^2-\delta_{1i}^2-\delta_{1j}^2\}$, which is equal 
to $x_i'x_j$ for Euclidean dissimilarities. We can then define a version of strain on the non-missing elements of $B$, but we still need a low-rank approximation of
a symmetric matrix with missing data. That is, we need low-rank symmetric matrix completion, for which there is a gigantic literature (@nguyen_kim_shim_19), although that literature mostly addresses the rectangular case.

Another disadvantage, or peculiarity, is emphasized by @deleeuw_meulman_C_86. If $\Delta$ is Euclidean then @gower_66 shows that 
$$
d_{ij}^2(X_1)\leq d_{ij}^2(X_2)\leq\cdots\leq d_{ij}^2(X_r)=\delta_{ij}^2,
$${#eq-frombelow}
where $X_p$ is the $p$-dimensional classical scaling 
solution and $r$ is the rank of $B$. Thus squared dissimilarities are 
approximated from below, which may not be the most obvious way to 
approximate. If $B$ has negative eigenvalues
then @eq-frombelow is no longer true and we have
$$
d_{ij}^2(X_1)\leq d_{ij}^2(X_2)\leq\cdots\leq d_{ij}^2(X_s)\geq\delta_{ij}^2,
$${#eq-frombelowp}
where $s$ is the number of positive eigenvalues.

Another peculiarity, discussed in the excellent
paper by @bailey_gower_90, is that strain is the sum of squares over all $n^2$ residuals, which means each off-diagonal element is used twice, each diagonal element only once. In minimizing stress
or sstress the diagonal does not play a role, minimizing over all elements below the diagonal gives the same result as minimizing over all elements.

And finally the squaring and double-centering of the dissimilarities may not
be such a good idea from a statistical point of view. Squaring will
emphasize large errors, and can easily lead to outliers. Double-centering
introduces dependencies between different observations, because the
pseudo scalar products in $B$ are linear combinations of multiple
(squared) dissimilarities.

Nevertheless the smacof project, and the code for this chapter, includes a version of classical MDS that can handle missing data (but not general weights). A similar non-metric version of strain was discussed by @trosset_98.


We formulate loss as
$$
\sigma(X,\theta)=\text{tr}\ (B(\theta)-XX')^2
$${#eq-strainmissing}
where
$$
B(\theta)=-\frac12J(\Delta_0^2+\sum \theta_kE_k)J=B_0+\sum \theta_kT_k
$${#eq-btheta}
Here $\Delta_0^2$ is the matrix with squared dissimilarities, with elements equal to zero for missing data.
The $E_k$ code missing data, and are for the form $e_ie_j'+e_je_i'$. Thus they are zero, except for
elements $(i,j)$ and $(j,i)$, which are one. 

Strain @eq-strainmissing must be minimized over configurations
$X$ and over $\theta\geq 0$.

We use alternating least squares. Minimizing over $X$ for fixed $\theta$ is classical MDS. Minimizing
over $\theta$ for fixed $X$ is a non-negative linear least squares problem. 
For the first problem we use the eigs_sym function from the RSpectra package (@qiu_mei_24), for the second the quadprog package and function (@turlach_weingessel_19). Both functions in an inner iteraton have a 
cold start, which may be wasteful in later iterations, and the plan therefor is to eventually replace them with the majorization methods in smacofEigenRoutines.R
and smacofNNLS.R (both in the smacofUtilities directory in smacofCode).

If there are no missing data the program just performs classical MDS.

Finding an better $X^{(\nu+1)}$ for given $\theta^{(\nu)}$ is classical scaling. In smacofTorgerson() we find $X$ to decrease strain, not necessarily to minimize it. So we have to decrease 
$$
\sigma(X)=\text{tr}\ (B(\theta^{(\nu)})-XX')^2
$${#eq-straintheta}
The majorization method smacofSymmetricEckartYoung() is described in the chapter on utilities.

To impute the missing data define the residuals
$$
R^{(k)}:=B_0-X^{(k)}\{X^{(k)}\}'.
$${#eq-strainres}
Then
$$
\sigma(\theta)=\text{tr}\ (R^{(\nu)}+\sum\theta_sT_s)^2
$${#eq-strainimpute}
where the $T_s$ are matrices of the form $-\frac12JE_{ij}J$. 
Note $R$ is doubly-centered and thus $-\frac12\text{tr}\ JE_{ij}JR=-r_{ij}$.
Consequently
$$
\sigma(\theta)=\text{tr}\ \{R^{(\nu)}\}^2-2\theta'r^{(\nu)}+\theta'V\theta,
$${#eq-strainexpand}
where $V$ has elements $\text{tr}\ T_sT_t$. After some additional computation we find the following convenient expression for the elements of $V$.
$$
\frac14\text{tr}\ JE_{ij}JJE_{kl}J=
\begin{cases}
\frac12-\frac{1}{n}+\frac{1}{n^2}&\text{two indices in common},\\
-\frac{1}{2n}+\frac{1}{n^2}&\text{one index in common},\\
\frac{1}{n^2}&\text{no indices in common}.
\end{cases}
$${#eq-strainvmat}
Minimizing @eq-strainexpand
over $\theta\geq 0$ is straightforward quadratic programming.

The utilities chapter has the majorization routine smacofNonnegativeQP() that find a $\theta^{(\nu)}$ that decreases the loss in @eq-strainexpand over $\theta\geq 0$. For the majorization we need an upper bound for the largest eigenvalue
of $V$.

\sectionbreak

# Weights

$$
\sigma(X):=\frac14\text{tr}\ V(\Delta^2-D^2(X))V(\Delta^2-D^2(X))
$${#eq-vstrain}
with $V$ symmetric, doubly-centered and of rank $n-1$. Suppose $s_i=x_i'x_i$. Then
$D^2(X)=se'+es'-2XX'$
and thus $-\frac12VD^2(X)V=VXX'V$. With $B=-\frac12V\Delta^2V$
we have residuals $-\frac12V(\Delta^2-D^2(X)V=B-VXX'V$ and @eq-vstrain
is the sum of squares of these residuals. Thus in the Euclidean case
$X=V^+K\Lambda L'$, with $K$ and $\Lambda$ from the eigen-analysis
of $B$. In the non-Euclidean case different $V$ will give different
solutions.

# References
