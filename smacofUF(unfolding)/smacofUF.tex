% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  12pt,
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
    \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{yfonts}
\usepackage{bm}


\newtcolorbox{greybox}{
  colback=white,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
\newcommand{\sectionbreak}{\clearpage}

 
\newcommand{\ds}[4]{\sum_{{#1}=1}^{#3}\sum_{{#2}=1}^{#4}}
\newcommand{\us}[3]{\mathop{\sum\sum}_{1\leq{#2}<{#1}\leq{#3}}}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\amin}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\amax}[1]{\mathop{\text{argmax}}_{#1}}

\newcommand{\ci}{\perp\!\!\!\perp}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newcommand{\eps}{\epsilon}
\newcommand{\lbd}{\lambda}
\newcommand{\alp}{\alpha}
\newcommand{\df}{=:}
\newcommand{\am}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\ls}[2]{\mathop{\sum\sum}_{#1}^{#2}}
\newcommand{\ijs}{\mathop{\sum\sum}_{1\leq i<j\leq n}}
\newcommand{\jis}{\mathop{\sum\sum}_{1\leq j<i\leq n}}
\newcommand{\sij}{\sum_{i=1}^n\sum_{j=1}^n}
	
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdfauthor={Jan de Leeuw - University of California Los Angeles},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Smacof at 50: A Manual\\
Part 5: Unfolding in Smacof}
\author{Jan de Leeuw - University of California Los Angeles}
\date{Started December 12 2022, Version of December 22, 2024}

\begin{document}
\maketitle
\begin{abstract}
TBD
\end{abstract}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\textbf{Note:} This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
\url{https://github.com/deleeuw} in the repositories smacofCode, smacofManual,
and smacofExamples.

\section{Introduction}\label{introduction}

In Multidimensional Unfolding (MDU) the objects of an MDS problem are partitioned into two sets.
There is a set of \(n\) row-objects and a set of
\(m\) column-objects, and a corresponding \(n\times p\) row-configuration \(X\) and \(m\times p\) column-configuration
\(Y\). We minimize stress defined as
\begin{equation}
\sigma(X,Y):=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(\delta_{ij}-d(x_i,y_j)^2.
\label{eq:ufstress}
\end{equation}
over both \(X\) and \(Y\). Here
\begin{equation}
d(x_i,y_j):=\sqrt{(x_i-y_j)'(x_i-y_j)}
\label{eq:ufdist}
\end{equation}
Thus the within-set
dissimilarities are missing, or ignored even if they are available, and only the between-set dissimilarities are fitted by between-set distances.

If we define \(Z\) as
\begin{equation}
Z:=
\end{equation}
and \(U\) as
\begin{equation}
U:=
\label{eq:ufudef}
\end{equation}
then we can also write
\begin{equation}
\sigma(Z)=\sum_{i=1}^{n+m}\sum_{j=1}^{n+m}u_{ij}(\delta_{ij}-d_{ij}(Z))^2.
\label{eq:ufzstress}
\end{equation}

Data Preferences Type A and Type B Conditional

The unfolding model for preference judgments is often attributed to Clyde H. Coombs (1950),
with further developments by Coombs and his co-workers reviewed in C. H. Coombs (1964).
After this path-breaking work the digital computer took over, and minimization of loss function
\eqref{eq:ufstress} and its variations was started by
Roskam (1968) and Kruskal and Carroll (1969).

In this manual we are not interested in MDU as a psychological theory, as a model for
preference judgments. We merely are interested in mapping off-diagonal dissimilarity
relations into low-dimensional Euclidean space, i.e.~in making a picture of the
data. In some cases (distance completion, distances with errors, spatial basis)

Sixty years ago Joseph B. Kruskal published his basic non-metric multidimensional scaling
papers (Kruskal (1964a), Kruskal (1964b)). They provided the necessary tools to analyze what
later became known as symmetric one-mode data, i.e.~symmetric dissimilarities between
\(n\) objects. It did not take long before Roskam (1968) and Kruskal and Carroll (1969) generalized
to rectangular two-mode data, i.e.~to non-metric multidimensional unfolding. The canonical
example of such data is a number of individuals ranking their preferences (interpreted as similarities) for a number of objects. The ordinal information in unfolding data is much smaller
than that in symmetric one mode data. Not only are there no dissimilarities between the individuals
and between the objects, but in addition the rankings are conditional, which means that
preferences can only be compared within individuals. It is safe to say that as a consequence
of this paucity of information the unmodified Kruskal approach to non-metric unfolding did not work.

If one applies a Kruskal-type non-metric scaling program to preference rank orders, then
one invariable finds what has become known as a trivial solution. To understand this, let
us consider what non-metric multidimensional unfolding want to accomplish. Suppose there
are \(n\) individuals, and each individual \(i\) provides us with a partial order \(\prec_i\)
on the \(m\) objects. We want to represent the individuals as \(n\) points \(x_i\) and the
objects as \(m\) points \(y_j\) in a low-dimensional Euclidean space, in such a way
that \(j\prec_i\ell\) in the data corresponds with \(d(x_i,y_j)<d(x_i,y_\ell)\) in the
representation. Here \(d(x,y)\) is Euclidean distance.

The Kruskal approach quantifies this objective using a least squares
loss function called stress. The stress for individual \(i\) is
\[
\sigma_i(X,Y):=\min_{\delta\in K_i}\sum_{j=1}^m w_{ij}(\delta_j-d(x_i,y_j))^2.
\]
Here \(K_i\) is the set of all vectors \(\delta_i\) with \(m\) elements that satisfy
\(\delta_{ij}\leq\delta_{i\ell}\) for all \((i,j,\ell)\) for which \(j\prec_i\ell\).
The total stress \(\sigma(X,Y)\) is the sum of the \(\sigma_i(X,Y)\) stresses
over the \(n\) individuals.

Thus we do not require directly that the distances satify the ordinal constraints,
we require that the distances are numerically as close as possible to a vector
\(\delta\) that does satisfy the ordinal constraints. Note, however, that
the constraints on \(\delta\) use \(\leq\), \emph{less than or equal to}, instead if \(<\),
\emph{less than}, which means we are OK with \(\delta_{ij}=\delta_{i\ell}\) for some,
or even all, \(j\prec_i\ell\). And that is where the trivial solutions come in.

The most obvious trivial solution collapses all \(x_i\) and all \(y_j\) into a single point.
This makes all distances zero, and thus \(\sigma(X,Y)=0\). We have achieved our
objective, which was minimizing stress, but the solution is completely independent
of the data. A slightly more elaborate, but equally trivial, solution puts all
\(x_i\) in one point and all \(y_j\) in another. All distances are non-zero, but equal.
Even more elaborately, we can put all \(x_i\) in the origin, and all \(y_j\) on a
circle with the origin as center.

Ever since Roskam (1968) and Kruskal and Carroll (1969) trivial solutions have been acknowledged
and ways to avoid them in non-metric multidimensional unfolding have been proposed,
since 1980 mostly by Willem Heiser and his students. There are excellent discussions
of these proposed solutions in the recent dissertations of Van Deun (2005) and Busing (2010).
None of them, however, solves the version of the
non-metric multidimensional unfolding problem where we require that \(d(x_i,y_j)\leq d(x_i,y_\ell)\)
if \(j\prec_i\ell\). In that version we do not need a computer to find a perfect solution
of the relevant inequalities, we just choose one of the trivial solutions.

In this paper we study one early attempt to salvage the Kruskal-Roskam approach, using
row-wise normalization of stress. The trivial solutions have in common that the
\(d(x_i,y_j)=d(x_i,y_\ell)\) for all \((i,j,\ell)\), and we can prevent them from happening by
redefining stress as
\[
\sigma(X,Y)=\sum_{i=1}^n\frac{\min_{\delta\in K_i}\sum_{j=1}^m w_{ij}(\delta_j-d(x_i,y_j))^2}
{\min_{\delta\in L}\sum_{j=1}^m w_{ij}(\delta_j-d(x_i,y_j))^2},
\]\{\#eq-rosstress\}
where \(L\) is the set of all vectors for which all \(m\) elements are the same. The optimal \(\delta_j\)
in denominator \(i\) are all equal to the weighted mean of the \(d(x_i,y_j)\). Thus, at a trivial
solution, both nominators and denominators are zero for all \(i\), and since \(0/0\) is undefined,
stress is undefined at trivial solutions. Thus the algorithm cannot converge to a trivial
solution.

In practice, however, the Kruskal-Roskam approach does not work well. All to often solutions still look like trivial solutions, or are partly trivial. An early attempt to explain why there are these failures
was De Leeuw (1983) (reissued as De Leeuw (2006)). That paper studies how the loss function from (\textbf{eq-rosstress?}) behaves near trivial solutions. Since the 1983 paper is somewhat tentative, we present a more complete, more general, and more rigorous version in this paper. We look again at the idea that since we do not like \(0/0\) the iterative algorithm must not like it too.
\# Loss function

\subsection{Metric}\label{metric}

\subsection{Non-linear}\label{non-linear}

\subsection{Non-metric}\label{non-metric}

\subsection{Constraints}\label{constraints}

\section{smacofUF}\label{smacofuf}

\subsection{Initial Configuration}\label{initial-configuration}

\[
\sigma(C)=\sigma(\tilde C+(C-\tilde C))=\sum_{i=1}^n\sum_{j=1}^m((\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)-\text{tr}\ A_{ij}(C-\tilde C))^2
\]
\[
\sigma(C)=\sigma(\tilde C)-2\sum_{i=1}^n\sum_{j=1}^m(\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)\text{tr}\ A_{ij}(C-\tilde C)+\sum_{i=1}^n\sum_{j=1}^m\{\text{tr}\ A_{ij}(C-\tilde C)\}^2
\]

From De Leeuw, Groenen, and Pietersz (2006)
\[
\sum_{i=1}^n\sum_{j=1}^m\{\text{tr}\ A_{ij}(C-\tilde C)\}^2\leq (n+m+2)\text{tr}\ (C-\tilde C)^2
\]
Define
\[
B(\tilde C):=\frac{1}{n+m+2}\sum_{i=1}^n\sum_{j=1}^m(\delta_{ij}^2-\text{tr}\ A_{ij}\tilde C)A_{ij}
\]
So we minimize
\[
-2\ \text{tr}\ B(\tilde C)C+\text{tr}\ C^2-2\text{tr}\ C\tilde C=\\
\text{tr}\ (C-\{\tilde C + B(\tilde C)\})^2
\]

\subsection{Constraints}\label{constraints-1}

Basic smacof theory ((\textbf{deleeuw\_heiser\_C\_81?})) tells us that having constraints
on \(X\) and \(Y\)

Let
\begin{equation}
\overline{Z}:=
\label{eq:ufolzdef}
\end{equation}
be the Guttman transform of \(Z^{(k)}\). In constrained smacof we
have to minimize, in each iteration,
\[
\text{tr}\ (X-\overline{X})'V_{11}(X-\overline{X})+
2\ \text{tr}\ (X-\overline{X})'V_{12}(Y-\overline{Y})+
\text{tr}\ (Y-\overline{Y})'V_{22}(Y-\overline{Y})
\]
over \(X\) and \(Y\) satisfying the constraints. That defines \(Z^{(k+1)}\). As in
other block relaxation cases it is not necessary to actually minimize \ldots, it suffices to
decrease it in some systematic way.

If there are no constraints we have \(Z^{(k+1)}=\Gamma(Z^{(k)})\).

\section{Normalization Restrictions}\label{normalization-restrictions}

\(X'V_{11}X=I\) or \(\text{tr}\ X'V_{11}X=1\).

\subsubsection{Centroid Restriction}\label{centroid-restriction}

\[
Z=\begin{bmatrix}
X\\Y
\end{bmatrix}=
\begin{bmatrix}
I\\
D^{-1}G'
\end{bmatrix}X=HX
\]
Minimize \(\text{tr}\ (\overline{Z}-HX)'V(\overline{Z}-HX)\). If there are
no further restrictions on \(X\) the minimum is attained at \(\hat X=(H'VH)^+H'V\overline{Z}\).
Otherwise write \(X=\hat X+(X-\hat X)\). Then
\[
\text{tr}\ (\overline{Z}-H\hat X-H(X-\hat X))'V(\overline{Z}-H\hat X-H(X-\hat X))=\\
\text{tr}\ (\overline{Z}-H\hat X)'V(\overline{Z}-H\hat X)+
\text{tr}\ (X-\hat X)'H'VH(X-\hat X)
\]
and we must minimize \(\text{tr}\ (X-\hat X)'H'VH(X-\hat X)\) for example over \(X'H'VHX=I\).
That is maximizing \(H'VH\hat X=H'VHXM\) with \(M\) a symmetric matrix of Lagrange multipliers.
Thus \(M^2=\hat X'H'VH\hat X\) and \(X=\hat X(\hat X'H'VH\hat X)^{-\frac12}\).

Over \(\text{tr}\ X'H'VHX=1\) we get \(H'VH(X-\hat X)=\lambda H'VHX\) or
\(X=(1-\lambda)H'VHX=H'VH\hat X\), wich means \(X\) is proportional to \(\hat X\) and we just have to normalize \(\hat X\) to find \(X\).

The constraint \(X'V_{11}X=I\) is more difficult to deal with
\[
\text{tr}\ (X-\hat X)'H'VH(X-\hat X)=
\]

Oblique Procrustus

\subsubsection{Linearly Restricted Unfolding}\label{linearly-restricted-unfolding}

\paragraph{External Unfolding}\label{external-unfolding}

\[
\begin{bmatrix}
X\\Y
\end{bmatrix}=
\begin{bmatrix}
R&0\\
0&S
\end{bmatrix}
\begin{bmatrix}
P\\
Q
\end{bmatrix}
\]
\#\#\# Normalization Restrictions

\(X'V_{11}X=I\).

\subsubsection{Rank-one Restriction}\label{rank-one-restriction}

\(Y=za'\)

Minimize
\[
2\ \text{tr}\ (za'-\overline{Y})'V_{21}(X-\overline{X})+
\text{tr}\ (za'-\overline{Y})'V_{22}(za'-\overline{Y})
\]
Removing irrelevant terms
\[
2\ z'\{V_{21}(X-\overline{X})-V_{22}\overline{Y}\}a+
a'a.z'V_{22}z
\]
Let \(H=-V_{21}(X-\overline{X})-V_{22}\overline{Y}\). Minimize using \(z'V_{22}z=1\).
\[
a=H'z=\{\overline{Y}'V_{22}-(X-\overline{X})'V_{12})\}z
\]
\# Only You

Each row object coincides with the first choice column object

\section{Examples}\label{examples}

\subsection{Roskam}\label{roskam}

\subsection{Breakfast}\label{breakfast}

\subsection{Gold}\label{gold}

\subsection{Indicator matrix / Matrices}\label{indicator-matrix-matrices}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-busing_10}
Busing, Frank M. T. A. 2010. {``Advances in Multidimensional Unfolding.''} PhD thesis, Leiden University.

\bibitem[\citeproctext]{ref-coombs_64}
Coombs, C. H. 1964. \emph{{A Theory of Data}}. Wiley.

\bibitem[\citeproctext]{ref-coombs_50}
Coombs, Clyde H. 1950. {``Psychological Scaling Without a Unit of Measurement.''} \emph{Psychological Review} 57: 148--58.

\bibitem[\citeproctext]{ref-deleeuw_R_83}
De Leeuw, J. 1983. {``{On Degenerate Multidimensional Unfolding Solutions}.''} Research Report. Leiden, The Netherlands: Department of Data Theory FSW/RUL.

\bibitem[\citeproctext]{ref-deleeuw_R_06a}
---------. 2006. {``{On Degenerate Nonmetric Unfolding Solutions}.''} Preprint Series 507. Los Angeles, CA: UCLA Department of Statistics. \url{https://jansweb.netlify.app/publication/deleeuw-r-06-a/deleeuw-r-06-a.pdf}.

\bibitem[\citeproctext]{ref-deleeuw_groenen_pietersz_U_06}
De Leeuw, J., P. J. F. Groenen, and R. Pietersz. 2006. {``{Optimizing Functions of Squared Distances}.''} UCLA Department of Statistics. \url{https://jansweb.netlify.app/publication/deleeuw-groenen-pietersz-u-06/deleeuw-groenen-pietersz-u-06.pdf}.

\bibitem[\citeproctext]{ref-kruskal_64a}
Kruskal, J. B. 1964a. {``{Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis}.''} \emph{Psychometrika} 29: 1--27.

\bibitem[\citeproctext]{ref-kruskal_64b}
---------. 1964b. {``{Nonmetric Multidimensional Scaling: a Numerical Method}.''} \emph{Psychometrika} 29: 115--29.

\bibitem[\citeproctext]{ref-kruskal_carroll_69}
Kruskal, J. B., and J. D. Carroll. 1969. {``{Geometrical Models and Badness of Fit Functions}.''} In \emph{Multivariate Analysis, Volume II}, edited by P. R. Krishnaiah, 639--71. North Holland Publishing Company.

\bibitem[\citeproctext]{ref-roskam_68}
Roskam, E. E. 1968. {``{Metric Analysis of Ordinal Data in Psychology}.''} PhD thesis, University of Leiden.

\bibitem[\citeproctext]{ref-vandeun_05}
Van Deun, K. 2005. {``Degeneracies in Multidimensional Scaling.''} PhD thesis, KU Leuven.

\end{CSLReferences}

\end{document}
