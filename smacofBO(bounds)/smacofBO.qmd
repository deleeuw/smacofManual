---
title: |
    | Smacof at 50: A Manual
    | Part 4: 
    | smacofBO: Bounds
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD 
---

```{r loadpackages, echo = FALSE}
library(smacofAC)
```

```{r load code, echo = FALSE}
source("~/Desktop/smacofProject/smacofExamples/smacofBS/gruijter/gruijter.R")
gruijter <- as.matrix(gruijter)
```

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. All files can be found at
<https://github.com/deleeuw> in the smacofAC directories of the repositories smacofCode, smacofManual, and smacofExamples. 

# Introduction

In this part of the manual we discuss metric MDS, and the program
smacofAC. Metric MDS is the core of all smacof programs, because they
all have the majorization algorithm based on the Guttman transform 
in common.

There are two options, *bounds* and *constant*, to make smacofAC more widely applicable. Using these options the metric MDS problem becomes minimization of
\begin{equation}
\sigma(X,\hat D)=\sum\sum w_{ij}(\hat d_{ij}-d_{ij}(X))^2
(\#eq:sdefac)
\end{equation}
over both $X$ and $\hat D$, allowing some limited "metric" transformations of the data $\Delta$. Here $\Delta^-$ and $\Delta^+$ are known matrices with bounds, and $c$ is an unknown additive constant. The four "metric" types of transformations 
relating disparities $\hat d_{ij}$ to dissimilarities $\delta_{ij}$ are

1. type AC1: if bounds = 0 and constant = 0 $\hat d_{ij}=\delta_{ij}$.
2. type AC2: if bounds = 0 and constant = 1 $\hat d_{ij}=\delta_{ij}+c$ for some $c$,
3. type AC3: if bounds = 1 and constant = 0 $\delta^-_{ij}\leq\hat d_{ij}\leq\delta^+_{ij}$,
4. type AC4: if bounds = 1 and constant = 1 $\delta^-_{ij}+c\leq\hat d_{ij}\leq\delta^+_{ij}+c$ for some $c$,

All four types of transformations also require that $\hat d_{ij}\geq 0$ for all $(i,j)$. There are extensions of the smacof theory (@heiser_91) that do not require non-negativity of the disparities, but in the implementations in this manual we always force them to be non-negative. Note that AC3 is AC4 with $c=0$ and AC2 is AC4 with $\Delta^-=\Delta^+=\Delta$.

Note that for types AC2 and AC4 the data $\Delta$ do not need to be non-negative. In fact,
the original motivation for the additive constant in classical scaling (@messick_abelson_56)
was that Thurstonian analysis of tetrad or triad comparisons produced dissimilarities on an interval scale, and thus could very well include negative values. 

In AC3 and AC4 there is no mention of $\Delta$, which means
the bounds $\Delta^-$ and $\Delta^+$ are actually the data.  There are several possible uses of the bounds. We could
collect dissimilarity data by asking subjects for interval judgments. Instead
of a rating scale with possible responses from one to ten we could ask
for a mark on a line between zero and ten, and then interpret the
marks as a choice of one of the intervals $[k, k+1]$. These finite precision
or interval type of data could even come from physical measurements of distances.
Thus the bounds parameter provides one way to incorporate uncertainty into MDS, similar to interval analysis, fuzzy computing, or soft computing.

The non-negativity requirement for $\hat D$ implies bounds for the additive constant
$c$. In AC2 we need $c\geq-\min\delta_{ij}$ to maintain non-negativity. For AC4 we must have $c\geq-\min\delta_{ij}^+$, otherwise the constraints on the transformation are inconsistent. 
Clearly for consistency of AC3 and AC4 we require that $\delta_{ij}^-\leq\delta_{ij}^+$
for all $(i,j)$. It makes sense in most situations to choose $\Delta^-$ and $\Delta^+$
to be monotone with $\Delta$, but there is no requirement to do so.

# Program

## Parameters

```{r parameters, eval = FALSE}
smacofAC <- function(delta,
                     ndim = 2,
                     wmat = NULL,
                     xold = NULL,
                     bounds = FALSE,
                     constant = FALSE,
                     deltalw = NULL,
                     deltaup = NULL,
                     alpha = 2,
                     labels = row.names(delta),
                     width = 15,
                     precision = 10,
                     itmax = 1000,
                     eps = 1e-10,
                     verbose = TRUE,
                     kitmax = 5,
                     keps = 1e-10,
                     kverbose = FALSE,
                     init = 1)
```

The parameters *constant*, *bounds*, *alpha*, *kitmax*, *kepsi*, and *kverbose* are only relevant for AC2, AC3, and AC4. Nevertheless even for AC1 they should have integer values, it just doesn't matter what these values are.
Parameter *ndim* is the number of dimensions, and *init* tells if an initial configuration
is read from a file (init = 1), is computed using classical scaling (init = 2), or is
a random configuration (init = 3). Parameters
*itmax*, *epsi*, and *verbose* control the iterations. The maximum number of iterations
is *itmax*, the iterations stop if the decrease of stress in an iteration is less than
1E-*epsi*, and if *verbose* is one intermediate iteration results are written to stdout.
These intermediate iteration results are formatted with the R function formatC(), using
*width* for the width argument and *precision* for the digits argument. 

## Algorithm

### Type AC1

This is standard non-metric smacof, no bells and whistles. 

### Type AC2

For AC2 we also optimize over the additive constant $c$, and thus the ALS algorithm has two
sub-steps. The first sub-step consists of a number of Guttman iterations to update $X$ for given
$\hat D$ (i.e. for given $c$) and the second sub-step updates $c$ for given $X$.
Parameters *kitmax*, *kepsi*, and *kverbose* control the inner iterations in the first sub-step in the same way as *itmax*, *epsi*, and *verbose* control the
outer iterations that include both sub-steps. No inner iterations are used
to update the additive constant, which only requires computing a weighted
average. 
\begin{equation}
c=-\frac{\sum\sum w_{ij}(\delta_{ij}-d_{ij}(X))}{\sum\sum w_{ij}}
(\#eq:updac2)
\end{equation}
AC2 should give the same results as the MDS method of @cooper_72.

### Type AC3

The algorithm for AC3 has the same structure as that for AC2. Instead of
a second sub-step computing the additive constant, the second sub-step
computes $\hat D$ by squeezing the $D(X)$ into the bounds. Thus
\begin{equation}
\hat d_{ij}=\begin{cases}
\delta_{ij}^-&\text{ if }d_{ij}(X)<\delta_{ij}^-,\\
\delta_{ij}^+&\text{ if }d_{ij}(X)>\delta_{ij}^+,\\
d_{ij}(X)&\text{ otherwise }.
\end{cases}
(\#eq:updac3)
\end{equation}
Obviously no iterations are required in the second sub-step. 

### Type AC4

Of the four regression problems in the second ALS sub-step only the one for AC4 with both bounds and additive constant is non-trivial. We'll give it some extra attention.

It may help to give an example of what it actually requires. We use the De Gruijter example
with nine Dutch political parties from 1967 (@degruijter_67). For ease of reference we include the data here. Dissimilarities are averages over a group of 100 students from an introductory psychology course.
```{r gruijter, echo = FALSE}
gruijter
```


We compute distances from the Torgerson solution. The Shepard plot for $c=0$ and the Torgerson distances is in figure \@ref(fig:bandplot). The two blue
lines are connecting the $\delta_{ij}^-$ and the $\delta_{ij}^+$, i.e. they give
the bounds for $c=0$. In our example the lines are parallel, because $\delta_{ij}^+-\delta_{ij}^-=2$ for all $(i,j)$, but in general this may not be the case.
The points between the two lines do not contribute to the loss, and
the points outside the band contribute by how much they are outside, as indicated by the
black vertical fitlines.



By varying $c$ we shift the region between the two parallel lines upwards or downwards. The width of the region, or more generally the shape, always remains the same, because it is determined by the difference of $\delta^+$ and $\delta^-$ and does not depend on $c$. The
optimal $c$ is that shift for which the red $(\delta_{ij},d_{ij}(X))$ points are as much as possible within the strip between the $\delta^-$ and $\delta^+$ lines. This is in the least squares sense, which means that we minimize the horizontal squared distances from the points outside the strip to the $\delta^-$ and $\delta^+$ lines (i.e. the black vertical lines).

Let's formalize this. Define
\begin{equation}
\phi_{ij}(c):=\min_{\delta_{ij}\geq 0}\{(\delta_{ij}-d_{ij}(X))^2\mid \delta^-_{ij}+c\leq\delta_{ij}\leq\delta^+_{ij}+c\}
(\#eq:phiijdef)
\end{equation}
and
\begin{equation}
\phi(c):=\sum\sum w_{ij}\phi_{ij}(c)
(\#eq:phidef)
\end{equation}
The constraints are consistent if $\delta_{ij}^++c\geq 0$, i.e. if $c\geq c_0:=-\min\delta_{ij}^+$.
The regression problem is to minimize $\phi$ over $c\geq c_0:=-\min\delta_{ij}^+$. 

```{r onefuncdef, echo = FALSE, eval = FALSE}
fc <- function(c, d, delta, dlow, dup) {
  if (d < (dlow + c)) {
    return ((d - (dlow + c)) ^ 2)
  }
  if (d > (dup + c)) {
    return ((d - (dup + c)) ^ 2)
  }
  else {
    return(0)
  }
}
c <- seq(-5, 5, length = 100)
f <- rep(0, length = 100)
k <- 10
for (i in 1:100) {
f[i] <- fc(c[i], d[k], delta[k], deltalow[k], deltaup[k])
}
```

Figure has an example of one of the $\phi_{ij}$. The value of the 
$d_{ij}(X)$ we used is , $\delta_{ij}$ is , $\delta_{ij}^-$ is , and $\delta_{ij}^+$ is .
The two red vertical lines
are at $c=d_{ij}(X)-\delta_{ij}^+$ and $c=d_{ij}(X)-\delta_{ij}^-$. 
 
Now
\begin{equation}
\hat d_{ij}=\begin{cases}
\delta_{ij}^-+c&\text{ if }c\geq d_{ij}(X)-\delta_{ij}^-,\\
\delta_{ij}^++c&\text{ if }c\leq d_{ij}(X)-\delta_{ij}^+,\\
d_{ij}(X)&\text{ otherwise}.
\end{cases}
(\#eq:solves)
\end{equation}
and thus
\begin{equation}
\phi_{ij}(c)=\begin{cases}
(d_{ij}(X)-(\delta_{ij}^-+c))^2&\text{ if }c\geq d_{ij}(X)-\delta_{ij}^-,\\
(d_{ij}(X)-(\delta_{ij}^++c))^2&\text{ if }c\leq d_{ij}(X)-\delta_{ij}^+,\\
0&\text{ otherwise}.
\end{cases}
(\#eq:funcs)
\end{equation}
It follows that $\phi_{ij}$ is
piecewise quadratic, convex, and continuously differentiable. The derivative
is piecewise linear, continuous, and increasing. In fact
\begin{equation}
\mathcal{D}\phi_{ij}(c)=\begin{cases}
2(c-(d_{ij}(X)-\delta_{ij}^-))&\text{ if }c\geq d_{ij}(X)-\delta_{ij}^-,\\
2(c-(d_{ij}(X)-\delta_{ij}^+))&\text{ if }c\leq d_{ij}(X)-\delta_{ij}^+,\\
0&\text{ otherwise}.
\end{cases}
(\#eq:derivs)
\end{equation}


```{r onefunc, echo = FALSE, fig.cap = "De Gruijter example, a single phi(i,j)", eval = FALSE}
plot(c, f, type = "l", xlab = "c", ylab = "phi(i,j)", col = "BLUE", lwd = 2)
abline(v = d[k] - deltalow[k], col = "RED")
abline(v = d[k] - deltaup[k], col = "RED")
```

Since $\phi$ is a positive linear combination of the $\phi_{ij}$ it is also piecewise quadratic, convex, and continuously differentiable, 
with a continuous piecewise linear increasing derivative. Note $\phi$ is **not** twice-differentiable
and **not** strictly convex. Figure \@ref(fig:morefunc) has a plot of $\phi$ for the De Gruijter
example. The red vertical lines are at $c=c_0$ and at $c_1:=\max\{d_{ij}(X)-\delta_{ij}^-\}$. From \@ref(eq:derivs) we see that if $c\geq c_1$
then $\mathcal{D}\phi(c_1)\geq 0$ and thus we can look for the optimum $c$ in the interval
$[c_0,c_1]$.

```{r gcfunc, echo = FALSE, eval = FALSE}
gc <- function(c, d, delta, deltalow, deltaup) {
  n <- length(delta)
  s <- 0
  for (k in 1:n) {
    s <- s + fc(c, d[k], delta[k], deltalow[k], deltaup[k])
  }
  return(s)
}
x <- seq(-5, 5, length = 100)
f <- rep(0, length = 100)
for (i in 1:100) {
f[i] <- gc(x[i],  d, delta, deltalow, deltaup)
}
```


We minimize $\phi$ by using the R function optimize(). 



## Output

## Plots

# Examples

## @degruijter_67

It may help to give an example of what it actually requires. We use the De Gruijter example
with nine Dutch political parties from 1967 (@degruijter_67). Dissimilarities are averages over a group of 100 students from an introductory psychology course.

We compute optimal
solutions for all four types AC1-AC4 (two dimensions, Torgerson initial estimate,
no weights). We iterate until the difference in consecutive stress values is
less than 1e-10. For each of the four runs we give the number of iterations,
the final stress, and the additive constant in case of AC2 and AC4. We also
make three plots: the Shepard plot with points $(\delta_{ij},d_{ij}(X))$
in blue and with points $(\delta_{ij},\hat d_{ij})$ in red, the configuration
plot with a labeled $X$, and the dist-dhat plot with points $(d_{ij}(X),\hat d_{ij})$
scattered around the line $d=\hat d$. Line segments are drawn in the plots to
show the fit of all pairs $(i,j)$.

## Type AC1



```{r gruijterh00, fig.align = "center", echo = FALSE}
h00 <- smacofAC(gruijter, bounds = 0, constant = 0, verbose = FALSE)
par(pty = "s")
smacofShepardPlot(h00, main = "Shepard Plot AC1", fitlines = TRUE)
smacofConfigurationPlot(h00, cex = .65, main = "Configuration Plot AC1")
#smacofDistDhatPlot(h00, cex = 1, main = "Dist-Dhat Plot AC1")
```

For AC1 we find a minimum stress of `r h00$snew` after `r h00$itel` iterations.
The Shepard plot has a substantial intercept, which suggest that an additive
constant may improve the fit. This is typical for average similarity judgments
over heterogeneous groups of individuals. It is the reason why @ekman_54
linearly transformed his average similarities so that the smallest became zero
and the largest became one. That amounts to applying the additive constant before 
the MDS analysis.

To give some content to the configuration plot: CPN (communists), PSP (pacifists),
and PvdA (social democrats) are leftists parties, ARP (protestants), CHU (other
protestants), KVP (catholics) are religious parties, BP (farmers) is a 
right-wing protest party, VVD (classical liberals) is a conservative party, and D'66
(pragmatists, centrists) was brand new in 1967 and was supposedly beyond left and right.

## Type AC2



```{r gruijterh10, fig.align = "center", echo = FALSE}
h10 <- smacofAC(gruijter, constant = 1, bounds = 0, verbose = FALSE)
par(pty = "s")
smacofShepardPlot(h10, main = "Shepard Plot AC2", fitlines = TRUE)
smacofConfigurationPlot(h10, cex = .65, main = "Configuration Plot AC2")
#smacofDistDhatPlot(h10, cex = 1, main = "Dist-Dhat Plot AC2")
```

As expected, the additive constant improves the fit. We have convergence after `r h10$itel`
iterations to stress `r h10$snew`. The additive constant is `r h10$addc`, which means the smallest $\delta_{ij}+c$, between ARP and CHU, is now zero. The configuration shows the same three groups, but they cluster a bit more tightly. This is to be expected. Without the
additive constant the dissimilarities are more equal and consequently the distances are
more equal to. The configuration tends more to what we see if all dissimilarities are equal,
i.e. to points regularly spaced on a circle (@deleeuw_stoop_A_84).

## Type AC3


```{r gruijterh01, fig.align = "center", echo = FALSE, verbose = FALSE}
h01 <- smacofAC(gruijter, constant = 1, bounds = 4, alpha = 1)
par(pty = "s")
smacofShepardPlot(h01, main = "Shepard Plot AC3", fitlines = TRUE)
smacofConfigurationPlot(h01, cex = .65, main = "Configuration Plot AC3")
#smacofDistDhatPlot(h01, cex = 1, main = "Dist-Dhat Plot AC3")
```

The bounds we use are $\delta_{ij}\pm 1$. After `r h01$itel` iterations
we arrive at stress `r h01$snew`. In the configuration plot the centrists
have moved to the center.
## Type AC4



```{r gruijterh11, fig.align = "center", echo = FALSE}
par(pty = "s")
h11 <- smacofAC(gruijter, constant = 1, bounds = 4, alpha = 1, verbose = FALSE)
smacofShepardPlot(h11, main = "Shepard Plot AC1", fitlines = TRUE)
smacofConfigurationPlot(h11, cex = .65, main = "Configuration Plot AC1")
#smacofDistDhatPlot(h11, cex = 1, main = "Dist-Dhat Plot AC1")
```

After `r h11$itel` iterations stress is `r h11$snew`, i.e. practically zero. We succeeded in moving all distances within the bounds. The additive constant is `r h11$addc`. The configuration 
is again pretty much the same with D'66 in the center. VVD moves closer to the Christian
Democrats, and BP is more isolated.
# References
