% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{yfonts}
\usepackage{bm}


\newtcolorbox{greybox}{
  colback=white,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
 
\newcommand{\ds}[4]{\sum_{{#1}=1}^{#3}\sum_{{#2}=1}^{#4}}
\newcommand{\us}[3]{\mathop{\sum\sum}_{1\leq{#2}<{#1}\leq{#3}}}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\amin}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\amax}[1]{\mathop{\text{argmax}}_{#1}}

\newcommand{\ci}{\perp\!\!\!\perp}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newcommand{\eps}{\epsilon}
\newcommand{\lbd}{\lambda}
\newcommand{\alp}{\alpha}
\newcommand{\df}{=:}
\newcommand{\am}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\ls}[2]{\mathop{\sum\sum}_{#1}^{#2}}
\newcommand{\ijs}{\mathop{\sum\sum}_{1\leq i<j\leq n}}
\newcommand{\jis}{\mathop{\sum\sum}_{1\leq j<i\leq n}}
\newcommand{\sij}{\sum_{i=1}^n\sum_{j=1}^n}

\newcommand{\sectionbreak}{\pagebreak}


	
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{plain}
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Robust Least Squares Multidimensional Scaling},
  pdfauthor={Jan de Leeuw},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Robust Least Squares Multidimensional Scaling}
\author{Jan de Leeuw}
\date{October 30, 2024}

\begin{document}
\maketitle
\begin{abstract}
Combining different loss functions with linear models and minimizing
loss with iteratively reweighted least squares (IRLS) has a long history
in robust statistics. In this paper we use an IRLS version of the smacof
algorithm to minimize various robust multidimensional scaling loss
functions. Our results use a general theorem on sharp quadratic
majorization of De Leeuw and Lange
(\citeproc{ref-deleeuw_lange_A_09}{2009}). We relate this theorem to
earlier results in robust statistics, location theory, and sparse
recovery. Code in R is included.
\end{abstract}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\sectionbreak

\listoffigures

\sectionbreak

\section{Introduction}\label{sec-intro}

The title of this paper is somewhat surprising. Least squares estimation
is typically not robust, it is sensitive to outliers and pays too much
attention to minimizing the largest residuals. What we mean by robust
least squares multidimensional scaling (MDS), however, is to use the
smacof machinery designed to minimize least squares loss functions of
the form \begin{equation}\phantomsection\label{eq-stressdef}{
\sigma(X):=\sum\omega_k(\delta_k-d_k(X))^2,
}\end{equation} to minimize robust loss functions. In
Equation~\ref{eq-stressdef} the \(\omega_k\) are positive
\emph{weights}, the \(\delta_k\) are positive \emph{dissimilarities},
and the \(d_k(X)\) are Euclidean \emph{distances} between two of the
rows of the \(n\times p\) \emph{configuration} matrix \(X\).

Some terminological conventions we use throughout the paper. By
\emph{positive} we mean smaller than or equal to zero. Smaller than zero
is \emph{strictly positive}. Some for \emph{negative} and \emph{strictly
negative}, for \emph{increasing} and \emph{strictly increasing}, and for
\emph{decreasing} and \emph{strictly decreasing}.

The prototypical robust loss function is least absolute value loss
\begin{equation}\phantomsection\label{eq-stradddef}{
\sigma(X):=\sum \omega_k|\delta_k-d_k(X)|,
}\end{equation} which we will call \emph{strife}, since the names
\emph{stress}, \emph{sstress}, and \emph{strain} are already taken. Note
we are overloading the symbol \(\sigma\), because we will use it for all
of the loss functions in this paper.

Strife is not differentiable at configurations \(X\) for which there is
at least one \(k\) for which either \(d_k(X)=\delta_k\) or \(d_k(X)=0\)
(or both). This lack of differentiability complicates the minimization
problem. Moreover experience with one-dimensional and city block MDS
suggests that having areas where the loss function is not differentiable
leads to (many) additional local minima.

In this paper we will discuss (and implement) various variations of
strife from Equation~\ref{eq-stradddef}. They can be interpreted in two
different ways. On the one hand they are smoothers of the absolute value
function, and consequently of strife. We want to eliminate the problems
with differentiability, at least the ones caused by \(\delta_k=d_k(X)\).
If this is our main goal, then we want to choose the smoother in such a
way that it is close to the absolute value function. This is similar to
the distance smoothing used by Pliner (\citeproc{ref-pliner_96}{1996})
and Groenen, Heiser, and Meulman
(\citeproc{ref-groenen_heiser_meulman_99}{1999}) in the global
minimization of stress from Equation~\ref{eq-stressdef}, except that we
do not smooth the distance function but the strife residual.

On the other hand our modified loss functions can be interpreted as more
robust versions of the least squares loss function, and consequently of
stress. Our goal then is to combine the robustness of the absolute value
function with the efficiency and computational ease of least squares. If
robustness is our main goal then there is no reason to stay close to the
absolute value function.

Our robust or smooth loss functions are all of the form
\begin{equation}\phantomsection\label{eq-strifedef}{
\sigma(X):=\sum\omega_k\ f(r_k(X)),
}\end{equation} where \(r_k(X)\) is the \emph{residual}, i.e.
\begin{equation}\phantomsection\label{eq-resdef}{
r_k(X):=\delta_k-d_k(X).
}\end{equation} The function \(f\) is assumed to be even (i.e.~symmetric
around zero) and attains its minimum, which is equal to zero, at zero.
For now, note that loss in Equation~\ref{eq-stressdef} is the special
case with \(f(x)=x^2\) and loss in Equation~\ref{eq-stradddef} is the
special case with \(f(x)=|x|\).

\sectionbreak

\section{Majorization}\label{sec-major}

Our loss functions will be minimized by using majorization algorithms
(these days more commonly known as MM algorithms). This paper discusses
a general way to construct majorization algorithms for robust loss
functions.

For completeness we give a short introduction to majorization, without
aiming for maximum generality. For the general theory of majorization
algorithms we refer to their introduction in De Leeuw
(\citeproc{ref-deleeuw_C_94c}{1994}) and to the excellent and
comprehensive book by Lange (\citeproc{ref-lange_16}{2016}). As
mentioned in Section~\ref{sec-intro} the robust loss functions in
Equation~\ref{eq-strifedef} are real-valued functions of a single real
variable, defined on the whole real line, they are even, and they attain
a minimum equal to zero at zero. Thus they are all symmetric bowls
anchored at zero.

\begin{definition}[]\protect\hypertarget{def-majorize}{}\label{def-majorize}

A function \(g\) \emph{majorizes} a function \(f\) at a point \(y\) if
\(g(x)\geq f(x)\) for all \(x\) and \(g(y)=f(y)\). The point \(y\) is a
\emph{support point} of the majorization. Majorization of \(f\) at \(y\)
is \emph{strict} if \(g(x)>f(x)\) for all \(x\not= y\).

\end{definition}

Majorizers have at least one support point, but they can have many. The
function \(g(x)=x^2+\sin^2(x)\) majorizes \(f(x)=x^2\) at every integer
multiple of \(\pi\), and strictly majorizes \(f\) at none of its support
points. Also any function \(f\) majorizes itself at all points of the
real line.

\begin{definition}[]\protect\hypertarget{def-sharp}{}\label{def-sharp}

If \(\mathfrak{H}\) is a family of functions that all majorize \(f\) at
\(y\) then \(h\in\mathfrak{H}\) is a \emph{sharp majorization} in
\(\mathfrak{H}\) if \(h(x)\leq g(x)\) for all \(g\in\mathfrak{H}\). The
sharp majorization, if it exists, is by definition unique.

\end{definition}

\begin{theorem}[]\protect\hypertarget{thm-diff}{}\label{thm-diff}

Suppose \(g\) majorizes \(f\) at \(y\).

\begin{itemize}
\tightlist
\item
  If \(f\) and \(g\) are differentiable at \(y\) then \(f'(y)=g'(y)\).
\item
  If \(f\) and \(g\) are twice-differentiable at \(y\) then
  \(f''(y)\leq g''(y)\).
\item
  If \(f''(y)<g''(y)\) then \(g\) strictly majorizes \(f\) in a
  neighborhood of \(y\).
\end{itemize}

\end{theorem}

\begin{proof}
\(h=g-f\) is negative and has a minimum equal to zero at \(y\). Thus the
derivative of \(h\) vanishes at \(y\) and the second derivative is
negative at \(y\). If the second derivative is strictly negative then we
use the sufficient condition for a local minimum.
\end{proof}

A \emph{majorization algorithm} to minimize \(f\) is iterative. We
update \(x^{(\nu)}\), the approximation of the minimizer in iteration
\(\nu\), by \begin{equation}\phantomsection\label{eq-majmaj}{
x^{(\nu+1)}\in\mathop{\text{argmin}}_x\ g_\nu(x),
}\end{equation} where \(g_\nu\) majorizes \(f\) at \(x^{(\nu)}\). If
\(x^{(\nu)}\in\mathop{\text{argmin}}_x\ g_\nu(x)\) we stop. Otherwise we
find a new majorizer \(g_{\nu+1}\), which majorizes \(f\) at
\(x^{(\nu+1)}\), and start a new iteration.

Convergence of majorization algorithms follows from the \emph{sandwich
inequality} \begin{equation}\phantomsection\label{eq-sandwich}{
f(x^{(\nu+1)})\leq g_\nu(x^{(\nu+1)})\leq g_\nu(x^{(\nu)})=f(x^{(\nu)}).
}\end{equation} This we the algorithm produces a decreasing sequence of
loss function values, which converges if loss is bounded below. In
Equation~\ref{eq-sandwich} the first inquality (from the left) follows
from majorization. If the majorization is strict, then so is the
inequality. The second inequality follows from minimization of \(g\). It
is strict if \(g\) has a unique minimizer, for example if \(g\) is
strictly convex. The final equality in Equation~\ref{eq-sandwich} comes
from majorization at \(x^{(\nu)}\).

If \(x^{(\nu)}\) minimizes \(g_\nu\) we stop and we have finite
convergence. If the algorithm generates an infinite sequence, which is
the usual case, the second inequality in Equation~\ref{eq-sandwich} is
always strict, and the algorithm generates a strictly decreasing
sequence of loss function values. Note that for the validity of the
sandwich inequality it suffices to decrease \(g_\nu\) in every
iteration, and not necessarily to minimize it. Different ways to
decrease \(g_\nu\) correspond with different step-size procedures in
gradient methods.

Of course convergence of loss function values does not guarantee
convergence of the \(x^{(k)}\). For the additional continuity,
campactness, and identification conditions that are needed we refer to
the majoriztaion and MM literature.

\section{Majorizing Strife}\label{sec-majorization}

The idea of minimizing a least absolute value (LAV) to obtain parameter
estimates dates back to the work of Boskovitch in the middle of the
eighteenth century. Until fairly recently it has been applied mainly to
fit location parameters and more general linear models.

The pioneering work in strife minimization using smacof is Heiser
(\citeproc{ref-heiser_88}{1988}), which builds on earlier work of Heiser
(\citeproc{ref-heiser_87}{1987}). It is based on a creative use of the
Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of
the absolute value function.

The AM/GM inequality says that for all non-negative \(x\) and \(y\) we
have \begin{equation}\phantomsection\label{eq-amgm}{
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),
}\end{equation} with equality if and only if \(|x|=|y|\). If \(|y|>0\)
we can write Equation~\ref{eq-amgm} as
\begin{equation}\phantomsection\label{eq-amgmmaj}{
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\
}\end{equation} and this provides a quadratic majorization of \(|x|\) at
\(y\). There is no quadratic majorization of \(|x|\) at \(y=0\), which
is a problem we will have to deal with at some point.

Using the majorization Equation~\ref{eq-amgmmaj}, and assuming
\(\delta_k\not=d_k(Y)\) for all \(k\), we define
\begin{equation}\phantomsection\label{eq-wk1def}{
\omega_k(X):=\omega_k\frac{1}{r_k(X)},
}\end{equation} and, for a fixed \(Y\),
\begin{equation}\phantomsection\label{eq-strifemaj}{
\eta(X):=\frac12\sum \omega_k(Y)(r_k^2(X)+r_k^2(Y)).
}\end{equation} Now \(\sigma(X)\leq\eta(X)\) for all \(X\) and
\(\sigma(Y)=\eta(Y)\), and thus \(\eta\) majorizes \(\sigma\) at \(Y\).

\subsection{Algorithm}\label{sec-amgm}

Reweighted smacof to minimize strife computes \(X^{(\nu+1)}\) by
minimizing or decreasing
\begin{equation}\phantomsection\label{eq-sstrf}{
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X^{(\nu)}))^2,
}\end{equation} using a standard smacof step. It then computes the new
weights \(\omega_k(X^{(\nu+1)})\) from Equation~\ref{eq-wk1def} and uses
them in the next smacof step to update \(X^{(\nu+1)}\). And so on, until
convergence.

A straightforward variation of the algorithm does a number of smacof
steps between upgrading the weights. This still leads to a monotone, and
thus convergent, algorithm. How many smacof steps we have to take in
these inner iterations is something that needs further study. It is
likely to depend on the fit of the data, on the shape of the function
near the local minimum, and on how far the currfent iteration is from
the local minimum.

\subsection{Zero Residuals}\label{sec-zero}

It may happen that for some \(k\) we have \(d_k(X^{(\nu)})=\delta_k\)
while iterating. There have been various proposals to deal with such an
unfortunate event, and we will discuss some of them below. Even more
importantly we will see that that the minimizer of the absolute value
loss usually satisfies \(d_k(X)=\delta_k\) for quite a few elements,
which means that near convergence the algorithm may become unstable
because the weights from Equation~\ref{eq-wk1def} become very large.

A large number of somewhat ad-hoc solutions have been proposed to deal
with the problem of zero residuals, both in location analysis and in the
statistical literature. We tend to agree with the assessment of Aftab
and Hartley (\citeproc{ref-aftab_hartley_15}{2015}).

\begin{quote}
.. attempts to analyze this difficulty {[}caused by infinite weights of
IRLS for the \(\ell_p\)-loss{]} have a long history of proofs and
counterexamples to incorrect claims.
\end{quote}

Schlossmacher (\citeproc{ref-schlossmacher_73}{1973}) is the first
discussion of the majorization method in the statistical literature (for
LAV linear regression). His proposal is to simply set a weight equal to
zero if the corresponding residual is less than some small positive
value \(\epsilon\). A similar approach, also used in location analysis,
is to cap the weights at some large positive value. In Heiser
(\citeproc{ref-heiser_88}{1988}) all residuals smaller than this epsilon
get a weight equal to the weighted average of all these small residuals.
Phillips (\citeproc{ref-phillips_02}{2002}) assumes double-exponential
errors in LAV regression and then concludes that the EM algorithm gives
the majorization method we have discussed. He uses \eqref{eq:wk1def}
throughout if all residuals are larger than \(\epsilon\). If one of more
residuals are smaller than epsilon then the weight for those residuals
is set equal to one, while for the remaining residuals the weight is set
to epsilon divided by the absolute value of the residual. Often we get
the assurance in these papers that the problem is not really important
in practice, because it is very rare, and by just wiggling we will get
to the unique solution anyway. But both in location analysis and in LAV
regression the loss function is convex, however, which guarantees a
unique minimum. This is certainly not the case in robust MDS. In this
paper we try to follow a more systematic approach that uses smooth
parametric approximations to the absolute value function, where the
parameter can be used to make the approximation as precise as necessary.

In the case of stress the directional derivatives can be used to prove
that if \(\omega_k\delta_k>0\) for all \(k\) then stress is
differentiable at each local minimum (De Leeuw
(\citeproc{ref-deleeuw_A_84f}{1984})). For strife to be differentiable
we would have to prove that at a local minimum both \(d_k(X)>0\) and
\((d_k(X)-\delta_k)\not= 0\) for all \(k\) with \(\omega_k>0\). But this
is impossible by the following argument.

In the one-dimensional case we can partition \(\mathbb{R}^n\) into
\(n!\) polyhedral convex cones corresponding with the permutations of
\(x\). Within each cone the distances are a linear function of \(x\).
Each cone can be partitioned by intersecting it with the polyhedra
defined by the linear inequalities \(\delta_k-d_k(x)\geq 0\) or
\(\delta_k-d_k(x)\leq 0\). Some of these intersections can and will
obviously be empty. Within each of these non-empty polyhedral regions
strife is a linear function of \(x\). Thus it attains its minimum for
the region at a vertex, which is a solution for which some distances are
zero and/or some residuals are zero. There can be no\\
minima, local or global, in the interior of one of these polyhedral
regions. We have thus shown that in one dimension strife is not
differentiable at a local minimum, and that there is presumably a large
number of them. Even for moderate \(n\) the number of regions is of
course too large to actually compute or draw.

In the multidimensional case linearity goes out the window. The set of
configurations \(d_k(X)=\delta_k\) is an ellipsoid and \(d_k(X)=0\)
defines a hyperplane. Strife is not differentiable at all intersections
of these ellipsoids and hyperplanes. The partitioning of
\(\mathbb{R}^n\) by these ellipsoids and hyperplanes is not simple to
describe. It has convex and non-convex cells, and within each cell
strife is the difference of two weighted sums of distances. Anything can
happen.

\subsection{\texorpdfstring{\(\ell_0\)
loss}{\textbackslash ell\_0 loss}}\label{ell_0-loss}

A somewhat extreme special case of Equation~\ref{eq-strifedef} has
\begin{equation}\phantomsection\label{eq-elnul}{
f(x)=\begin{cases}
0&\text{ if }x = 0,\\
1&\text{ otherwise}.
\end{cases}
}\end{equation} This is \(\ell_0\) loss. Minimizing \(\ell_0\) loss
means maximizing the number of cases with perfect fit, i.e.~with
\(\delta_k=d_k(X)\). The reason we mention it here is that the work of
Donoho and Elad (\citeproc{ref-donoho_elad_03}{2003}) and Candes and Tao
(\citeproc{ref-candes_tao_05}{2005}) suggests that the minimizer of
\(\ell_1\) loss, i.e.~absolute value loss, gives a good approximation to
the minimizer of \(\ell_0\) loss, at least in a number of special cases.
In MDS we do not have linearity or convexity, but nevertheless the
theoretical results in simpler cases are suggestive. We have seen that
at least in the one-dimensional MDS case a number of residuals will
indeed be zero at the optimum LAV solution.

There is an excellent review of the use of \(\ell_1\) in various sparse
recovery fields in Candes, Wakin, and Boyd
(\citeproc{ref-candes_wakin_boyd_08}{2008}). In that paper they also
propose an iteratively reweighted LAV algorithm, which solves \(\ell_1\)
problems between weight updates. Maybe because of that they go so far as
calling \(\ell_1\) ``the modern least squares''. But let's not get
carried away, in actual ease and frequency of use \(\ell_1\) still has a
long way to go if it wants to replace \(\ell_2\).

\sectionbreak

\section{Generalizing Strife}\label{generalizing-strife}

We have seen that Heiser (\citeproc{ref-heiser_88}{1988}) applied
majorization to minimize strife, using the AM/GM inequality. We now
generalize this approach so that it can deal with other robust loss
functions. A great number of different loss functions will be discussed.
My intention is certainly not to confuse the reader and potential user
by presenting a large number of alternatives with rather limited
information. We show all these loss functions as examples of a general
principle of algorithm construction and as examples of loss functions
that have been used in statistics, location analysis, image analysis,
and engineering over the years. They are all implemented in the function
smacofRobust(), written in R (R Core Team
(\citeproc{ref-r_core_team_24}{2024})), and listed in the appendix of
this paper.

\subsection{Sharp Quadratic
Majorization}\label{sharp-quadratic-majorization}

The AM/GM inequality was used in Section~\ref{sec-majorization} to
construct a quadratic majorization of strife. In this paper we are
specifically interested in sharp quadratic majorization, in which
\(\mathfrak{H}\) is the set of all (not necessarily convex) quadratics
that majorize \(f\) at \(y\). This case has been studied in detail (in
the case of real-valued functions on the line) in De Leeuw and Lange
(\citeproc{ref-deleeuw_lange_A_09}{2009}), and much of this section is
taken from their paper. We added some minor extensions and
reformulations.

For the loss functions we study in this paper there are two problems
that have to be solved. First, we want a general procedure to construct
quadratic majorizers. Second, we what to show that some of our
majorizers are sharp.

If \(f\) is differentiable at \(y\), then all quadratics \(g\) that
majorize \(f\) at \(y\) are of the form
\begin{equation}\phantomsection\label{eq-qmaj}{
g_a(x):=f(y)+f'(y)(x-y)+\frac12a(x-y)^2
}\end{equation} for some \(a=g_a''(x)\), not necessarily positive. If
\(f\) is twice differentiable at \(y\) then \(g_a''(y)\geq f''(y)\) by
Theorem~\ref{thm-diff}, and thus we have the necessary condition
\(a\geq f''(y)\). Note that not all functions have quadratic
majorizations. If \(f\) is a non-trivial cubic and \(g\) is quadratic,
then \(h=g-f\) is a non-trivial cubic, and consequently we cannot have
\(h\geq 0\) on the whole real line.

We now look more closely at \(a\) in Equation~\ref{eq-qmaj}, initially
concentrating on necessary conditions for majorization. For
\(x\not = y\) define \begin{equation}\phantomsection\label{eq-alpdef}{
\alpha(x):=\frac{f(x)-f(y)-f'(y)(x-y)}{\frac12(x-y)^2}.
}\end{equation} Of course \(\alpha\) is a different function of \(x\)
for each \(y\), but since we are dealing with majorization at one fixed
\(y\) we suppress this dependence.

If \(f\) is two times differentiable at \(y\) then, by the definion of
the second derivative, \begin{equation}\phantomsection\label{eq-alphop}{
\lim_{x\rightarrow y}\alpha(x)=f''(y),
}\end{equation} and thus can we define \(\alpha(y)=f''(y)\) to make
\(\alpha\) continuous at \(y\).

If \(f\) is convex, then \(\alpha\) is the ratio of two positive convex
functions of \(x\), and is thus positive. If \(f\) is concave then
\(\alpha\) is negative. If \(f\) is two times differentiable then there
is a \(z\) between \(x\) and \(y\) such that \(\alpha(x)=f''(z)\). Thus
if \(f''(x)\leq K\) for all \(x\), then \(\alpha(x)\leq K\) as well.

Quadratic majorization of \(f\) by \(g_a\) from Equation~\ref{eq-qmaj}
at \(y\) is equivalent to \(\alpha(x)\leq a\) for all \(x\). Thus
\(g_a\) majorizes \(f\) at \(y\) if and only if \(\alpha\) is bounded
above by \(a\). If \(\alpha\) is unbounded above there is no quadratic
majorizer at \(y\). We can also define
\begin{equation}\phantomsection\label{eq-aplus}{ 
a_+:=\sup_x\alpha(x),
}\end{equation} and say that sharp quadratic majorization at \(y\) is
possible if \(a_+<+\infty\). We have majorization at \(y\) by \(g_a\) if
\(a\geq a_+\), we have sharp majorization if \(a=a_+\). It follows that
sharp quadratic majorizations exist if \(f''\) is bounded above.

We illustrate these concepts by applying them to low-degree polynomials.
First a cubic. Expand the cubic around \(y\) as
\begin{equation}\phantomsection\label{eq-cubexp}{
f(x)=f(y)+f'(y)(x-y)+\frac12f''(y)(x-y)^2+\frac16f'''(y)(x-y)^3.
}\end{equation} Thus \begin{equation}\phantomsection\label{eq-cubalp}{
\alpha(x)=f''(y)+\frac13f'''(y)(x-y).
}\end{equation} Since \(\alpha\) is always unbounded above, no quadratic
majorizer exists at any \(y\).

Now apply the same reasoning to a non-trivial quartic. We find
\begin{equation}\phantomsection\label{eq-quaalp}{
\alpha(x)=f''(y)+\frac13f'''(y)(x-y)+\frac{1}{12}f^{iv}(y)(x-y)^2,
}\end{equation} a quadratic in \(x\). Of course for a quartic
\(f^{iv}(y)\) is the same for every \(y\) and we may as well write
\(f^{iv}\). If \(f^{iv}\) is strictly positive the quadratic in
Equation~\ref{eq-quaalp} is unbounded above, and no quadratic
majorization exists at any \(y\). If \(f^{iv}\) is strictly negative
then \(\alpha\) has a maximum at \(x=y-2f'''(y)/f^{iv}\), and a sharp
quadratic majorization exists at any \(y\).

We can get some more information about \(a_+\) by differentiating
\(\alpha\).

\begin{theorem}[]\protect\hypertarget{thm-locmax}{}\label{thm-locmax}

Suppose there is an \(x\) where \(\alpha\) is two times differentiable
and \(a_+=\alpha(x)\). Then \begin{subequations}
\begin{align}
\frac{f(x)-f(y)}{x-y}&=\frac12(f'(x)+f'(y)),\label{eq-loc1}\\
f''(x)&\leq\frac{f'(x)-f'(y)}{x-y}.\label{eq-loc2}
\end{align}
Moreover
\begin{equation}
a_+=\frac{f'(x)-f'(y)}{x-y}.\label{eq-loc3}
\end{equation}
\end{subequations} If \eqref{eq-loc1} and the inequality in
\eqref{eq-loc2} is strict then \(\alpha\) has a local maximum at \(x\).

\end{theorem}

\begin{proof}
After some manipulation \eqref{eq-loc1} and \eqref{eq-loc2} are the
necessary conditions \(\alpha'(x)=0\) and \(\alpha''(x)\leq 0\) for a
local maximum. If \(\alpha'(x)=0\) and \(\alpha''(x)>0\) the conditions
are sufficient. If \(x\) satisfies \eqref{eq-loc1} then substitution in
Equation~\ref{eq-alpdef} gives \eqref{eq-loc3}.
\end{proof}

Note that we have not shown that \(\alpha\) always attains its maximum.
De Leeuw and Lange (\citeproc{ref-deleeuw_lange_A_09}{2009}) give the
example of the differentiable function \begin{equation}
f(x)=\begin{cases}
x^2&\text{ if }x\leq 1,\\
2x-1&\text{ otherwise},
\end{cases}\label{eq:dllexam}
\end{equation} which has \(\alpha(x)=0\) for \(x>1\) and \(\alpha(x)<2\)
for \(x\leq 1\), so that \(a_+=\sup_{x\leq 1}\alpha(x)=2\) and the
maximum does not exist.

Also, the conditions of Theorem~\ref{thm-locmax} cannot possibly show
that \(\alpha\) has a \emph{global} maximum at \(x\), and that
consequently \(g_a\) of Equation~\ref{eq-qmaj} with \(a\) given by
\eqref{eq-loc3} is a sharp quadratic majorizer.

Differentiation of \(\alpha\) gives the following result.

\begin{corollary}[]\protect\hypertarget{cor-incdec}{}\label{cor-incdec}

Suppose \(\alpha\) is differentiable. Then it is strictly increasing if
and only if for all \(x\)
\begin{equation}\phantomsection\label{eq-strinc}{
\frac{f(x)-f(y)}{x-y}<\frac12(f'(x)+f'(y)),
}\end{equation} and strictly decreasing if and only if for all \(x\)
\begin{equation}\phantomsection\label{eq-strdec}{
\frac{f(x)-f(y)}{x-y}<\frac12(f'(x)+f'(y)).
}\end{equation}

\end{corollary}

\begin{proof}
These are the conditions \(\alpha'(x)>0\) and \(\alpha'(x)<0\) for all
\(x\).
\end{proof}

If Equation~\ref{eq-strinc} or Equation~\ref{eq-strdec} is true then
\(\alpha\) does not have a maximum, and the supremum (possibly infinite)
is the limit of \(\alpha\) to \(-\infty\) or \(+\infty\).

Because of our robust loss functions we are especially interested in the
case that \(f\) is even. Setting \(x=-y\) makes both sides of
\eqref{eq-loc1} equal to zero. Thus \(\alpha\) has a stationary point at
\(x=-y\). If in addition \begin{equation}\phantomsection\label{eq-miny}{
f''(y)<\frac{f'(y)}{y}
}\end{equation} then \(\alpha\) has a local maximum at \(x=-y\).

\begin{theorem}[]\protect\hypertarget{thm-evena}{}\label{thm-evena}

Suppose \(f\) is even and twice-differentiable. If \(f'(x)/x\) is
decreasing on the positive real line then \(\alpha\) has a local maximum
at \(x=-y\).

\end{theorem}

\begin{proof}
If \(f'(x)/x\) is strictly decreasing its derivative is strictly
negative. Thus \(xf''(x)-f'(x)<0\) for \(x>0\). For an even function
\(f'(x)/x\) is strictly decreasing on the positive real line if and only
if it is strictly increasing on the negative real line. Thus
\(xf''(x)-f'(x)>0\) for \(x<0\). In both cases Equation~\ref{eq-miny}
follows.
\end{proof}

\subsection{Two Support Points}\label{sec-twosupport}

We can say more if it is known that the quadratic majorization has more
than one support point.

\begin{theorem}[]\protect\hypertarget{thm-twop}{}\label{thm-twop}

Suppose the quadratic \(g_a\) of Equation~\ref{eq-qmaj} majorizes \(f\)
at \(y\) and at \(z\not= y\). Then
\begin{equation}\phantomsection\label{eq-atwop}{
a=\frac{f'(z)-f'(y)}{z-y}
}\end{equation}

\end{theorem}

\begin{proof}
From Equation~\ref{eq-qmaj} we have \(g_a'(x)=f'(y)+a(x-y)\). But
because of Theorem~\ref{thm-diff} we must also have \(g_a'(z)=f'(z)\).
Thus \(g_a'(z)=f'(y)+a(z-y)=f'(z)\).
\end{proof}

Again, we have not shown that \(g_a\) with \(a\) from
Equation~\ref{eq-atwop} majorizes \(f\) at \(y\) and \(z\). Only the
reverse implication, which is that if \(g_a\) majorizes \(f\) at \(y\)
and \(z\) then \(g_a\) is uniquely determined by
Equation~\ref{eq-atwop}. In practice, even if one knows the support
points \(y\) and \(z\), one still has to prove majorization. This is
precisely how Heiser (\citeproc{ref-heiser_88}{1988}), Verboon and
Heiser (\citeproc{ref-verboon_heiser_94}{1994}), and Groenen, Giaquinto,
and Kiers (\citeproc{ref-groenen_giaquinto_kiers_03}{2003}) establish
their majorizations.

In his master's thesis Van Ruitenburg
(\citeproc{ref-vanruitenburg_05}{2005}) takes us one step further down
the necessary conditions road. To present his main result we need two
lemmas.

\begin{lemma}[]\protect\hypertarget{lem-fan}{}\label{lem-fan}

If different quadratics \(g\) and \(h\) majorize \(f\) at \(y\) then
either \(g\) strictly majorizes \(h\) or \(h\) strictly majorizes \(g\).

\end{lemma}

\begin{proof}
We have \begin{subequations}
\begin{align}
g(x)&=f(y)+f'(y)(x-y)+\frac12a_1(x-y)^2,\label{eq-gfunc}\\
h(x)&=f(y)+f'(y)(x-y)+\frac12a_2(x-y)^2.\label{eq-hfunc}.
\end{align}
\end{subequations} Thus \(g(x)-h(x)=\frac12(a_1-a_2)(x-y)^2\), which is
either strictly positive or strictly negative for \(x\not= y\).
\end{proof}

\begin{lemma}[]\protect\hypertarget{lem-ruit}{}\label{lem-ruit}

Suppose quadratics \(g\) and \(h\not= g\) majorize \(f\) at \(y\).
Suppose, in addition, that \(g\) majorizes \(f\) at \(z\not=y\). Then
\(h\) strictly majorizes \(g\) at \(y\).

\end{lemma}

\begin{proof}
By Lemma~\ref{lem-fan} one of \(g\) and \(h\) has to strictly majorize
the other. If \(g\) strictly majorizes \(h\) then \(h(z)<g(z)=f(z)\) and
thus \(h\) does not majorize \(f\), contrary to assumption. It follows
that \(h\) strictly majorizes \(g\) at \(y\).
\end{proof}

\begin{theorem}[]\protect\hypertarget{thm-ruit}{}\label{thm-ruit}

If the quadratic \(g\) majorizes \(f\) at \(y\) and at \(z\not= y\),
then \(g\) is a sharp majorizer of \(f\) at \(y\).

\end{theorem}

\begin{proof}
Directly from Lemma~\ref{lem-ruit}.
\end{proof}

Again our results simplify if the function \(f\) is even.

\begin{theorem}[]\protect\hypertarget{thm-evqu}{}\label{thm-evqu}

If \(f\) is even and the quadratic \(g\) majorizes \(f\) at \(y\) and
\(-y\), where \(y\not= 0\), then \(g\) is the even quadratic given by
\begin{equation}\phantomsection\label{eq-geven}{
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
}\end{equation} Moreover \(g\) is the sharp quadratic majorization of
\(f\) at \(y\) and \(-y\).

\end{theorem}

\begin{proof}
If \(f\) is even then \(f'\) is odd. Consequently
Equation~\ref{eq-atwop} becomes
\begin{equation}\phantomsection\label{eq-aeven}{
a=\frac{f'(y)}{y},
}\end{equation} which is even. Moreover because \(g\) majorizes \(f\) at
\(y\) \begin{subequations}
\begin{equation}
g(x)=f(y)+f'(y)(x-y)+\frac12\frac{f'(y)}{y}(x-y)^2,\label{eq-gatplusy}
\end{equation}
and because $g$ majorizes $f$ at $-y$
\begin{equation}
g(x)=f(y)-f'(y)(x+y)+\frac12\frac{f'(y)}{y}(x+y)^2.\label{eq-gatminy}
\end{equation}
\end{subequations} Averaging the two equations \eqref{eq-gatplusy} and
\eqref{eq-gatminy} for \(g\), and simplifying, gives the required
result. That the majorization is sharp follows from
Theorem~\ref{thm-ruit}.
\end{proof}

\subsection{Sufficient Conditions}\label{sufficient-conditions}

De Leeuw and Lange (\citeproc{ref-deleeuw_lange_A_09}{2009}) gives a way
to construct quadratic majorizers of a differentiable function on the
real line. We give a slightly more general version of their basic
theorem that provides a convenient way to deal with loss functions that
are not differentiable everywhere. Our proofs are different.

\begin{theorem}[]\protect\hypertarget{thm-wght}{}\label{thm-wght}

Suppose there is a function \(h\), concave on \(\mathbb{R}^+\), such
that \(f(x)=h(x^2)\). Then
\begin{equation}\phantomsection\label{eq-given}{
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
}\end{equation} is a sharp quadratic majorizer of \(f\) at \(y\)

\end{theorem}

\begin{proof}
By concavity for all \(u\in\mathbb{R}^+\) and for all
\(v\in\mathbb{R}^+\) for which \(h'(v)\) exists
\begin{equation}\phantomsection\label{eq-concave}{
h(u)\leq h(v)+h'(v)(u-v).
}\end{equation} Substituting \(u=x^2\) and \(v=y^2\) gives the quadratic
majorization \begin{equation}\phantomsection\label{eq-conc2}{
f(x)\leq f(y)+h'(y^2)(x^2-y^2),
}\end{equation} which is true for all real \(x\) and \(y\), provided
\(h\) is differentiable at \(y^2\). If it is then using
\(f'(y)=2yh'(y^2)\) in Equation~\ref{eq-conc2} gives
\begin{equation}\phantomsection\label{eq-concmaj}{
f(x)\leq f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
}\end{equation}
\end{proof}

Note that \(f(x)=h(x^2)\) implies that \(f\) is even. If we want to get
rid of the differentiability assumption altogether we can use the
majorization \[
f(x)\leq f(y)+a(y)(x^2-y^2)
\] where \(a(y)\) is any element of \(\partial h(y^2)\), the
superdifferential of \(h\) at \(y^2\). The superdifferential, which is
the analog of the subdifferential for concave functions (Border
(\citeproc{ref-border_18}{2018})), is non-empty on \(\mathbb{R}^+\),
except possibly at zero.

\begin{theorem}[]\protect\hypertarget{thm-sqrt}{}\label{thm-sqrt}

\(f(x)=h(x^2)\) for concave \(h\) on \(\mathbb{R}^+\) if and only if
\(f(\sqrt{x})\) is concave in \(x\), and, for differentiable \(f\), if
and only if \(f'(x)/x\) is decreasing on \(\mathbb{R}^+\).

\end{theorem}

\begin{proof}
The first part of the theorem merely states the obvious.
\end{proof}

Some quick examples of the use of Theorem~\ref{thm-wght} before we go to
the robust loss functions.

\begin{example}[]\protect\hypertarget{exm-sqrt}{}\label{exm-sqrt}

Suppose \(h\) is the square root, so that \(f(x)=\sqrt(x^2)=|x|\) and
for \(y\not=0\) we have \(f'(y)/y=1/|y|\). From Theorem~\ref{thm-wght}
we have the quadratic majorization \[
|x|\leq|y|+\frac12\frac{1}{|y|}(x^2-y^2)=\frac12\frac{1}{|y|}(x^2+y^2),
\] which is the AM/GM inequality. So AM/GM majorization of the absolute
value is sharp.

\end{example}

\begin{example}[]\protect\hypertarget{exm-quartic}{}\label{exm-quartic}

Suppose \(h(x)=\frac12x(1-\frac12x)\). Thus \(f\) is the even quartic \[
f(x)=\frac12x^2(1-\frac12x^2).
\] We have \(f'(y)/y=1-y^2,\) and thus the majorizer \[
g(x)=f(y)+\frac12(1-y^2)(x^2-y^2).
\] For all \(y^2<1\) \(g\) has its minimum at zero. If \(y^2>1\) the
majorizer is a concave quadratic, which has no minimum and is unbounded
below. If \(y^2=1\) the majorizer is the horizontal line \(x=\frac14\)
and the majorization method stops at a local maximum.

\end{example}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-quartic-1.pdf}}

}

\caption{\label{fig-quartic}A Quartic}

\end{figure}%

We now apply Theorem~\ref{thm-wght} to functions of the form
\begin{equation}\phantomsection\label{eq-fstressdef}{
\sigma_f(X):=\sum \omega_k\ f(\delta_k-d_k(X)),
}\end{equation} where \(f\) satisfies the conditions in the theorem. If
\begin{equation}\phantomsection\label{eq-fstressmaj}{
\eta(X):=\sum \omega_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),
}\end{equation} then \(\eta\) is a sharp quadratic majorization at
\(Y\).

In iteration \(k\) the robust smacof algorithm does a smacof step
towards minimization of \(\eta\) over \(X\). We can ignore the parts of
Equation~\ref{eq-fstressmaj} that only depend on \(Y\), and minimize
\begin{equation}\phantomsection\label{eq-fstressaux}{
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X))^2,
}\end{equation} with \begin{equation}\phantomsection\label{eq-wkdef}{
\omega_k(X^{(\nu)}):=\omega_k\frac{f'(\delta_k-d_k(X^{(\nu)}))}{2(\delta_k-d_k(Y))}.
}\end{equation} We then recomputes the weights \(\omega_k(X^{(\nu+1)})\)
and go to the smacof step again. This can be thought of as iteratively
reweighted least squares (IRLS), and also as nested majorization, with
the smacof majorization based on the Cauchy-Schwartz inequality within
the sharp quadratic majorization of the loss function based on
Theorem~\ref{thm-wght}.

\sectionbreak

\section{Power Smoothers}\label{power-smoothers}

We first discuss a class of smoothers of the absolute value function
that maintain most of its structure. They have a shift parameter \(c\)
that takes care of the non-differentiability. Although different
smoothers have different scales and interpretations for \(c\), we will
use the same symbol throughout. Also some smoothers have a power
parameter \(q\) that determines the shape of the loss function bowl.

\subsection{Charbonnier}\label{sec-charb}

The first, and perhaps most obvious, choice for smoothing the absolute
value function is \begin{equation}\phantomsection\label{eq-charf}{
f(x)=\sqrt{x^2 + c^2}.
}\end{equation} In the engineering literature Equation~\ref{eq-charf} is
known as Charbonnier loss, after Charbonnier et al.
(\citeproc{ref-charbonnier_blanc-feraud_aubert_barlaud_94}{1994}), who
were possibly the first researchers to use it in image restauration.
Ramirez et al.
(\citeproc{ref-ramirez_sanchez_kreinovich_argaez_14}{2014}) count the
number of computer operations and conclude that Equation~\ref{eq-charf}
is also the ``most computationally efficient smooth approximation to
\(|x|\)''.

For \(c>0\) we have \(f_c(x)>|x|\). If \(c\rightarrow 0\) then
\(f_c(x)\) decreases monotonically to \(|x|\). Also
\(\max_x|f_c(x)-|x||=c\) attained at \(x=0\), which implies uniform
convergence of \(f_c\) to \(|x|\).

By l'HÃ´pital \begin{subequations}
\begin{equation}
\lim_{x\rightarrow 0}\frac{\sqrt{x^2+c^2}-c}{\frac12x^2}=1.
\end{equation}
Of course also
\begin{equation}
\lim_{x\rightarrow\infty}\frac{\sqrt{x^2+c^2}}{|x|}=1
\end{equation}
and
\begin{equation}
\lim_{x\rightarrow\pm\infty}\sqrt{x^2+c^2}-|x|=0
\end{equation}
\end{subequations} Thus if \(x\) is much smaller than \(c\) then loss is
approximately a quadratic in \(x\), and if \(x\) is much larger than
\(c\) then loss is approximately the absolute value.

Loss function Equation~\ref{eq-charf} is infinitely many times
differentiable. Its first derivative is \begin{equation}
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,\label{eq:charg}
\end{equation} which converges, again in the sup-norm and uniformly, to
the sign function if \(c\rightarrow 0\). The IRLS weights are
\begin{equation}
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}\label{eq:charw}
\end{equation} which is clearly a decreasing function of \(x\) on
\(\mathbb{R}^+\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-char-1.pdf}}

}

\caption{\label{fig-char}Charbonnier Loss}

\end{figure}%

\subsection{Generalized Charbonnier}\label{generalized-charbonnier}

The loss function \((x^2+c^2)^\frac12\) smoothes \(|x|\). In the same
way generalized Charbonnier loss smoothes \(\ell_p\) loss \(|x|^q\). We
have a two-parameter family of loss functions in this case.
\begin{equation}\phantomsection\label{eq-gcharf}{
f_{c,q}(x):=(x^2+c^2)^{\frac12 q}
}\end{equation} \begin{equation}
w_{c,q}(x)=q(x^2+c^2)^{\frac12 q-1}\label{eq:gcharw}
\end{equation} which is non-increasing for \(q\leq 2\). Note that we do
not assume that \(q>0\), and consequently generalized Charbonnier loss
provides us with more flexibility than Charbonnier loss from
Equation~\ref{eq-charf}. Of course if \(q<0\) ``loss'' becomes ``gain'',
with a maximum at zero instead of a minimum. To get a proper loss
function, take the negative. Figure~\ref{fig-gchar} plots generalized
Charbonnier loss for some negative values of \(q\). We see that for
\(\alpha\rightarrow-\infty\) generalized Charbonnier loss aproximates
\(\ell_0\) loss.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-gchar-1.pdf}}

}

\caption{\label{fig-gchar}Generalized Charbonnier Loss}

\end{figure}%

\subsection{Barron}\label{barron}

There are a fair number of generalizations of the power smoother loss
functions in the engineering literature. We will discuss one nice
generalization from Barron (\citeproc{ref-barron_19}{2019}).

\begin{equation}\phantomsection\label{eq-barron}{
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).\
}\end{equation}

To quote Barron

\begin{quote}
Here \(\alpha\in\mathbb{R}\) is a shape parameter that controls the
robustness of the loss and \(c>0\) is a scale parameter that controls
the size of the loss's quadratic bowl near \(x=0\).
\end{quote}

A number of interesting special cases of Equation~\ref{eq-barron} are
obtained by selecting various values of the \(\alpha\) parameters. For
\(\alpha=1\) it becomes Charbonnier loss, and for \(\alpha=-2\) it is
Geman-McClure loss. There are also some limiting cases. For
\(\alpha\rightarrow 2\) Barron loss becomes squared error loss, for
\(\alpha\rightarrow 0\) it becomes Cauchy loss, and for
\(\alpha\rightarrow-\infty\) it becomes Welsch loss. Accordingly
\begin{equation}\phantomsection\label{eq-barronf}{
f'_{\alpha,c}(x)=\begin{cases}
\frac{x}{c^2}&\text{ if }\alpha=2,\\
\frac{2x}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{x}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{x}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
}\end{equation} and thus
\begin{equation}\phantomsection\label{eq-barronh}{
h_{\alpha,c}(x)=\begin{cases}
\frac{1}{c^2}&\text{ if }\alpha=2,\\
\frac{2}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{1}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{1}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
}\end{equation}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-barron-1.pdf}}

}

\caption{\label{fig-barron}Barron Loss}

\end{figure}%

\sectionbreak

\section{Convolution Smoothers}\label{convolution-smoothers}

Suppose \(\pi\) is a probability density, symmetric around zero, with
finite or infinite support, expectation zero, and variance one. Define
the convolution \[
f_c(x):=\frac{1}{c}\int_{-\infty}^{+\infty}|x-y|\ \pi(\frac{y}{c})dy.
\] Now \(c^{-1}\pi(y/c)\) is still a symmetric probability density
integrating to one, with expectation zero, but it now has variance
\(c^2\). Thus if \(c\rightarrow 0\) it becomes more and more like the
Dirac delta function and \(f_c(x)\) converges to the absolute value
function.

It is clear that we can use any scale family of probability densities to
define convolution smoothers. There is an infinite number of possible
choices, with finite or infinite support, smooth or nonsmooth, using
splines or wavelets, and so on. We give two quite different examples.

\subsection{Huber}\label{huber}

Take \[
\pi(x)=\begin{cases}\frac12 &\text{ if }|x|\leq 1,\\0&\text{ otherwise.}\end{cases}
\] Then \begin{equation}\phantomsection\label{eq-hubera}{
f_c(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&\text{ if }|x|\leq c,\\
|x|&\text{ otherwise}.
\end{cases}
}\end{equation}

The Huber function (Huber (\citeproc{ref-huber_64}{1964})) is
traditionally transformed linearly so that it is zero for \(x=0\). This
gives \begin{equation}\phantomsection\label{eq-huberb}{
f_c(x)=\begin{cases}
\frac12x^2&\text{ if }|x|<c,\\
c|x|-\frac12 c^2&\text{ otherwise}.
\end{cases}
}\end{equation} For robust estimation and IRLS it does not matter if we
use Equation~\ref{eq-hubera} or Equation~\ref{eq-huberb}. Our discussion
in the introduction suggests that if we just want a smoother of the
absolute value function, then Equation~\ref{eq-hubera} is the natural
choice, if we want a robust loss function that combines the advantages
of least squares and least absolute value then that leads us to
Equation~\ref{eq-huberb}.

Because Charbonnier loss behaves the same way as Huber loss, as absolute
value loss for large \(x\) and as squared loss for small \(x\), it is
also known as Pseudo-Huber loss.

The Huber function is differentiable, although not twice diffentiable.
Its derivative is \[
f'(x)=\begin{cases}
c&\text{ if }x\geq c,\\
x&\text{ if }|x|\leq c,\\
-c&\text{ if }x\leq -c.
\end{cases}
\] \[
\omega(x)=
\begin{cases}
\hfill\frac{c}{x}&\text{ if }x\geq c,\\
\hfill1&\text{ if }|x|\leq c,\\
-\frac{c}{x}&\text{ if }x\leq -c.
\end{cases}
\] The Huber function is even and differentiable. Moreover \(f'(x)/x\)
decreases from. Thus Theorem~\ref{thm-wght} applies.

The MDS majorization algorithm for the Huber loss is to update \(Y\) by
minimizing (or by performing one smacof step to decrease) \[
\sum \omega_k(Y)(\delta_k-d_k(X))^2
\] where \[
\omega_k(Y)=\begin{cases}
\omega_k&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac{c\omega_k}{|\delta_k-d_k(Y)|}&\text{ otherwise}.
\end{cases}
\]

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-huber-1.pdf}}

}

\caption{\label{fig-huber}Huber Loss}

\end{figure}%

\subsection{Gaussian}\label{gaussian}

In De Leeuw (\citeproc{ref-deleeuw_E_18f}{2018}) we also discussed the
convolution smoother proposed by Voronin, Ozkaya, and Yoshida
(\citeproc{ref-voronin_ozkaya_yoshida_15}{2014}). The idea is to use the
convolution of the absolute value function and a Gaussian pdf. \[
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
\]

Carrying out the integration gives

\[
f_c(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
\] The derivative is \[
f'_c(x)=2\Phi(x/c)-1
\] It may not be immediately obvious in this case that the weight
function \(f'(x)/x\) is non-increasing on \(\mathbb{R}^+\). We prove
that its derivative is negative on \((0,+\infty)\). The derivative of
\(f'(x)/x\) has the sign of \(xf''(x)-f'(x)\), which is
\(z\phi(z)-\Phi(z)+1/2\), with \(z=x/c\). It remains to show that
\(\Phi(z)-z\phi(z)\geq\frac12\), or equivalently that
\(\int_0^z\phi(x)dx-z\phi(z)\geq 0\). Now if \(0\leq x\leq z\) then
\(\phi(x)\geq\phi(z)\) and thus
\(\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)\), which completes the
proof.

\[
\omega_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
\]

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-gaussian-1.pdf}}

}

\caption{\label{fig-gaussian}Gaussian Convolution Loss}

\end{figure}%

\sectionbreak

\section{A Bouquet of Loss Functions}\label{a-bouquet-of-loss-functions}

In the early seventies, after the pioneering mostly theoretical work in
robust statistics of Huber, Hampel, and Tukey, the mainframe computer
allowed statisticians to make large-scale comparisons of many robust
loss functions. The most impressive of such comparisons was the
Princeton Robustness Study (Andrews et al.
(\citeproc{ref-andrews_bickel_hampel_huber_rogers_tukey_72}{1972})).

In Holland and Welsch (\citeproc{ref-holland_welsch_77}{1977}) the
computer package ROSEPACK was introduced that made it relatively easy to
compute robust estimators using several different loss functions. Eight
different weight functions were implemented as options. Somewhat later
Coleman et al.
(\citeproc{ref-coleman_holland_kaden_klema_peters_80}{1980}) made an
more modern computer implementation available, using the same eight
weight functions, which was not limited to mainframes.

We have implemented the same eight weight functions in smacofRobust.
Below we give formulas for the loss function, the influence function,
and the weight function. One of the eight is Huber loss, which we
already discussed in the convolution section. We graph the remaining
seven loss functions for selected values of the ``tuning constants''
\(c\).

Holland and Welsch (\citeproc{ref-holland_welsch_77}{1977}), following
Andrews et al.
(\citeproc{ref-andrews_bickel_hampel_huber_rogers_tukey_72}{1972}),
distnguish between ``hard redescenders'' that have an influence function
\(f'\) equal to zero if \(x\) is large enough (Andrews, Tukey, and
Hinich loss), ``soft redescenders'' with influence functions asymptotic
to zero for large \(x\) (Cauchy, Welsch loss), and loss functions with a
monotone influence function (Huber, Logistic, Fair loss)

\subsection{Andrews}\label{andrews}

The first loss function in this section is taken from Andrews et al.
(\citeproc{ref-andrews_bickel_hampel_huber_rogers_tukey_72}{1972}).

\begin{align}
f(x)&=\begin{cases}
c^2(1-\cos(x/c))&\text{ if }|x|\leq\pi c,\\
2c^2&\text{ otherwise.}
\end{cases}\\
f'(x)&=\begin{cases}
c\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}\\
\omega(x)&=\begin{cases}
(x/c)^{-1}\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}
\end{align}

Because \(\cos\) is even and \(\sin(x)/x\) decreases on \([0,\pi]\)
Theorem~\ref{thm-wght} applies.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-andrews-1.pdf}}

}

\caption{\label{fig-andrews}Andrews Loss}

\end{figure}%

\subsection{Tukey}\label{tukey}

The usual reference for Tukey loss is Beaton and Tukey
(\citeproc{ref-beaton_tukey_74}{1974}), although closely related hard
redescenders are also in Andrews et al.
(\citeproc{ref-andrews_bickel_hampel_huber_rogers_tukey_72}{1972}).

\begin{align}
f(x)&=\begin{cases}
\frac{c^2}{6}\left(1-\left(1-(x/c)^2\right)^3\right)&\text{ if } |x|\leq c,\\
\frac{c^2}{6}&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
x\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
\omega(x)&=\begin{cases}
\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

The conditions of Theorem~\ref{thm-wght} are clearly satisfied.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-tukey-1.pdf}}

}

\caption{\label{fig-tukey}Tukey Loss}

\end{figure}%

\subsection{Hinich}\label{hinich}

Hinich loss, from Hinich and Talwar
(\citeproc{ref-hinich_talwar_75}{1975}), is somewhat special because it
is not differentiable at \(c\). For \(x\not= c\) and \(x>0\) the
function \(f'(x)/x\) is discontinuous, but non-increasing on
\([0,+\infty)\).

\begin{align}
f(x)&=\begin{cases}
\frac12 x^2&\text{ if } |x|\leq c,\\
\frac12 c^2&\text{ otherwise}.
\end{cases}\\
g(x)&=\begin{cases}
x&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
h(x)&=\begin{cases}
1&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-hinich-1.pdf}}

}

\caption{\label{fig-hinich}Hinich Loss}

\end{figure}%

\subsection{Cauchy}\label{cauchy}

Cauchy loss seems to have many names. Black and Anandan
(\citeproc{ref-black_anandan_96}{1996}) call it Lorentzian loss, and
Holland and Welsch (\citeproc{ref-holland_welsch_77}{1977}) call it
t-likelihood loss. It is related to the Cauchy distribution, which is
Student's t distribution with one degree of freedom.

Mlotshwa, Van Deventer, and Sergeevna Bosman
(\citeproc{ref-mlotshwa_vandeventer_bosman_23}{2023})

\begin{align}
f(x)&=\frac12c^2\log(1+\{\frac{x}{c}\}^2),\\
f'(x)&=x\frac{1}{\{1+\frac{x}{c}\}^2},\\
\omega(x)&=\frac{1}{\{1+\frac{x}{c}\}^2}
\end{align}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-cauchy-1.pdf}}

}

\caption{\label{fig-cauchy}Cauchy Loss}

\end{figure}%

\subsection{Welsch}\label{welsch}

Dennis Jr and Welsch (\citeproc{ref-dennis_welsch_78}{1978})

Leclerc loss

\begin{align}
f(x)&=\frac12c^2[1-\exp(-\{\frac{x}{c}\}^2)],\\
f'(x)&=x\exp(-\{\frac{x}{c}\}^2,\\
\omega(x)&=\exp(-\{\frac{x}{c}\}^2),
\end{align}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-welsch-1.pdf}}

}

\caption{\label{fig-welsch}Welsch Loss}

\end{figure}%

\subsection{Logistic}\label{logistic}

\begin{align}
f(x)&=c^2[\log(\cosh(x/c))],\\
f'(x)&=c\tanh(x/c),\\
\omega(x)&=(x/c)^{-1}\tanh(x/c).
\end{align}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-logistic-1.pdf}}

}

\caption{\label{fig-logistic}Logistic Loss}

\end{figure}%

\subsection{Fair}\label{fair}

\begin{align}
f(x)&=c^2\{|x|/c-\log(1+|x|/c)\},\\
f'(x)&=x(1+(|x|/c))^{-1},\\
\omega(x)&=(1+(|x|/c))^{-1}.
\end{align}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/fig-fair-1.pdf}}

}

\caption{\label{fig-fair}Fair Loss}

\end{figure}%

\sectionbreak

\section{Examples}\label{examples}

\subsection{Gruijter}\label{gruijter}

The example we use are dissimilarities between nine Dutch political
parties, collected by De Gruijter (\citeproc{ref-degruijter_67}{1967}).
They are averages over a politically heterogenous group of 100
introductory psychology students, and consequently they regress to the
mean. Any reasonable MDS analysis of these data would at least allow for
an additive constant.

Some background on Dutch politics around that time may be useful.

\begin{itemize}
\tightlist
\item
  CPN - Communists.
\item
  PSP - Pacifists, left-wing.
\item
  PvdA - Labour, Democratic Socialists.
\item
  D'66 - Pragmatists, nether left-wing nor right-wing, brand new in
  1967.
\item
  KVP - Christian Democrats, catholic.
\item
  ARP - Christian Democrats, protestant.
\item
  CHU - Christian Democrats, protestant.
\item
  VVD - Liberals, European flavour, conservative.
\item
  BP - Farmers, protest party, right-wing.
\end{itemize}

The dissimilarities are in the table below.

\begin{longtable}[]{@{}lrrrrrrrrr@{}}
\toprule\noalign{}
& KVP & PvdA & VVD & ARP & CHU & CPN & PSP & BP & D66 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
KVP & 0.00 & 5.63 & 5.27 & 4.60 & 4.80 & 7.54 & 6.73 & 7.18 & 6.17 \\
PvdA & 5.63 & 0.00 & 6.72 & 5.64 & 6.22 & 5.12 & 4.59 & 7.22 & 5.47 \\
VVD & 5.27 & 6.72 & 0.00 & 5.46 & 4.97 & 8.13 & 7.55 & 6.90 & 4.67 \\
ARP & 4.60 & 5.64 & 5.46 & 0.00 & 3.20 & 7.84 & 6.73 & 7.28 & 6.13 \\
CHU & 4.80 & 6.22 & 4.97 & 3.20 & 0.00 & 7.80 & 7.08 & 6.96 & 6.04 \\
CPN & 7.54 & 5.12 & 8.13 & 7.84 & 7.80 & 0.00 & 4.08 & 6.34 & 7.42 \\
PSP & 6.73 & 4.59 & 7.55 & 6.73 & 7.08 & 4.08 & 0.00 & 6.88 & 6.36 \\
BP & 7.18 & 7.22 & 6.90 & 7.28 & 6.96 & 6.34 & 6.88 & 0.00 & 7.36 \\
D66 & 6.17 & 5.47 & 4.67 & 6.13 & 6.04 & 7.42 & 6.36 & 7.36 & 0.00 \\
\end{longtable}

The reason we have chosen this example is partly because CPN and BP are
outliers, and we can expect the robust loss functions to handle outlying
dissimilarities differently from the bulk of the data.

Unless otherwise indicated we run smacofRobust() with a maximum of
10,000 iterations, and we decide that we have convergence if the
difference between consecutive stress values is less than
\ensuremath{10^{-15}}. We perform one single smacof iteration between
the updates of the weights. For each analysis we show the configuration
plot, the Shepard plot, and a histogram of the absolute values of the
residuals. In the Shepard plot points corresponding to the eight
CPN-dissimilarities are labeled ``C'', while BP-dissimilarities are
``B''.

\subsubsection{Least Squares}\label{least-squares}

We start with a least squares analysis, actually with Huber loss with
\(c=10\), which for these data is equivalent to least squares. The
process converges in 859 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pxls-1.pdf}}

}

\caption{Gruijter Configuration Least Squares}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pdls-1.pdf}}

}

\caption{Grujter Shepard Plot Least Squares}

\end{figure}%

The Shepard plot clearly shows why an additive constant would be very
beneficial in this case.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/phlsh-1.pdf}}

}

\caption{Gruijter Histogram Least Squares Residuals}

\end{figure}%

\subsubsection{Least Absolute Value}\label{least-absolute-value}

For our LAV smacof we use engine smacofCharbonnier with \(c=.001\). We
have convergence in 637 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pxav-1.pdf}}

}

\caption{Gruijter Configuration Least Absolute Value}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pdav-1.pdf}}

}

\caption{Gruijter Shepard Plot Least Absolute Value}

\end{figure}%

In the Shepard plot we see that there are a number of dissimilarities
which are fitted exactly. If we count them there are about 15-20. Note
that configurations in two dimensions have \((n-1)+(n-2)=2n-3\) degrees
of freedom, which is 15 in this case. Thus if we take the 15
dissimilarities which are fitted exactly, give them weight one, give all
other 21 dissimilarities weight zero, and do a regular non-robust smacof
analysis using these weights, then we will have perfect fit in two
dimensions, and the solution will be the LAV solution. All this is
easier said than done, because it presumes that we use Charbonnier loss
with \(c=0\) and that we are able to decide which residuals are exactly
equal to zero. The LAV analysis also suggests the possibility of a huge
number of local minima, because there are so many ways to pick 15 out of
36 dissimilarities.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/phavh-1.pdf}}

}

\caption{Gruijter Histogram Least Absolute Value Residuals}

\end{figure}%

\subsubsection{Huber}\label{huber-1}

smacofHuber with \(c=1\) converges in 165 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pxhme-1.pdf}}

}

\caption{Gruijter Configuration Huber c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pdhme-1.pdf}}

}

\caption{Gruijter Shepard Plot Huber c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/phmeh-1.pdf}}

}

\caption{Gruijter Histogram Huber Residuals}

\end{figure}%

\subsubsection{Tukey}\label{tukey-1}

smacofTukey with \(c=2\) converges in 180 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pxtmed-1.pdf}}

}

\caption{Gruijter Configuration Tukey c = 2}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pdtmed-1.pdf}}

}

\caption{Gruijter Shepard Plot Tukey c = 2}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/phtuh-1.pdf}}

}

\caption{Gruijter Histogram Tukey Residuals}

\end{figure}%

\subsection{Rothkopf}\label{rothkopf}

Our second example are the Rothkopf Morse data (Rothkopf
(\citeproc{ref-rothkopf_57}{1957})), which have a better fit and have
fewer outliers than the Gruijter data. We used the asymetric confusion
matrix from the smacof package (De Leeuw and Mair
(\citeproc{ref-deleeuw_mair_A_09c}{2009})) and defined dissimilarities
by the Shepard-Luce formula \[
\delta_{ij}=-\log\frac{p_{ij}p_{ji}}{p_{ii}p_{jj}}.
\]

\subsubsection{Least Squares}\label{least-squares-1}

For least squares we use the smacofHuber engine with \(c=25\), well
outside the range of the residuals. We have convergence in 213
iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpxls-1.pdf}}

}

\caption{Rothkopf Configuration Least Squares}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mkpdls-1.pdf}}

}

\caption{Rothkopf Shepard Plot Least Squares}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mphlsh-1.pdf}}

}

\caption{Rothkopf Histogram Least Squares Residuals}

\end{figure}%

\subsubsection{Least Absolute Value}\label{least-absolute-value-1}

For least absolute value we use Chardonnier loss with \(c=.001\). We
have convergence in 2291 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpxav-1.pdf}}

}

\caption{Rothkopf Configuration Least Absolute Value}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpdav-1.pdf}}

}

\caption{Rothkopf Shepard Plot Least Absolute Value}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mphlavh-1.pdf}}

}

\caption{Rothkopf Histogram Least Absolute Value Residuals}

\end{figure}%

\subsubsection{Huber}\label{huber-2}

smacofHuber with \(c=1\) converges in 680 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpxhme-1.pdf}}

}

\caption{Rothkopf Configuration Huber c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpdhme-1.pdf}}

}

\caption{Rothkopf Shepard Plot Huber c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mphmeh-1.pdf}}

}

\caption{Rothkopf Histogram Huber Residuals}

\end{figure}%

\subsubsection{Tukey}\label{tukey-2}

Tukey with \(c=1\) converges in 812 iterations.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mpxtu-1.pdf}}

}

\caption{Rothkopf Configuration Tukey c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/pdttu-1.pdf}}

}

\caption{Rothkopf Shepard Plot Tukey c = 1}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{av_files/figure-pdf/mphtuh-1.pdf}}

}

\caption{Rothkopf Histogram Tukey Residuals}

\end{figure}%

\sectionbreak

\section{Literature}\label{sec-literature}

The literature on results like Theorem~\ref{thm-wght} and
Theorem~\ref{thm-sqrt} is difficult to review. There are various reasons
for that. Relevant results have been published in robust statistics,
computational statistics, optimization, location analysis, image
restoration, sparse recovery. As is often the case, there are not many
references between fields, almost everything is within. Even the names
of the loss functions differ between fields. Much of it is hard to find
in conference proceedings. Also, in most cases, the authors have
specific applications in mind, which they then embed in a likelihood,
Bayesian, linear regression, logistic regression, facility location, or
EM framework and language.

De Leeuw and Lange (\citeproc{ref-deleeuw_lange_A_09}{2009}) give some
references to previous work on results like Theorem~\ref{thm-wght},
notably Groenen, Giaquinto, and Kiers
(\citeproc{ref-groenen_giaquinto_kiers_03}{2003}), Jaakkola and Jordan
(\citeproc{ref-jaakkola_jordan_00}{2000}), and Hunter and Li
(\citeproc{ref-hunter_li_05}{2005}). In these earlier papers we do not
find Theorem~\ref{thm-wght} in its full generality. In Groenen,
Giaquinto, and Kiers (\citeproc{ref-groenen_giaquinto_kiers_03}{2003})
majorization of the log logistic function is considered. Besides
requiring equality of the function and the majorizing quadratic at the
support point \(y\) they also require equality at \(-y\) and then check
that the resulting quadratic is indeed a majorizer. In Jaakkola and
Jordan (\citeproc{ref-jaakkola_jordan_00}{2000}) also consider a
symmetrized version of the log logistic function. They note that the
resulting function is a convex funcion of \(x^2\), and use a linear
majorizer at \(x^2\) to obtain a quadratic majorization. Hunter and Li
(\citeproc{ref-hunter_li_05}{2005}) come closest to
Theorem~\ref{thm-wght}. In their proposition 3.1 they approximate the
general penalty function they use for varable selection at \(y\) by a
quadratic with coefficient \(f'(y)/2y\), and then show that it provides
a quadratic majorization. In neither of the three papers there is a
notion of sharp quadratic majorization.

I will discuss some of the literature under the headings ``robust
statistics'', ``location analysis'', and ``sparse recovery''. Since I
most definitely am not an expert in either of these three fields the
literature reviews will be biased and incomplete. A final section, where
I am somewhat more sure-footed, is ``multivariate analysis''.

\subsection{Robust Statistics}\label{robust-statistics}

In robust statistics it has been known for a long time that iterative
reweighted least squares (IRLS) with weights \(f'(x)/x\) gives a
quadratic majorization algorithm. This result, and the corresponding
IRLS algorithm, is often attributed to Beaton and Tukey
(\citeproc{ref-beaton_tukey_74}{1974}).

\subsection{Location Analysis}\label{location-analysis}

In location analysis the first majorization/IRLS method is generally
attributed to a 16-year old Hungarian mathematics prodigy (Weiszfeld
(\citeproc{ref-weiszfeld_37}{1937})). His algorithm avant-la-lettre was
intended to find the minimum of a function of the form
\begin{equation}\phantomsection\label{eq-web}{
\sigma(x)=\sum_k w_kd_k(x),
}\end{equation} where \(d_k(x)=\|x-y_k\|\), over \(x\) in
\(\mathbb{R}^n\). The \(y_k\) are known locations, called \emph{anchors}
in the literature, and the norm is Euclidean. Actually Weiszfeld
(\citeproc{ref-weiszfeld_37}{1937}) did not use weights \(w_k\) and
worked in three-dimensional space. There is an English translation of
Weiszfeld's paper, with bibliography and comments, in Weiszfeld and
Plastria (\citeproc{ref-weiszfeld_plastria_09}{2009}).

The problem of minimizing Equation~\ref{eq-web} is known under various
names, usually consisting of one of the seven different non-empty
selections from the triple (Torricelli, Fermat, Weber). The history of
the problem is discussed, for example, in Plastria
(\citeproc{ref-plastria_11}{2011}).

Weiszfeld first acknowledges that Sturm (\citeproc{ref-sturm_84}{1884})
has already established the existence and uniqueness of the minimum
point. He then proceeds to give three new proofs. We are interested in
the first one, described in his first theorem. It defines the iterative
sequence \begin{equation}\phantomsection\label{eq-wup}{
x^{(\nu+1)}=\frac{\sum\omega_k(x^{(\nu)})y_k}{\sum\omega_k(x^{(\nu)})}
}\end{equation} with weights
\begin{equation}\phantomsection\label{eq-ww}{
\omega_\nu(x)=\frac{1}{d_k(x^{(\nu)})}.
}\end{equation} The proof then consists of showing that the sequence
converges to the unique minimum point of Equation~\ref{eq-web}. It would
have been nice if we were told where Equation~\ref{eq-wup} came from,
but it is simply taken as the starting point.

We can guess what suggested this particular sequence. Observe first that
the correponding problem with \(d_k(x)\) replaced by \(d_k^2(x)\) is
easy to solve. The solution is simply the weighted mean of the \(y_k\).
This suggests the rewrite \begin{equation}\phantomsection\label{eq-rew}{
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k(x)}d_k^2(x),
}\end{equation} which in turn suggests Equation~\ref{eq-wup}.
Alternatively, differentiate the loss in Equation~\ref{eq-web} and set
the partials equal to zero. This gives
\begin{equation}\phantomsection\label{eq-wstatio1}{
\sum_k w_k\frac{1}{d_k(x)}(x-y_k)=0,
}\end{equation} or \begin{equation}\phantomsection\label{eq-wstatio2}{
x=\frac{\sum\omega_k(x)y_k}{\sum\omega_k(x)}.
}\end{equation} Sturm (\citeproc{ref-sturm_84}{1884}) mentions that he
derived his existence and uniqueness theorem without using
differentiation, but he mentions a paper by Lorenz LindelÃ¶f from 1866
which derives his necessary conditions using differentiation (I have not
been able to find a copy of that paper). The discussion is a clear
example of the nineteenth century tension between using synthetic
(geometric) methods or analytic (calculus) methods.

Even more synthetic were the methods in a paper by LamÃ© and Clapeyron of
1829. (I have not been able to find a copy of that paper either). There
is a partial translation in Franksen and Grattan-Guinness
(\citeproc{ref-franksen_grattan-guinness_89}{1989}). LamÃ© and Clapeyron
suggest solving their ``moindre distances'' problems, which generalize
the single facility location problem in various ways, by ingeneous
systems of pulleys. There is a translation of the general principles
section of the LamÃ© and Clapeyron paper in Franksen and Grattan-Guinness
(\citeproc{ref-franksen_grattan-guinness_89}{1989}). From that
translation we read in General Principle 9 their defense of the
mechanical method they propose.

\begin{quote}
But if one considers that the proposed problem is totally insoluble, in
its entire generality, by the current means of analysis and geometry;
that it is only in very special and very simple cases that one may
obtain a complete graphical solution; that finally in the applications
the data themselves are only approximate; one will be forced to adrmt
that in the state of imperfection in which algebraic analysis is still
found today, the manner of solution in question here is the only one
which obtains for the proposed problem.
\end{quote}

In General Principle 21 they suggest an iterative method of ``trial and
error'' to solve the weighted location problem, where the weights in
each iteration are adjusted by multiplying them with the inverse of the
distances in the previous iteration. Thus they propose the Weiszfeld
algorithm, albeit in a version using pulleys.

Over the years the location problem has been generalized in numerous
directions, to multiple locations, to using different norms, to unknown
anchors, to nonlinear manifolds, and to obnoxious anchors you want to be
far from. A good recent overview is Beck and Sabach
(\citeproc{ref-beck_sabach_15}{2015}). A paper close in spirit to our
paper is Aftab, Hartley, and Trumpf
(\citeproc{ref-aftab_hartley_trumpf_15}{2015}), which has
generalizations to \(\ell_q\) norms and to Riemannian manifolds of
non-negative curvature.

There is a huge literature on the convergence of the Weiszfeld
algorithm. As in our Section~\ref{sec-amgm} we can simply use AM/GM
inequality. Thus \begin{equation}\phantomsection\label{eq-webamgm}{
\|x-y_\nu\|\leq\frac12\frac{1}{\|x^{(\nu)}-y_\nu\|}(\|x-y_\nu\|^2+\|x^{(\nu)}-y_\nu\|^2),
}\end{equation} which immediately gives Equation~\ref{eq-wup}.
Convergence follows from the general majorization of MM theory.

Unlike robust smacof the Toricelli-Fermat-Weber problem is convex, and
consequently has no problems with non-global local minima. A most
elegant proof of convergence using the tools of modern convex analysis
is in Mordukhovich and Nam (\citeproc{ref-mordukhovich_nam_19}{2019}).
Older proofs sometimes have difficulty dealing with cases in which the
iterates coincide with one of the anchors or in which the solution is
actually one of the anchors. This creates problems similar to the
problems in our Section~\ref{sec-zero}, but in this simple case the
problem is can be completely resolved using convexity and has no serious
algorithmic consequences.

In a straightforward generalization of the Toricelli-Fermat-Weber
problem , which is particularly relevant for the developments in our
paper, Katz (\citeproc{ref-katz_69}{1969}) proposes to minimize
\begin{equation}\phantomsection\label{eq-katzp}{
\sigma(x)=\sum_{k=1}^mw_kd_k^q(x),
}\end{equation} and even
\begin{equation}\phantomsection\label{eq-katzf}{
\sigma(x)=\sum_{k=1}^mw_kf_k(d_k(x))
}\end{equation} with Euclidean distances and \(f_k\) functions defined
on the non-negative reals. Note that if \(f\) is convex ands increasing
then \(\sigma\) of Equation~\ref{eq-katzf} is convex. If \(q\geq 1\)
then \(\sigma\) of Equation~\ref{eq-katzp} is convex.

The algorithm Katz suggests for minimizinf \(\sigma\) of
Equation~\ref{eq-katzp} generalizes the decompostion in
Equation~\ref{eq-rew} to
\begin{equation}\phantomsection\label{eq-katzdec}{
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k^{2-q}(x)}d_k^2(x),
}\end{equation} which leads to
\begin{equation}\phantomsection\label{eq-katzupdp}{
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)}}}.
}\end{equation} For Equation~\ref{eq-katzf}, analogous with
Equation~\ref{eq-wstatio1}, we set the derivative of
Equation~\ref{eq-katzf} equal to zero. Thus we solve
\begin{equation}\phantomsection\label{eq-katzder}{
\sum_{k=1}^mw_k\frac{f_k'(d_k(x))}{d_k(x)}(x-y_k)=0,
}\end{equation} and the iteration becomes
\begin{equation}\phantomsection\label{eq-katzupdf}{
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}}.
}\end{equation} The conditions in Katz (\citeproc{ref-katz_69}{1969}) on
\(f\) needed for the convergence proof, and also the proof itself, are
rather complicated. We can simply use the conditions of
Theorem~\ref{thm-wght} that \(f'(x)/x\) is non-increasing on the
non-negatives reals to construct a quadratic majorization algorithm in
which we minimize \begin{equation}\phantomsection\label{eq-katzmajor}{
\sum_{k=1}^m\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)})}{d_k(x^{(\nu)})}d_k^2(x)
}\end{equation} to find \(x^{k+1}\). This gives directly the update
Equation~\ref{eq-katzupdp}.

\subsection{Sparse Recovery}\label{sparse-recovery}

This is a field which is difficult to delineate. A somewhat ad-hoc
definition is recovering complete information from incomplete
information, often in the context of specific engineering problems.
There is overlap with signal detection, image analysis, matrix
completion, \ldots{} But ``sparse recovery'' scientific activities
``sparse recovery'' could be extended far beyond these boundaries. Since
classical statistics infers properties of the population from those of a
sample it is a form of sparse recovery. Since science infers properties
of the real world from outcomes of experiments it is sparse recovery
too.

\subsection{Multivariate Analysis}\label{multivariate-analysis}

The smacof majorization method for multidimensional scaling was first
presented at the \emph{US-Japan Seminar on Theory, Methods and
Applications of Multidimensional Scaling and Related Techniques} at UCSD
in La Jolla, August 1975. Shortly after that I read the basic EM paper
by Dempster, Laird, and Rubin
(\citeproc{ref-dempster_laird_rubin_77}{1977}), and shortly after that I
realized that smacof and EM were both special cases of a general
minimization strategy, which I called majorization at the time. In June
1978 both Nan Laird and I attended the \emph{Fifth International
Symposium on Multivariate Analysis} at the University of Pittsburgh. I
remember mentioning majorization, excitedly, to Nan on the conference
bus.

The smacof majorization method was fully discussed in De Leeuw
(\citeproc{ref-deleeuw_C_77}{1977}), De Leeuw and Heiser
(\citeproc{ref-deleeuw_heiser_C_77}{1977}), and De Leeuw and Heiser
(\citeproc{ref-deleeuw_heiser_C_80}{1980}). The familiar picture
illustrating two steps of the general majorization algorithm first
appears in De Leeuw (\citeproc{ref-deleeuw_A_88b}{1988a}). But unlike
EM, which took off as a rocket in 1977, the general idea of majorization
remained unpublished, until De Leeuw
(\citeproc{ref-deleeuw_C_94c}{1994}) and Heiser
(\citeproc{ref-heiser_95}{1995}). Majorization was used regularly in the
Gifi project. The book Gifi (\citeproc{ref-gifi_B_90}{1990}), which is a
version of 1981 lecture notes, mentions majorization only once, but
since then a stream of papers and dissertations from the Data Theory
department in Leiden using majorization appeared. Heiser
(\citeproc{ref-heiser_95}{1995}) mentions most of them. In De Leeuw
(\citeproc{ref-deleeuw_C_88b}{1988b}) another large majorization
subfield, the \emph{aspect approach} to multivariate analysis, was
developed. In section 7 of that paper the general
majorization/minorization approach to optimization is outlined, possibly
for the first time in print.

Robust versions of low rank matrix approximation, a.k.a. principal
component analysis, were first considered by Gabriel and Odoroff
(\citeproc{ref-gabriel_odoroff_84}{1984}). They start by discussing the
alternating least squares algorithm for least squares weighted matrix
approximation of Gabriel and Zamir
(\citeproc{ref-gabriel_zamir_79}{1979}). The alternating is to compute
new row scores for currently fixed column scores by linear regression,
and then computing new column scores corresponding with the new row
scores, again by linear regression. Gabriel and Odoroff
(\citeproc{ref-gabriel_odoroff_84}{1984}) suggest to replace the linear
least squares weighted averages in each of the two stages by medians or
trimmed means to get a robust PCA. There is no sign of a convergence
proof, but there is the suggestion to use alternating least absolute
value methods to minimize the sum of absolute residuals of the matrix
approximation. This suggestion was taken up by Verboon and Heiser
(\citeproc{ref-verboon_heiser_94}{1994}) using the majorization approach
and the Huber and Tukey robust loss functions. Their robust PCA method
is very similar to our robust MDS method, but the presentation of their
method has some magical elements. The Huber and Tukey majorization
functions are presented without any discussion where they came from, and
it is then verified that they are indeed majorizations. There is clearly
nothing wrong with this, but using our Theorem~\ref{thm-wght} gives a
more general and more direct approach.

Heiser (\citeproc{ref-heiser_86}{1986}) was the first to connect the
Weiszfeld problem with correspondence analysis and multidimensional
scaling, emphasizing the majorization aspects. As we have seen in Heiser
(\citeproc{ref-heiser_87}{1987}) and Heiser
(\citeproc{ref-heiser_88}{1988}) he constructed majorization algorithms
for multidimensional scaling and correspondence analysis.

The IRLS approach to robustifying multivariate matrix approximation
techniques could easily lead to a large and varied number of
publications. There are some excellent examples making their way through
the usual publication channels. I will just give two recent examples,
with good bibliographies. They are Huber Principal Component Analysis
(He et al. (\citeproc{ref-he_li_liu_zhou_23}{2023})) and Cauchy Factor
Analysis (Li (\citeproc{ref-li_24}{2024})).

\sectionbreak

\section{Discussion}\label{discussion}

\subsection{Bounding the Second
Derivative}\label{bounding-the-second-derivative}

In some cases our basic theorems may not apply, but there may be an
alternative way to majorize loss. In fact, this is classic quadratic
bounding as in Vosz and Eckhardt (\citeproc{ref-vosz_eckhardt_80}{1980})
or BÃ¶hning and Lindsay (\citeproc{ref-boehning_lindsay_88}{1988}). As
before, we want to minimize \(\sum \omega_k\ f(\delta_k-d_k(X))\), but
now we suppose that there is a \(K>0\) such that \(f''(x)\leq K\). We
then have the majorization

\begin{equation}\phantomsection\label{eq-qbound}{
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
}\end{equation} and in iteration \(k\) we minimize, or at least
decrease, \begin{equation}\phantomsection\label{eq-qiter}{
\sum \omega_k\left[d_k(X)-\{d_k(X^{(\nu)})-K^{-1}f'(\delta_k-d_k(X^{(\nu)}))\}\right]^2
}\end{equation} Note that in this algorithm the weights do not change.
Instead of fitting a fixed target with moving weights, we fit a moving
target with fixed weights.

We can apply bounding the second derivative, for example, to Charbonnier
loss, using the inequality
\begin{equation}\phantomsection\label{eq-chark}{
f_c''(x)=(x^2 + c^2)^{-\frac12}-x^2(x^2 + c^2)^{-\frac32}\leq(x^2 + c^2)^{-\frac12}\leq c^{-1},
}\end{equation} Of course this method requires that the second
derivative exists at \(x\). Although I have not done any comparisons it
will probably require more iterations and take longer than the method in
Section~\ref{sec-charb}.

The paper by Vosz and Eckhardt (\citeproc{ref-vosz_eckhardt_80}{1980})
deserves some special mention here.

\[
\mathcal{D}^2\sigma(x)=\sum\frac{1}{d_i(x)}\left\{I-\frac{(x-y_i)(x-y_i)'}{d_i^2(x)}\right\}
\]

\subsection{Fixed Weights}\label{fixed-weights}

One could also consider using the fixed weights in regular non-robust
smacof to achieve some form of robustness. Redefine stress as
\begin{equation}\phantomsection\label{eq-wstress}{
\sigma(X):=\sum_k \omega_kf(\delta_k)(\delta_k-d_k(X))^2
}\end{equation} For example, we can choose a negative power for \(f\),
so that it downweights the large dissimilarities. If the dissimilarities
is large, then it should have less influence on the fit, and thus on the
solution \(X\). This type of fixed power-weighting is used in various
places (De Leeuw and Heiser (\citeproc{ref-deleeuw_heiser_C_80}{1980}),
Groenen and Van de Velden (\citeproc{ref-groenen_vandevelden_16}{2016}))
to approximate loss functions such the one with logarithmic residuals in
Ramsay (\citeproc{ref-ramsay_77}{1977}).

But we have to keep in mind that downweighting large dissimilarities is
not the same thing as downweighting large residuals. The residuals
depend on \(X\), and it is perfectly possible that some small
dissimilarities have large residuals. On the other hand emphasizing
small dissimilarities in the loss function means that we want small
dissimilarities to be fitted relatively well, which means that on
average we want small dissimilarities to have small residuals. The
Shepard plot will tend to fan out at the high end.

Despite these reservations, it will be useful to study if and how fixed
weights can be used to improve robustness of smacof. If only because
fixed weights correspond with a simpler and presumably more efficient
algorithm.

\subsection{Residual Definition}\label{residual-definition}

In our examples and in our code we use the residuals \(\delta_k-d_k(X)\)
are arguments of our loss functions. From the statistical point of view
we have to remember, however, that most of these loss functions were
designed for the robust estimation of a location parameter or a linear
regression function. The error distributions were explicitly or
implicitly assumed to be symmetric around zero, and defined on the whole
real line, which was reflected in the fact that loss functions were even
and had infinite support. In MDS, however, distances and dissimilarities
are non-negative and reasonable error functions are not symmetric. One
could follow the example of Ramsay (\citeproc{ref-ramsay_77}{1977}) and
measure residuals as \(\log\delta_{ij}-\log d_{ij}(X)\). This does not
have any effect on the majorization of the loss functions, but it means
that in the smacof step to find \(X^{(\nu+1)}\) we have to minimize \[
\sigma(X)=\sum \omega_k(X^{(\nu)})(\log\delta_{ij}-\log d_{ij}(X))^2,
\] which is considerably more complicated (De Leeuw, Groenen, and Mair
(\citeproc{ref-deleeuw_groenen_mair_E_16a}{2016})).

\subsection{Robust Nonmetric MDS}\label{robust-nonmetric-mds}

Our discussion and our software is all about metric MDS. It seems easy
to extend the discussion to non-linear and non-metric MDS by adding an
alternating least squares step optimally scaling the dissimilarities.
This would take place between two majorizations of the robust loss
function, so one or more transformation and smacof steps can be taken
between updating the weights. But this paradigm does not work for robust
smacof.

Consider any hard redescender, such as Tukey or Hinich. At iteration
\(\nu\), for current weights, first first improve the configuration,
then compute the optimal transformation of the dissimilarities, and then
compute new weights. This is a recipe for disaster. At some point we
minimize \[
\sigma(\hat d)=\sum \omega_k(X^{(\nu)})(\hat d_k-d_k(X^{(\nu+1)}))^2
\]\{\#eq:zero\} over the disparities \(\hat d\), which must be monotone
with the dissimilarities. Because of the hard redescending some of the
weights, for current absolute residuals larger than \(c\), will be zero.
The monotone regression is done for the observations with non-zero
weights, and the disparities corresponding with zero weights are only
determined by the order they are required to have. Thus they can be
freely chosen in an interval between two disparities obtained from the
monotone regression. That interval can be large, in fact if one of the
zero weights corresponds with the largest dissimilarity it can be
infinite. What we choose in the interval will determine the new residual
and thus the next set of weights.

In the unfortunate situation that the current absolute residuals are all
larger than \(c\), even after choosing the optimal \(\hat d\), the next
weights will all be zero and the algorithm stops with zero stress.

\subsection{Practicalities}\label{practicalities}

Recommending one particular loss function from the many we have
discussed is not easy. In some cases, for example for Cauchy loss, one
can justify the choice of a loss function by assuming a particular error
distribution and using the maximum likelihood principle. But in general
perhaps the best way to proceed for a given MDS problem is to take what
we could call a \emph{trajectory approach}. Choose one particular
parametric family, for example the Huber one, and compute the robust
smacof solution \(X(c)\) for a number of increasing positive \(c\)
values. For small \(c\) we start with a close approximation of the LAV
solution, increasing \(c\) will eventually take us to the LS solution.
The starting point for computing each solution will be the solution for
the previous \(c\). We can plot the trajectory of the points in the
configurations \(X(c)\), and even make an animation. It seems that the
Huber family is a good candidate for such a study, with the generalized
Charbonnier a good second. If the main concern is to suppress the
influence of outliers then trying some of the hard redescenders, such as
the Tukey family, makes sense. Studying trajectories for some of robust
loss functions is clearly interesting, but it is not something we can or
will explore in this paper.

\sectionbreak

\section{Code}\label{code}

The function smacofRobust has a parameter ``engine'', which can be equal
to smacofCharbonnier, smacofGeneralizedCharbonnier, smacofBarron,
smacofHuber, smacofTukey, smacofHinnich, smacofCauchy, smacofFair,
smacofAndrews, smacofLogistic, smacofWelsch, or smacofGaussian. These
thirteen small modules compute the respective loss function values and
weights for the IRLS procedure. This makes it easy for interested
parties to add additional robust loss functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smacofRobust }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(delta,}
                         \AttributeTok{weights =} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{diag}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(delta)),}
                         \AttributeTok{ndim =} \DecValTok{2}\NormalTok{,}
                         \AttributeTok{xold =} \FunctionTok{smacofTorgerson}\NormalTok{(delta, ndim),}
                         \AttributeTok{engine =}\NormalTok{ smacofAV,}
                         \AttributeTok{cons =} \DecValTok{0}\NormalTok{,}
                         \AttributeTok{itmax =} \DecValTok{1000}\NormalTok{,}
                         \AttributeTok{eps =} \FloatTok{1e{-}15}\NormalTok{,}
                         \AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{) \{}
\NormalTok{  nobj }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(delta)}
\NormalTok{  wmax }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(weights)}
\NormalTok{  dold }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{dist}\NormalTok{(xold))}
\NormalTok{  h }\OtherTok{\textless{}{-}} \FunctionTok{engine}\NormalTok{(nobj, weights, delta, dold, cons)}
\NormalTok{  rold }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{resi}
\NormalTok{  wold }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{wght}
\NormalTok{  sold }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{strs}
\NormalTok{  itel }\OtherTok{\textless{}{-}} \DecValTok{1}
  \ControlFlowTok{repeat}\NormalTok{ \{}
\NormalTok{    vmat }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{wold}
    \FunctionTok{diag}\NormalTok{(vmat) }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{rowSums}\NormalTok{(vmat)}
\NormalTok{    vinv }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(vmat }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ nobj)) }\SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ nobj)}
\NormalTok{    bmat }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{wold }\SpecialCharTok{*}\NormalTok{ delta }\SpecialCharTok{/}\NormalTok{ (dold }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(nobj))}
    \FunctionTok{diag}\NormalTok{(bmat) }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{rowSums}\NormalTok{(bmat)}
\NormalTok{    xnew }\OtherTok{\textless{}{-}}\NormalTok{ vinv }\SpecialCharTok{\%*\%}\NormalTok{ (bmat }\SpecialCharTok{\%*\%}\NormalTok{ xold)}
\NormalTok{    dnew }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{dist}\NormalTok{(xnew))}
\NormalTok{    h }\OtherTok{\textless{}{-}} \FunctionTok{engine}\NormalTok{(nobj, weights, delta, dnew, cons)}
\NormalTok{    rnew }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{resi}
\NormalTok{    wnew }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{wght}
\NormalTok{    snew }\OtherTok{\textless{}{-}}\NormalTok{ h}\SpecialCharTok{$}\NormalTok{strs}
    \ControlFlowTok{if}\NormalTok{ (verbose) \{}
      \FunctionTok{cat}\NormalTok{(}
        \StringTok{"itel "}\NormalTok{,}
        \FunctionTok{formatC}\NormalTok{(itel, }\AttributeTok{width =} \DecValTok{4}\NormalTok{, }\AttributeTok{format =} \StringTok{"d"}\NormalTok{),}
        \StringTok{"sold "}\NormalTok{,}
        \FunctionTok{formatC}\NormalTok{(sold, }\AttributeTok{digits =} \DecValTok{10}\NormalTok{, }\AttributeTok{format =} \StringTok{"f"}\NormalTok{),}
        \StringTok{"snew "}\NormalTok{,}
        \FunctionTok{formatC}\NormalTok{(snew, }\AttributeTok{digits =} \DecValTok{10}\NormalTok{, }\AttributeTok{format =} \StringTok{"f"}\NormalTok{),}
        \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}
\NormalTok{      )}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{ ((itel }\SpecialCharTok{==}\NormalTok{ itmax) }\SpecialCharTok{||}\NormalTok{ ((sold }\SpecialCharTok{{-}}\NormalTok{ snew) }\SpecialCharTok{\textless{}}\NormalTok{ eps)) \{}
      \ControlFlowTok{break}
\NormalTok{    \}}
\NormalTok{    xold }\OtherTok{\textless{}{-}}\NormalTok{ xnew}
\NormalTok{    dold }\OtherTok{\textless{}{-}}\NormalTok{ dnew}
\NormalTok{    sold }\OtherTok{\textless{}{-}}\NormalTok{ snew}
\NormalTok{    wold }\OtherTok{\textless{}{-}}\NormalTok{ wnew}
\NormalTok{    rold }\OtherTok{\textless{}{-}}\NormalTok{ rnew}
\NormalTok{    itel }\OtherTok{\textless{}{-}}\NormalTok{ itel }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ xnew,}
    \AttributeTok{s =}\NormalTok{ snew,}
    \AttributeTok{d =}\NormalTok{ dnew,}
    \AttributeTok{r =}\NormalTok{ rnew,}
    \AttributeTok{itel =}\NormalTok{ itel}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofTorgerson }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(delta, ndim) \{}
\NormalTok{  dd }\OtherTok{\textless{}{-}}\NormalTok{ delta}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  rd }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(dd, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{  md }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(dd)}
\NormalTok{  sd }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5} \SpecialCharTok{*}\NormalTok{ (dd }\SpecialCharTok{{-}} \FunctionTok{outer}\NormalTok{(rd, rd, }\StringTok{"+"}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ md)}
\NormalTok{  ed }\OtherTok{\textless{}{-}} \FunctionTok{eigen}\NormalTok{(sd)}
  \FunctionTok{return}\NormalTok{(ed}\SpecialCharTok{$}\NormalTok{vectors[, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{ndim] }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(ed}\SpecialCharTok{$}\NormalTok{values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{ndim])))}
\NormalTok{\}}

\NormalTok{smacofCharbonnier }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{((delta }\SpecialCharTok{{-}}\NormalTok{ dmat)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ cons)}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(resi }\SpecialCharTok{\textless{}} \FloatTok{1e{-}10}\NormalTok{, }\DecValTok{2} \SpecialCharTok{*} \FunctionTok{max}\NormalTok{(wmat), resi)}
\NormalTok{  rmin }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(cons)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{/}\NormalTok{ (resi }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(nobj))}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi) }\SpecialCharTok{{-}}\NormalTok{ rmin }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(wmat)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofGeneralizedCharbonnier }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  resi }\OtherTok{\textless{}{-}}\NormalTok{ ((delta }\SpecialCharTok{{-}}\NormalTok{ dmat) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+}\NormalTok{ cons[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{\^{}}\NormalTok{ cons[}\DecValTok{2}\NormalTok{]}
\NormalTok{  rmin }\OtherTok{\textless{}{-}}\NormalTok{ cons[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\^{}}\NormalTok{ cons[}\DecValTok{2}\NormalTok{]}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*}\NormalTok{ ((delta }\SpecialCharTok{{-}}\NormalTok{ dmat) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+}\NormalTok{ cons[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(nobj)) }\SpecialCharTok{\^{}}\NormalTok{ (cons[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi) }\SpecialCharTok{{-}}\NormalTok{ rmin }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(wmat)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofBarron }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  f1 }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(cons[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ cons[}\DecValTok{2}\NormalTok{]}
\NormalTok{  f2 }\OtherTok{\textless{}{-}}\NormalTok{ ((((delta }\SpecialCharTok{{-}}\NormalTok{ dmat) }\SpecialCharTok{/}\NormalTok{ cons[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{abs}\NormalTok{(cons[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }
\NormalTok{  resi }\OtherTok{\textless{}{-}}\NormalTok{ f1 }\SpecialCharTok{*}\NormalTok{ (f2 }\SpecialCharTok{\^{}}\NormalTok{ (cons[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*}\NormalTok{ f2 }\SpecialCharTok{\^{}}\NormalTok{ (cons[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{/} \DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofGauss }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}}\NormalTok{ difi }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{pnorm}\NormalTok{(difi }\SpecialCharTok{/}\NormalTok{ cons) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ cons }\SpecialCharTok{*} \FunctionTok{dnorm}\NormalTok{(difi }\SpecialCharTok{/}\NormalTok{ cons)}
\NormalTok{  rmin }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ cons }\SpecialCharTok{*} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{pnorm}\NormalTok{(difi }\SpecialCharTok{/}\NormalTok{ cons) }\SpecialCharTok{{-}} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (difi }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(nobj))}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi) }\SpecialCharTok{{-}}\NormalTok{ rmin }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(wmat)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofHuber }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons, (difi }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, cons }\SpecialCharTok{*} \FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{{-}}\NormalTok{ ((cons }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{))}
\NormalTok{  wght }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons,}
\NormalTok{                 wmat,}
\NormalTok{                 wmat }\SpecialCharTok{*} \FunctionTok{sign}\NormalTok{(difi }\SpecialCharTok{{-}}\NormalTok{ cons) }\SpecialCharTok{*}\NormalTok{ cons }\SpecialCharTok{/}\NormalTok{ (difi }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(nobj)))}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofTukey }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  cans }\OtherTok{\textless{}{-}}\NormalTok{ (cons }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{6}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons, cans }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{), cans)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons, (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofCauchy }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{((difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ ((difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}\NormalTok{))}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofWelsch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofAndrews }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ cons, }
\NormalTok{                 (cons }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{cos}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ cons)), }
                 \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (cons}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ cons, }\FunctionTok{sin}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ cons) }\SpecialCharTok{/}\NormalTok{ (x }\SpecialCharTok{/}\NormalTok{ cons), }\DecValTok{0}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofHinich }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons, (difi}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, (cons}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(difi) }\SpecialCharTok{\textless{}}\NormalTok{ cons, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofLogistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}}\NormalTok{ (cons }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\FunctionTok{cosh}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ cons))}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*} \FunctionTok{tanh}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ cons) }\SpecialCharTok{/}\NormalTok{ (x }\SpecialCharTok{/}\NormalTok{ cons)}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}

\NormalTok{smacofFair }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(nobj, wmat, delta, dmat, cons) \{}
\NormalTok{  difi }\OtherTok{\textless{}{-}}\NormalTok{ delta }\SpecialCharTok{{-}}\NormalTok{ dmat}
\NormalTok{  resi }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{((difi }\SpecialCharTok{/}\NormalTok{ cons)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{  wght }\OtherTok{\textless{}{-}}\NormalTok{ wmat }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ ((difi }\SpecialCharTok{/}\NormalTok{ cons) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{1}\NormalTok{))}
\NormalTok{  strs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(wmat }\SpecialCharTok{*}\NormalTok{ resi)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
    \AttributeTok{resi =}\NormalTok{ resi,}
    \AttributeTok{wght =}\NormalTok{ wght,}
    \AttributeTok{strs =}\NormalTok{ strs}
\NormalTok{  ))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\sectionbreak

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-aftab_hartley_15}
Aftab, K., and R. Hartley. 2015. {``Convergence of Iteratively
Re-Weighted Least Squares to Robust m-Estimators.''} In \emph{2015 IEEE
Winter Conference on Applications of Computer Vision,} 480--87.
\url{https://doi.org/10.1109/WACV.2015.70}.

\bibitem[\citeproctext]{ref-aftab_hartley_trumpf_15}
Aftab, K., R. Hartley, and J. Trumpf. 2015. {``Generalized Weiszfeld
Algorithms for Lq Optimization.''} \emph{IEEE Transactions on Pattern
Analysis and Machine Intelligence} 37 (4): 728--44.
\url{https://doi.org/10.1109/TPAMI.2014.2353625}.

\bibitem[\citeproctext]{ref-andrews_bickel_hampel_huber_rogers_tukey_72}
Andrews, D. F., P. J. Bickel, F. R. Hampel, P. J. Huber, W. H. Rogers,
and J. W. Tukey. 1972. \emph{Robust Estimators of Location: Survey and
Advances}. Princeton University Press.

\bibitem[\citeproctext]{ref-barron_19}
Barron, J. T. 2019. {``A General and Adaptive Robust Loss Function.''}
In \emph{Proceedings 2019 IEEE/CVF Conferehce on Computer Vision and
Pattern Recognition}, 4331--39.
\url{https://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf}.

\bibitem[\citeproctext]{ref-beaton_tukey_74}
Beaton, A. E., and W. Tukey J. 1974. {``The Fitting of Power Series,
Meaning Polynomials, Illustrated on Band-Spectroscopic Data.''}
\emph{Technometrics} 16 (147--185).

\bibitem[\citeproctext]{ref-beck_sabach_15}
Beck, A., and S. Sabach. 2015. {``Weiszfeld's Method: Old and New
Results.''} \emph{Journal of Optimization Theory and Applications} 164:
1--40.

\bibitem[\citeproctext]{ref-black_anandan_96}
Black, M. J., and P. Anandan. 1996. {``The Robust Estimation of Multiple
Motions: Parametric and Piecewise-Smooth Flow Fields.''} \emph{Computer
Vision and Image Understanding} 63 (1): 75--104.

\bibitem[\citeproctext]{ref-boehning_lindsay_88}
BÃ¶hning, D., and B. G. Lindsay. 1988. {``{Monotonicity of
Quadratic-approximation Algorithms}.''} \emph{Annals of the Institute of
Statistical Mathematics} 40 (4): 641--63.

\bibitem[\citeproctext]{ref-border_18}
Border, K. C. 2018. {``Supergradients.''}
\url{https://healy.econ.ohio-state.edu/kcb/Notes/Supergrad.pdf}.

\bibitem[\citeproctext]{ref-candes_tao_05}
Candes, E. J., and T. Tao. 2005. {``Decoding by Linear Programming.''}
\emph{IEEE Transactions on Information Theory} 51 (12): 4203--15.

\bibitem[\citeproctext]{ref-candes_wakin_boyd_08}
Candes, E. J., M. B. Wakin, and S. P. Boyd. 2008. {``Enhancing Sparsity
by Reweighted l\_1 Minimization.''} \emph{Journal of Fourier Analysis
and Applications} 14: 877--905.
\url{https://doi.org/10.1007/s00041-008-9045-x}.

\bibitem[\citeproctext]{ref-charbonnier_blanc-feraud_aubert_barlaud_94}
Charbonnier, P., L. Blanc-Feraud, G. Aubert, and M. Barlaud. 1994.
{``{Two deterministic half-quadratic regularization algorithms for
computed imaging}.''} \emph{Proceedings of 1st International Conference
on Image Processing} 2: 168--72.
\url{https://doi.org/10.1109/icip.1994.413553}.

\bibitem[\citeproctext]{ref-coleman_holland_kaden_klema_peters_80}
Coleman, D., P. Holland, N. Kaden, V. Klema, and S. C. Peters. 1980.
{``A System of Subroutines for Iteratively Reweighted Least Squares
Computations.''} \emph{ACM Transactions on Mathematical Software} 6 (3):
327--36.

\bibitem[\citeproctext]{ref-degruijter_67}
De Gruijter, D. N. M. 1967. {``{The Cognitive Structure of Dutch
Political Parties in 1966}.''} Report E019-67. Psychological Institute,
University of Leiden.

\bibitem[\citeproctext]{ref-deleeuw_C_77}
De Leeuw, J. 1977. {``Applications of Convex Analysis to
Multidimensional Scaling.''} In \emph{Recent Developments in
Statistics}, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van
Cutsem, 133--45. Amsterdam, The Netherlands: North Holland Publishing
Company.

\bibitem[\citeproctext]{ref-deleeuw_A_84f}
---------. 1984. {``{Differentiability of Kruskal's Stress at a Local
Minimum}.''} \emph{Psychometrika} 49: 111--13.

\bibitem[\citeproctext]{ref-deleeuw_A_88b}
---------. 1988a. {``Convergence of the Majorization Method for
Multidimensional Scaling.''} \emph{Journal of Classification} 5:
163--80.

\bibitem[\citeproctext]{ref-deleeuw_C_88b}
---------. 1988b. {``{Multivariate Analysis with Optimal Scaling}.''} In
\emph{Proceedings of the International Conference on Advances in
Multivariate Statistical Analysis}, edited by S. Das Gupta and J. K.
Ghosh, 127--60. Calcutta, India: Indian Statistical Institute.

\bibitem[\citeproctext]{ref-deleeuw_C_94c}
---------. 1994. {``{Block Relaxation Algorithms in Statistics}.''} In
\emph{Information Systems and Data Analysis}, edited by H. H. Bock, W.
Lenski, and M. M. Richter, 308--24. Berlin: Springer Verlag.
\url{https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf}.

\bibitem[\citeproctext]{ref-deleeuw_E_18f}
---------. 2018. {``{MM Algorithms for Smoothed Absolute Values}.''}
2018.
\url{https://jansweb.netlify.app/publication/deleeuw-e-18-f/deleeuw-e-18-f.pdf}.

\bibitem[\citeproctext]{ref-deleeuw_groenen_mair_E_16a}
De Leeuw, J., P. Groenen, and P. Mair. 2016. {``{Minimizing rStress
Using Majorization}.''} 2016.
\url{https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-a/deleeuw-groenen-mair-e-16-a.pdf}.

\bibitem[\citeproctext]{ref-deleeuw_heiser_C_77}
De Leeuw, J., and W. J. Heiser. 1977. {``Convergence of Correction
Matrix Algorithms for Multidimensional Scaling.''} In \emph{Geometric
Representations of Relational Data}, edited by J. C. Lingoes, 735--53.
Ann Arbor, Michigan: Mathesis Press.

\bibitem[\citeproctext]{ref-deleeuw_heiser_C_80}
---------. 1980. {``Multidimensional Scaling with Restrictions on the
Configuration.''} In \emph{Multivariate Analysis, Volume {V}}, edited by
P. R. Krishnaiah, 501--22. Amsterdam, The Netherlands: North Holland
Publishing Company.

\bibitem[\citeproctext]{ref-deleeuw_lange_A_09}
De Leeuw, J., and K. Lange. 2009. {``Sharp Quadratic Majorization in One
Dimension.''} \emph{Computational Statistics and Data Analysis} 53:
2471--84.

\bibitem[\citeproctext]{ref-deleeuw_mair_A_09c}
De Leeuw, J., and P. Mair. 2009. {``{Multidimensional Scaling Using
Majorization: SMACOF in R}.''} \emph{Journal of Statistical Software} 31
(3): 1--30. \url{https://www.jstatsoft.org/article/view/v031i03}.

\bibitem[\citeproctext]{ref-dempster_laird_rubin_77}
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. {``{Maximum
Likelihood for Incomplete Data via the EM Algorithm}.''} \emph{Journal
of the Royal Statistical Society} B39: 1--38.

\bibitem[\citeproctext]{ref-dennis_welsch_78}
Dennis Jr, J. E., and R. E. Welsch. 1978. {``Techniques for Nonlinear
Least Squares and Robust Regression.''} \emph{Communications in
Statistics - Simulation and Computation} 7 (4): 345--59.

\bibitem[\citeproctext]{ref-donoho_elad_03}
Donoho, D. L., and M. Elad. 2003. {``Optimally Sparse Representation in
General (Nonorthogonal) Dictionaries via â\_1 Minimization.''}
\emph{Proceedings of the National Academy of Sciences} 100 (5):
2197--2202.

\bibitem[\citeproctext]{ref-franksen_grattan-guinness_89}
Franksen, O. I., and I. Grattan-Guinness. 1989. {``The Eraliest
Contribution to Location Theory ? Spatio-Economic Equilibrium with
Lam{Ã©} and Clapeyron.''} \emph{Mathematics and Computers in Simulation}
3w1: 195--220.

\bibitem[\citeproctext]{ref-gabriel_odoroff_84}
Gabriel, K. R., and Ch. L. Odoroff. 1984. {``Resistant Lower Rank
Approximation of Matrices.''} In \emph{Data Analysis and Informatics},
edited by E. Diday, M. Jambu, L. Lebart, J. Pages, and R. Tomassone,
3:23--30. North Holland Publishing Company.

\bibitem[\citeproctext]{ref-gabriel_zamir_79}
Gabriel, K. R., and S. Zamir. 1979. {``{Lower Rank Approximation of
Matrices by Least Squares with Any Choize of Weights}.''}
\emph{Technometrics} 21 (4): 489--98.

\bibitem[\citeproctext]{ref-gifi_B_90}
Gifi, A. 1990. \emph{Nonlinear Multivariate Analysis}. New York, N.Y.:
Wiley.

\bibitem[\citeproctext]{ref-groenen_giaquinto_kiers_03}
Groenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. {``{Weighted
Majorization Algorithms for Weighted Least Squares Decomposition
Models}.''} Econometric Institute Report EI 2003-09. Econometric
Institute, Erasmus University Rotterdam.
\url{https://repub.eur.nl/pub/1700}.

\bibitem[\citeproctext]{ref-groenen_heiser_meulman_99}
Groenen, P. J. F., W. J. Heiser, and J. J. Meulman. 1999. {``{Global
Optimization in Least-Squares Multidimensional Scaling by Distance
Smoothing}.''} \emph{Journal of Classification} 16: 225--54.

\bibitem[\citeproctext]{ref-groenen_vandevelden_16}
Groenen, P. J. F., and M. Van de Velden. 2016. {``{Multidimensional
Scaling by Majorization: A Review}.''} \emph{Journal of Statistical
Software} 73 (8): 1--26.
\url{https://www.jstatsoft.org/index.php/jss/article/view/v073i08}.

\bibitem[\citeproctext]{ref-he_li_liu_zhou_23}
He, Y., L. Li, D. Liu, and W.-Z. Zhou. 2023. {``Huber Principal
Component Analysis for Large-Dimensional Factor Models.''}
\url{https://arxiv.org/abs/2303.02817}.

\bibitem[\citeproctext]{ref-heiser_86}
Heiser, W. J. 1986. {``{A Majorization Algorithm for the Reciprocal
Location Problem}.''} RR-86-12. Department of Data Theory, University of
Leiden.

\bibitem[\citeproctext]{ref-heiser_87}
---------. 1987. {``{Correspondence Analysis with Least Absolute
Residuals}.''} \emph{Computational Statistics and Data Analysis} 5:
337--56.

\bibitem[\citeproctext]{ref-heiser_88}
---------. 1988. {``{Multidimensional Scaling with Least Absolute
Residuals}.''} In \emph{Classification and Related Methods of Data
Analysis}, edited by H. H. Bock, 455--62. North-Holland Publishing Co.

\bibitem[\citeproctext]{ref-heiser_95}
---------. 1995. {``{Convergent Computing by Iterative Majorization:
Theory and Applications in Multidimensional Data Analysis}.''} In
\emph{Recent Advantages in Descriptive Multivariate Analysis}, edited by
W. J. Krzanowski, 157--89. Oxford: Clarendon Press.

\bibitem[\citeproctext]{ref-hinich_talwar_75}
Hinich, M. J., and P. P. Talwar. 1975. {``A Simple Method for Robust
Regression.''} \emph{Journal of the American Statistical Association}
70: 113--19.

\bibitem[\citeproctext]{ref-holland_welsch_77}
Holland, P. W., and R. E. Welsch. 1977. {``Robust Regression Using
Iteratively Reweighted Least-Squares.''} \emph{Communications in
Statistics - Theory and Methods} 6 (9): 813--27.
\url{https://doi.org/10.1080/03610927708827533}.

\bibitem[\citeproctext]{ref-huber_64}
Huber, P. J. 1964. {``Robust Estimation of a Location Parameter.''}
\emph{Annals of Mathematical Statistics} 35 (1): 73--101.

\bibitem[\citeproctext]{ref-hunter_li_05}
Hunter, D. R., and R. Li. 2005. {``{Variable Selection Using MM
Algorithms}.''} \emph{The Annals of Statistics} 33: 1617--42.

\bibitem[\citeproctext]{ref-jaakkola_jordan_00}
Jaakkola, T. S., and M. I. Jordan. 2000. {``{Bayesian Parameter
Estimation via Variational Methods}.''} \emph{Statistics and Computing}
10: 25--37.

\bibitem[\citeproctext]{ref-katz_69}
Katz, I. N. 1969. {``On the Convergence of a Numerical Scheme for
Solving Some Locational Equilibrium Problems.''} \emph{{SIAM} Journal on
Applied Mathematics} 17 (6): 1224--31.

\bibitem[\citeproctext]{ref-lange_16}
Lange, K. 2016. \emph{MM Optimization Algorithms}. SIAM.

\bibitem[\citeproctext]{ref-li_24}
Li, J. 2024. {``Robust Matrix Factor Analysis Method with Adaptive
Parameter Adjustment Using Cauchy Weighting.''} \emph{Computational
Statistics}. \url{https://doi.org/10.1007/s00180-024-01548-4}.

\bibitem[\citeproctext]{ref-mlotshwa_vandeventer_bosman_23}
Mlotshwa, T., H. Van Deventer, and Sergeevna Bosman A. 2023. {``Cauchy
Loss Function: Robustness Under Gaussian and Cauchy Noise.''}
\url{https://arxiv.org/abs/2302.07238}.

\bibitem[\citeproctext]{ref-mordukhovich_nam_19}
Mordukhovich, B. S., and N. M. Nam. 2019. {``The Fermat-Torricelli
Problem and Weiszfeld's Algorithm in the Light of Convex Analysis.''}
\emph{Journal of Applied Numerical Optimization} 1 (3): 205--19.
https://doi.org/\url{https://doi.org/10.23952/jano.1.2019.3.02}.

\bibitem[\citeproctext]{ref-phillips_02}
Phillips, R. F. 2002. {``Least Absolute Deviations Estimation via the EM
Algorithm.''} \emph{Statistics and Computing} 12: 281--85.

\bibitem[\citeproctext]{ref-plastria_11}
Plastria, F. 2011. {``The Weiszfeld Algorithm: Proof, Amendments and
Extensions.''} In \emph{Foundations of Location Analysis}, edited by H.
A. Eiselt and V. Marianov, 155:357--89. International Series
inOperations Research and Management Science. Springer.

\bibitem[\citeproctext]{ref-pliner_96}
Pliner, V. 1996. {``{Metric Unidimensional Scaling and Global
Optimization}.''} \emph{Journal of Classification} 13: 3--18.

\bibitem[\citeproctext]{ref-r_core_team_24}
R Core Team. 2024. \emph{R: A Language and Environment for Statistical
Computing}. {Vienna, Austria}: R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\bibitem[\citeproctext]{ref-ramirez_sanchez_kreinovich_argaez_14}
Ramirez, C., R. Sanchez, V. Kreinovich, and M. Argaez. 2014.
{``{\(\sqrt{x^2+\mu}\) is the Most Computationally Efficient Smooth
Approximation to \vert{}x\vert{}}.''} \emph{Journal of Uncertain
Systems} 8: 205--10.

\bibitem[\citeproctext]{ref-ramsay_77}
Ramsay, J. O. 1977. {``{Maximum Likelihood Estimation in
Multidimensional Scaling}.''} \emph{Psychometrika} 42: 241--66.

\bibitem[\citeproctext]{ref-rothkopf_57}
Rothkopf, E. Z. 1957. {``{A Measure of Stimulus Similarity and Errors in
some Paired-associate Learning}.''} \emph{Journal of Experimental
Psychology} 53: 94--101.

\bibitem[\citeproctext]{ref-schlossmacher_73}
Schlossmacher, E. J. 1973. {``An Iterative Technique for Absolute
Deviations Curve Ftting.''} \emph{Journal of the American Statistical
Association} 68: 857--59.

\bibitem[\citeproctext]{ref-sturm_84}
Sturm, R. 1884. {``Ueber Den Punkt Kleinster Entferningssumme von
Gegebenen Punkten.''} \emph{Journal f{Ã¼}r Die Reine Und Angewandte
Mathematik} 97: 49--61.

\bibitem[\citeproctext]{ref-vanruitenburg_05}
Van Ruitenburg, J. 2005. {``{Algorithms for Parameter Estimation in the
Rasch Model}.''} Measurement and Research Department Reports 2005-04.
Arnhem, Netherlands: CITO.
\url{https://www.researchgate.net/publication/355568984_Algorithms_for_parameter_estimation_in_the_Rasch_model_Measurement_and_Research_Report_05-04\#fullTextFileContent}.

\bibitem[\citeproctext]{ref-verboon_heiser_94}
Verboon, P., and W. J. Heiser. 1994. {``{Resistant Lower Rank
Approximation of Matrices by Iterative Majorization}.''}
\emph{Computational Statistics and Data Analysis} 18: 457--67.

\bibitem[\citeproctext]{ref-voronin_ozkaya_yoshida_15}
Voronin, S., G. Ozkaya, and Y. Yoshida. 2014. {``{Convolution Based
Smooth Approximations to the Absolute Value Function with Application to
Non-smooth Regularization}.''} 2014.
\url{https://arxiv.org/abs/1408.6795}.

\bibitem[\citeproctext]{ref-vosz_eckhardt_80}
Vosz, H., and U. Eckhardt. 1980. {``{Linear Convergence of Generalized
{W}eiszfeld's Method}.''} \emph{Computing} 25: 243--51.

\bibitem[\citeproctext]{ref-weiszfeld_37}
Weiszfeld, E. 1937. {``{Sur le Point par lequel la Somme des Distances
de n Points DonnÃ©s est Minimum}.''} \emph{Tohoku Mathematics Journal}
43: 355--86.

\bibitem[\citeproctext]{ref-weiszfeld_plastria_09}
Weiszfeld, E., and F. Plastria. 2009. {``On the Point for Which the Sum
of the Distances to n Given Points Is Minimum.''} \emph{Annals of
Operations Research} 167: 7--41.

\end{CSLReferences}




\end{document}
