<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jan de Leeuw">
<meta name="dcterms.date" content="2024-10-29">

<title>Robust Least Squares Multidimensional Scaling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="av_files/libs/clipboard/clipboard.min.js"></script>
<script src="av_files/libs/quarto-html/quarto.js"></script>
<script src="av_files/libs/quarto-html/popper.min.js"></script>
<script src="av_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="av_files/libs/quarto-html/anchor.min.js"></script>
<link href="av_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="av_files/libs/quarto-html/quarto-syntax-highlighting-0c8af03b2fbd4cf44d6703e337af0e8d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="av_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="av_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="av_files/libs/bootstrap/bootstrap-b0df8e4da9e133b4f2ae0d0250a87660.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#sec-major" id="toc-sec-major" class="nav-link" data-scroll-target="#sec-major"><span class="header-section-number">2</span> Majorization</a></li>
  <li><a href="#sec-majorization" id="toc-sec-majorization" class="nav-link" data-scroll-target="#sec-majorization"><span class="header-section-number">3</span> Majorizing Strife</a>
  <ul class="collapse">
  <li><a href="#sec-amgm" id="toc-sec-amgm" class="nav-link" data-scroll-target="#sec-amgm"><span class="header-section-number">3.1</span> Algorithm</a></li>
  <li><a href="#sec-zero" id="toc-sec-zero" class="nav-link" data-scroll-target="#sec-zero"><span class="header-section-number">3.2</span> Zero Residuals</a></li>
  <li><a href="#ell_0-loss" id="toc-ell_0-loss" class="nav-link" data-scroll-target="#ell_0-loss"><span class="header-section-number">3.3</span> <span class="math inline">\(\ell_0\)</span> loss</a></li>
  </ul></li>
  <li><a href="#generalizing-strife" id="toc-generalizing-strife" class="nav-link" data-scroll-target="#generalizing-strife"><span class="header-section-number">4</span> Generalizing Strife</a>
  <ul class="collapse">
  <li><a href="#sharp-quadratic-majorization" id="toc-sharp-quadratic-majorization" class="nav-link" data-scroll-target="#sharp-quadratic-majorization"><span class="header-section-number">4.1</span> Sharp Quadratic Majorization</a></li>
  <li><a href="#sec-twosupport" id="toc-sec-twosupport" class="nav-link" data-scroll-target="#sec-twosupport"><span class="header-section-number">4.2</span> Two Support Points</a></li>
  <li><a href="#sufficient-conditions" id="toc-sufficient-conditions" class="nav-link" data-scroll-target="#sufficient-conditions"><span class="header-section-number">4.3</span> Sufficient Conditions</a></li>
  </ul></li>
  <li><a href="#power-smoothers" id="toc-power-smoothers" class="nav-link" data-scroll-target="#power-smoothers"><span class="header-section-number">5</span> Power Smoothers</a>
  <ul class="collapse">
  <li><a href="#sec-charb" id="toc-sec-charb" class="nav-link" data-scroll-target="#sec-charb"><span class="header-section-number">5.1</span> Charbonnier</a></li>
  <li><a href="#generalized-charbonnier" id="toc-generalized-charbonnier" class="nav-link" data-scroll-target="#generalized-charbonnier"><span class="header-section-number">5.2</span> Generalized Charbonnier</a></li>
  <li><a href="#barron" id="toc-barron" class="nav-link" data-scroll-target="#barron"><span class="header-section-number">5.3</span> Barron</a></li>
  </ul></li>
  <li><a href="#convolution-smoothers" id="toc-convolution-smoothers" class="nav-link" data-scroll-target="#convolution-smoothers"><span class="header-section-number">6</span> Convolution Smoothers</a>
  <ul class="collapse">
  <li><a href="#huber" id="toc-huber" class="nav-link" data-scroll-target="#huber"><span class="header-section-number">6.1</span> Huber</a></li>
  <li><a href="#gaussian" id="toc-gaussian" class="nav-link" data-scroll-target="#gaussian"><span class="header-section-number">6.2</span> Gaussian</a></li>
  </ul></li>
  <li><a href="#a-bouquet-of-loss-functions" id="toc-a-bouquet-of-loss-functions" class="nav-link" data-scroll-target="#a-bouquet-of-loss-functions"><span class="header-section-number">7</span> A Bouquet of Loss Functions</a>
  <ul class="collapse">
  <li><a href="#andrews" id="toc-andrews" class="nav-link" data-scroll-target="#andrews"><span class="header-section-number">7.1</span> Andrews</a></li>
  <li><a href="#tukey" id="toc-tukey" class="nav-link" data-scroll-target="#tukey"><span class="header-section-number">7.2</span> Tukey</a></li>
  <li><a href="#hinich" id="toc-hinich" class="nav-link" data-scroll-target="#hinich"><span class="header-section-number">7.3</span> Hinich</a></li>
  <li><a href="#cauchy" id="toc-cauchy" class="nav-link" data-scroll-target="#cauchy"><span class="header-section-number">7.4</span> Cauchy</a></li>
  <li><a href="#welsch" id="toc-welsch" class="nav-link" data-scroll-target="#welsch"><span class="header-section-number">7.5</span> Welsch</a></li>
  <li><a href="#logistic" id="toc-logistic" class="nav-link" data-scroll-target="#logistic"><span class="header-section-number">7.6</span> Logistic</a></li>
  <li><a href="#fair" id="toc-fair" class="nav-link" data-scroll-target="#fair"><span class="header-section-number">7.7</span> Fair</a></li>
  </ul></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">8</span> Examples</a>
  <ul class="collapse">
  <li><a href="#gruijter" id="toc-gruijter" class="nav-link" data-scroll-target="#gruijter"><span class="header-section-number">8.1</span> Gruijter</a>
  <ul class="collapse">
  <li><a href="#least-squares" id="toc-least-squares" class="nav-link" data-scroll-target="#least-squares"><span class="header-section-number">8.1.1</span> Least Squares</a></li>
  <li><a href="#least-absolute-value" id="toc-least-absolute-value" class="nav-link" data-scroll-target="#least-absolute-value"><span class="header-section-number">8.1.2</span> Least Absolute Value</a></li>
  <li><a href="#huber-1" id="toc-huber-1" class="nav-link" data-scroll-target="#huber-1"><span class="header-section-number">8.1.3</span> Huber</a></li>
  <li><a href="#tukey-1" id="toc-tukey-1" class="nav-link" data-scroll-target="#tukey-1"><span class="header-section-number">8.1.4</span> Tukey</a></li>
  </ul></li>
  <li><a href="#rothkopf" id="toc-rothkopf" class="nav-link" data-scroll-target="#rothkopf"><span class="header-section-number">8.2</span> Rothkopf</a>
  <ul class="collapse">
  <li><a href="#least-squares-1" id="toc-least-squares-1" class="nav-link" data-scroll-target="#least-squares-1"><span class="header-section-number">8.2.1</span> Least Squares</a></li>
  <li><a href="#least-absolute-value-1" id="toc-least-absolute-value-1" class="nav-link" data-scroll-target="#least-absolute-value-1"><span class="header-section-number">8.2.2</span> Least Absolute Value</a></li>
  <li><a href="#huber-2" id="toc-huber-2" class="nav-link" data-scroll-target="#huber-2"><span class="header-section-number">8.2.3</span> Huber</a></li>
  <li><a href="#tukey-2" id="toc-tukey-2" class="nav-link" data-scroll-target="#tukey-2"><span class="header-section-number">8.2.4</span> Tukey</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-literature" id="toc-sec-literature" class="nav-link" data-scroll-target="#sec-literature"><span class="header-section-number">9</span> Literature</a>
  <ul class="collapse">
  <li><a href="#robust-statistics" id="toc-robust-statistics" class="nav-link" data-scroll-target="#robust-statistics"><span class="header-section-number">9.1</span> Robust Statistics</a></li>
  <li><a href="#location-analysis" id="toc-location-analysis" class="nav-link" data-scroll-target="#location-analysis"><span class="header-section-number">9.2</span> Location Analysis</a></li>
  <li><a href="#sparse-recovery" id="toc-sparse-recovery" class="nav-link" data-scroll-target="#sparse-recovery"><span class="header-section-number">9.3</span> Sparse Recovery</a></li>
  <li><a href="#multivariate-analysis" id="toc-multivariate-analysis" class="nav-link" data-scroll-target="#multivariate-analysis"><span class="header-section-number">9.4</span> Multivariate Analysis</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">10</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#bounding-the-second-derivative" id="toc-bounding-the-second-derivative" class="nav-link" data-scroll-target="#bounding-the-second-derivative"><span class="header-section-number">10.1</span> Bounding the Second Derivative</a></li>
  <li><a href="#fixed-weights" id="toc-fixed-weights" class="nav-link" data-scroll-target="#fixed-weights"><span class="header-section-number">10.2</span> Fixed Weights</a></li>
  <li><a href="#residual-definition" id="toc-residual-definition" class="nav-link" data-scroll-target="#residual-definition"><span class="header-section-number">10.3</span> Residual Definition</a></li>
  <li><a href="#robust-nonmetric-mds" id="toc-robust-nonmetric-mds" class="nav-link" data-scroll-target="#robust-nonmetric-mds"><span class="header-section-number">10.4</span> Robust Nonmetric MDS</a></li>
  <li><a href="#practicalities" id="toc-practicalities" class="nav-link" data-scroll-target="#practicalities"><span class="header-section-number">10.5</span> Practicalities</a></li>
  </ul></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code"><span class="header-section-number">11</span> Code</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">12</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="av.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Robust Least Squares Multidimensional Scaling</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jan de Leeuw <a href="mailto:jan@deleeuwpdx.net" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-1420-1797" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="www.ucla.edu">
            University of California Los Angeles
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 29, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    Combining different loss functions with linear models and minimizing loss with iteratively reweighted least squares (IRLS) has a long history in robust statistics. In this paper we use an IRLS version of the smacof algorithm to minimize various robust multidimensional scaling loss functions. Our results use a general theorem on sharp quadratic majorization of <span class="citation" data-cites="deleeuw_lange_A_09">De Leeuw and Lange (<a href="#ref-deleeuw_lange_A_09" role="doc-biblioref">2009</a>)</span>. We relate this theorem to earlier results in robust statistics, location theory, and sparse recovery. Code in R is included.
  </div>
</div>


</header>


<section id="sec-intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The title of this paper is somewhat surprising. Least squares estimation is typically not robust, it is sensitive to outliers and pays too much attention to minimizing the largest residuals. What we mean by robust least squares multidimensional scaling (MDS), however, is to use the smacof machinery designed to minimize least squares loss functions of the form <span id="eq-stressdef"><span class="math display">\[
\sigma(X):=\sum\omega_k(\delta_k-d_k(X))^2,
\tag{1}\]</span></span> to minimize robust loss functions. In <a href="#eq-stressdef" class="quarto-xref">Equation&nbsp;1</a> the <span class="math inline">\(\omega_k\)</span> are positive <em>weights</em>, the <span class="math inline">\(\delta_k\)</span> are positive <em>dissimilarities</em>, and the <span class="math inline">\(d_k(X)\)</span> are Euclidean <em>distances</em> between two of the rows of the <span class="math inline">\(n\times p\)</span> <em>configuration</em> matrix <span class="math inline">\(X\)</span>.</p>
<p>Some terminological conventions we use throughout the paper. By <em>positive</em> we mean smaller than or equal to zero. Smaller than zero is <em>strictly positive</em>. Some for <em>negative</em> and <em>strictly negative</em>, for <em>increasing</em> and <em>strictly increasing</em>, and for <em>decreasing</em> and <em>strictly decreasing</em>.</p>
<p>The prototypical robust loss function is least absolute value loss <span id="eq-stradddef"><span class="math display">\[
\sigma(X):=\sum \omega_k|\delta_k-d_k(X)|,
\tag{2}\]</span></span> which we will call <em>strife</em>, since the names <em>stress</em>, <em>sstress</em>, and <em>strain</em> are already taken. Note we are overloading the symbol <span class="math inline">\(\sigma\)</span>, because we will use it for all of the loss functions in this paper.</p>
<p>Strife is not differentiable at configurations <span class="math inline">\(X\)</span> for which there is at least one <span class="math inline">\(k\)</span> for which either <span class="math inline">\(d_k(X)=\delta_k\)</span> or <span class="math inline">\(d_k(X)=0\)</span> (or both). This lack of differentiability complicates the minimization problem. Moreover experience with one-dimensional and city block MDS suggests that having areas where the loss function is not differentiable leads to (many) additional local minima.</p>
<p>In this paper we will discuss (and implement) various variations of strife from <a href="#eq-stradddef" class="quarto-xref">Equation&nbsp;2</a>. They can be interpreted in two different ways. On the one hand they are smoothers of the absolute value function, and consequently of strife. We want to eliminate the problems with differentiability, at least the ones caused by <span class="math inline">\(\delta_k=d_k(X)\)</span>. If this is our main goal, then we want to choose the smoother in such a way that it is close to the absolute value function. This is similar to the distance smoothing used by <span class="citation" data-cites="pliner_96">Pliner (<a href="#ref-pliner_96" role="doc-biblioref">1996</a>)</span> and <span class="citation" data-cites="groenen_heiser_meulman_99">Groenen, Heiser, and Meulman (<a href="#ref-groenen_heiser_meulman_99" role="doc-biblioref">1999</a>)</span> in the global minimization of stress from <a href="#eq-stressdef" class="quarto-xref">Equation&nbsp;1</a>, except that we do not smooth the distance function but the strife residual.</p>
<p>On the other hand our modified loss functions can be interpreted as more robust versions of the least squares loss function, and consequently of stress. Our goal then is to combine the robustness of the absolute value function with the efficiency and computational ease of least squares. If robustness is our main goal then there is no reason to stay close to the absolute value function.</p>
<p>Our robust or smooth loss functions are all of the form <span id="eq-strifedef"><span class="math display">\[
\sigma(X):=\sum\omega_k\ f(r_k(X)),
\tag{3}\]</span></span> where <span class="math inline">\(r_k(X)\)</span> is the <em>residual</em>, i.e. <span id="eq-resdef"><span class="math display">\[
r_k(X):=\delta_k-d_k(X).
\tag{4}\]</span></span> The function <span class="math inline">\(f\)</span> is assumed to be even (i.e.&nbsp;symmetric around zero) and attains its minimum, which is equal to zero, at zero. For now, note that loss in <a href="#eq-stressdef" class="quarto-xref">Equation&nbsp;1</a> is the special case with <span class="math inline">\(f(x)=x^2\)</span> and loss in <a href="#eq-stradddef" class="quarto-xref">Equation&nbsp;2</a> is the special case with <span class="math inline">\(f(x)=|x|\)</span>.</p>
</section>
<section id="sec-major" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Majorization</h1>
<p>Our loss functions will be minimized by using majorization algorithms (these days more commonly known as MM algorithms). This paper discusses a general way to construct majorization algorithms for robust loss functions.</p>
<p>For completeness we give a short introduction to majorization, without aiming for maximum generality. For the general theory of majorization algorithms we refer to their introduction in <span class="citation" data-cites="deleeuw_C_94c">De Leeuw (<a href="#ref-deleeuw_C_94c" role="doc-biblioref">1994</a>)</span> and to the excellent and comprehensive book by <span class="citation" data-cites="lange_16">Lange (<a href="#ref-lange_16" role="doc-biblioref">2016</a>)</span>. As mentioned in <a href="#sec-intro" class="quarto-xref">Section&nbsp;1</a> the robust loss functions in <a href="#eq-strifedef" class="quarto-xref">Equation&nbsp;3</a> are real-valued functions of a single real variable, defined on the whole real line, they are even, and they attain a minimum equal to zero at zero. Thus they are all symmetric bowls anchored at zero.</p>
<div id="def-majorize" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1</strong></span> A function <span class="math inline">\(g\)</span> <em>majorizes</em> a function <span class="math inline">\(f\)</span> at a point <span class="math inline">\(y\)</span> if <span class="math inline">\(g(x)\geq f(x)\)</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(g(y)=f(y)\)</span>. The point <span class="math inline">\(y\)</span> is a <em>support point</em> of the majorization. Majorization of <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> is <em>strict</em> if <span class="math inline">\(g(x)&gt;f(x)\)</span> for all <span class="math inline">\(x\not= y\)</span>.</p>
</div>
<p>Majorizers have at least one support point, but they can have many. The function <span class="math inline">\(g(x)=x^2+\sin^2(x)\)</span> majorizes <span class="math inline">\(f(x)=x^2\)</span> at every integer multiple of <span class="math inline">\(\pi\)</span>, and strictly majorizes <span class="math inline">\(f\)</span> at none of its support points. Also any function <span class="math inline">\(f\)</span> majorizes itself at all points of the real line.</p>
<div id="def-sharp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2</strong></span> If <span class="math inline">\(\mathfrak{H}\)</span> is a family of functions that all majorize <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> then <span class="math inline">\(h\in\mathfrak{H}\)</span> is a <em>sharp majorization</em> in <span class="math inline">\(\mathfrak{H}\)</span> if <span class="math inline">\(h(x)\leq g(x)\)</span> for all <span class="math inline">\(g\in\mathfrak{H}\)</span>. The sharp majorization, if it exists, is by definition unique.</p>
</div>
<div id="thm-diff" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> Suppose <span class="math inline">\(g\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span>.</p>
<ul>
<li>If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are differentiable at <span class="math inline">\(y\)</span> then <span class="math inline">\(f'(y)=g'(y)\)</span>.</li>
<li>If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are twice-differentiable at <span class="math inline">\(y\)</span> then <span class="math inline">\(f''(y)\leq g''(y)\)</span>.</li>
<li>If <span class="math inline">\(f''(y)&lt;g''(y)\)</span> then <span class="math inline">\(g\)</span> strictly majorizes <span class="math inline">\(f\)</span> in a neighborhood of <span class="math inline">\(y\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(h=g-f\)</span> is negative and has a minimum equal to zero at <span class="math inline">\(y\)</span>. Thus the derivative of <span class="math inline">\(h\)</span> vanishes at <span class="math inline">\(y\)</span> and the second derivative is negative at <span class="math inline">\(y\)</span>. If the second derivative is strictly negative then we use the sufficient condition for a local minimum.</p>
</div>
<p>A <em>majorization algorithm</em> to minimize <span class="math inline">\(f\)</span> is iterative. We update <span class="math inline">\(x^{(\nu)}\)</span>, the approximation of the minimizer in iteration <span class="math inline">\(\nu\)</span>, by <span id="eq-majmaj"><span class="math display">\[
x^{(\nu+1)}\in\mathop{\text{argmin}}_x\ g_\nu(x),
\tag{5}\]</span></span> where <span class="math inline">\(g_\nu\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(x^{(\nu)}\)</span>. If <span class="math inline">\(x^{(\nu)}\in\mathop{\text{argmin}}_x\ g_\nu(x)\)</span> we stop. Otherwise we find a new majorizer <span class="math inline">\(g_{\nu+1}\)</span>, which majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(x^{(\nu+1)}\)</span>, and start a new iteration.</p>
<p>Convergence of majorization algorithms follows from the <em>sandwich inequality</em> <span id="eq-sandwich"><span class="math display">\[
f(x^{(\nu+1)})\leq g_\nu(x^{(\nu+1)})\leq g_\nu(x^{(\nu)})=f(x^{(\nu)}).
\tag{6}\]</span></span> This we the algorithm produces a decreasing sequence of loss function values, which converges if loss is bounded below. In <a href="#eq-sandwich" class="quarto-xref">Equation&nbsp;6</a> the first inquality (from the left) follows from majorization. If the majorization is strict, then so is the inequality. The second inequality follows from minimization of <span class="math inline">\(g\)</span>. It is strict if <span class="math inline">\(g\)</span> has a unique minimizer, for example if <span class="math inline">\(g\)</span> is strictly convex. The final equality in <a href="#eq-sandwich" class="quarto-xref">Equation&nbsp;6</a> comes from majorization at <span class="math inline">\(x^{(\nu)}\)</span>.</p>
<p>If <span class="math inline">\(x^{(\nu)}\)</span> minimizes <span class="math inline">\(g_\nu\)</span> we stop and we have finite convergence. If the algorithm generates an infinite sequence, which is the usual case, the second inequality in <a href="#eq-sandwich" class="quarto-xref">Equation&nbsp;6</a> is always strict, and the algorithm generates a strictly decreasing sequence of loss function values. Note that for the validity of the sandwich inequality it suffices to decrease <span class="math inline">\(g_\nu\)</span> in every iteration, and not necessarily to minimize it. Different ways to decrease <span class="math inline">\(g_\nu\)</span> correspond with different step-size procedures in gradient methods.</p>
<p>Of course convergence of loss function values does not guarantee convergence of the <span class="math inline">\(x^{(k)}\)</span>. For the additional continuity, campactness, and identification conditions that are needed we refer to the majoriztaion and MM literature.</p>
</section>
<section id="sec-majorization" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Majorizing Strife</h1>
<p>The idea of minimizing a least absolute value (LAV) to obtain parameter estimates dates back to the work of Boskovitch in the middle of the eighteenth century. Until fairly recently it has been applied mainly to fit location parameters and more general linear models.</p>
<p>The pioneering work in strife minimization using smacof is <span class="citation" data-cites="heiser_88">Heiser (<a href="#ref-heiser_88" role="doc-biblioref">1988</a>)</span>, which builds on earlier work of <span class="citation" data-cites="heiser_87">Heiser (<a href="#ref-heiser_87" role="doc-biblioref">1987</a>)</span>. It is based on a creative use of the Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of the absolute value function.</p>
<p>The AM/GM inequality says that for all non-negative <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> we have <span id="eq-amgm"><span class="math display">\[
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),
\tag{7}\]</span></span> with equality if and only if <span class="math inline">\(|x|=|y|\)</span>. If <span class="math inline">\(|y|&gt;0\)</span> we can write <a href="#eq-amgm" class="quarto-xref">Equation&nbsp;7</a> as <span id="eq-amgmmaj"><span class="math display">\[
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\
\tag{8}\]</span></span> and this provides a quadratic majorization of <span class="math inline">\(|x|\)</span> at <span class="math inline">\(y\)</span>. There is no quadratic majorization of <span class="math inline">\(|x|\)</span> at <span class="math inline">\(y=0\)</span>, which is a problem we will have to deal with at some point.</p>
<p>Using the majorization <a href="#eq-amgmmaj" class="quarto-xref">Equation&nbsp;8</a>, and assuming <span class="math inline">\(\delta_k\not=d_k(Y)\)</span> for all <span class="math inline">\(k\)</span>, we define <span id="eq-wk1def"><span class="math display">\[
\omega_k(X):=\omega_k\frac{1}{r_k(X)},
\tag{9}\]</span></span> and, for a fixed <span class="math inline">\(Y\)</span>, <span id="eq-strifemaj"><span class="math display">\[
\eta(X):=\frac12\sum \omega_k(Y)(r_k^2(X)+r_k^2(Y)).
\tag{10}\]</span></span> Now <span class="math inline">\(\sigma(X)\leq\eta(X)\)</span> for all <span class="math inline">\(X\)</span> and <span class="math inline">\(\sigma(Y)=\eta(Y)\)</span>, and thus <span class="math inline">\(\eta\)</span> majorizes <span class="math inline">\(\sigma\)</span> at <span class="math inline">\(Y\)</span>.</p>
<section id="sec-amgm" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-amgm"><span class="header-section-number">3.1</span> Algorithm</h2>
<p>Reweighted smacof to minimize strife computes <span class="math inline">\(X^{(\nu+1)}\)</span> by minimizing or decreasing <span id="eq-sstrf"><span class="math display">\[
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X^{(\nu)}))^2,
\tag{11}\]</span></span> using a standard smacof step. It then computes the new weights <span class="math inline">\(\omega_k(X^{(\nu+1)})\)</span> from <a href="#eq-wk1def" class="quarto-xref">Equation&nbsp;9</a> and uses them in the next smacof step to update <span class="math inline">\(X^{(\nu+1)}\)</span>. And so on, until convergence.</p>
<p>A straightforward variation of the algorithm does a number of smacof steps between upgrading the weights. This still leads to a monotone, and thus convergent, algorithm. How many smacof steps we have to take in these inner iterations is something that needs further study. It is likely to depend on the fit of the data, on the shape of the function near the local minimum, and on how far the currfent iteration is from the local minimum.</p>
</section>
<section id="sec-zero" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-zero"><span class="header-section-number">3.2</span> Zero Residuals</h2>
<p>It may happen that for some <span class="math inline">\(k\)</span> we have <span class="math inline">\(d_k(X^{(\nu)})=\delta_k\)</span> while iterating. There have been various proposals to deal with such an unfortunate event, and we will discuss some of them below. Even more importantly we will see that that the minimizer of the absolute value loss usually satisfies <span class="math inline">\(d_k(X)=\delta_k\)</span> for quite a few elements, which means that near convergence the algorithm may become unstable because the weights from <a href="#eq-wk1def" class="quarto-xref">Equation&nbsp;9</a> become very large.</p>
<p>A large number of somewhat ad-hoc solutions have been proposed to deal with the problem of zero residuals, both in location analysis and in the statistical literature. We tend to agree with the assessment of <span class="citation" data-cites="aftab_hartley_15">Aftab and Hartley (<a href="#ref-aftab_hartley_15" role="doc-biblioref">2015</a>)</span>.</p>
<blockquote class="blockquote">
<p>.. attempts to analyze this difficulty [caused by infinite weights of IRLS for the <span class="math inline">\(\ell_p\)</span>-loss] have a long history of proofs and counterexamples to incorrect claims.</p>
</blockquote>
<p><span class="citation" data-cites="schlossmacher_73">Schlossmacher (<a href="#ref-schlossmacher_73" role="doc-biblioref">1973</a>)</span> is the first discussion of the majorization method in the statistical literature (for LAV linear regression). His proposal is to simply set a weight equal to zero if the corresponding residual is less than some small positive value <span class="math inline">\(\epsilon\)</span>. A similar approach, also used in location analysis, is to cap the weights at some large positive value. In <span class="citation" data-cites="heiser_88">Heiser (<a href="#ref-heiser_88" role="doc-biblioref">1988</a>)</span> all residuals smaller than this epsilon get a weight equal to the weighted average of all these small residuals. <span class="citation" data-cites="phillips_02">Phillips (<a href="#ref-phillips_02" role="doc-biblioref">2002</a>)</span> assumes double-exponential errors in LAV regression and then concludes that the EM algorithm gives the majorization method we have discussed. He uses <span class="math inline">\(\eqref{eq:wk1def}\)</span> throughout if all residuals are larger than <span class="math inline">\(\epsilon\)</span>. If one of more residuals are smaller than epsilon then the weight for those residuals is set equal to one, while for the remaining residuals the weight is set to epsilon divided by the absolute value of the residual. Often we get the assurance in these papers that the problem is not really important in practice, because it is very rare, and by just wiggling we will get to the unique solution anyway. But both in location analysis and in LAV regression the loss function is convex, however, which guarantees a unique minimum. This is certainly not the case in robust MDS. In this paper we try to follow a more systematic approach that uses smooth parametric approximations to the absolute value function, where the parameter can be used to make the approximation as precise as necessary.</p>
<p>In the case of stress the directional derivatives can be used to prove that if <span class="math inline">\(\omega_k\delta_k&gt;0\)</span> for all <span class="math inline">\(k\)</span> then stress is differentiable at each local minimum (<span class="citation" data-cites="deleeuw_A_84f">De Leeuw (<a href="#ref-deleeuw_A_84f" role="doc-biblioref">1984</a>)</span>). For strife to be differentiable we would have to prove that at a local minimum both <span class="math inline">\(d_k(X)&gt;0\)</span> and <span class="math inline">\((d_k(X)-\delta_k)\not= 0\)</span> for all <span class="math inline">\(k\)</span> with <span class="math inline">\(\omega_k&gt;0\)</span>. But this is impossible by the following argument.</p>
<p>In the one-dimensional case we can partition <span class="math inline">\(\mathbb{R}^n\)</span> into <span class="math inline">\(n!\)</span> polyhedral convex cones corresponding with the permutations of <span class="math inline">\(x\)</span>. Within each cone the distances are a linear function of <span class="math inline">\(x\)</span>. Each cone can be partitioned by intersecting it with the polyhedra defined by the linear inequalities <span class="math inline">\(\delta_k-d_k(x)\geq 0\)</span> or <span class="math inline">\(\delta_k-d_k(x)\leq 0\)</span>. Some of these intersections can and will obviously be empty. Within each of these non-empty polyhedral regions strife is a linear function of <span class="math inline">\(x\)</span>. Thus it attains its minimum for the region at a vertex, which is a solution for which some distances are zero and/or some residuals are zero. There can be no<br>
minima, local or global, in the interior of one of these polyhedral regions. We have thus shown that in one dimension strife is not differentiable at a local minimum, and that there is presumably a large number of them. Even for moderate <span class="math inline">\(n\)</span> the number of regions is of course too large to actually compute or draw.</p>
<p>In the multidimensional case linearity goes out the window. The set of configurations <span class="math inline">\(d_k(X)=\delta_k\)</span> is an ellipsoid and <span class="math inline">\(d_k(X)=0\)</span> defines a hyperplane. Strife is not differentiable at all intersections of these ellipsoids and hyperplanes. The partitioning of <span class="math inline">\(\mathbb{R}^n\)</span> by these ellipsoids and hyperplanes is not simple to describe. It has convex and non-convex cells, and within each cell strife is the difference of two weighted sums of distances. Anything can happen.</p>
</section>
<section id="ell_0-loss" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="ell_0-loss"><span class="header-section-number">3.3</span> <span class="math inline">\(\ell_0\)</span> loss</h2>
<p>A somewhat extreme special case of <a href="#eq-strifedef" class="quarto-xref">Equation&nbsp;3</a> has <span id="eq-elnul"><span class="math display">\[
f(x)=\begin{cases}
0&amp;\text{ if }x = 0,\\
1&amp;\text{ otherwise}.
\end{cases}
\tag{12}\]</span></span> This is <span class="math inline">\(\ell_0\)</span> loss. Minimizing <span class="math inline">\(\ell_0\)</span> loss means maximizing the number of cases with perfect fit, i.e.&nbsp;with <span class="math inline">\(\delta_k=d_k(X)\)</span>. The reason we mention it here is that the work of <span class="citation" data-cites="donoho_elad_03">Donoho and Elad (<a href="#ref-donoho_elad_03" role="doc-biblioref">2003</a>)</span> and <span class="citation" data-cites="candes_tao_05">Candes and Tao (<a href="#ref-candes_tao_05" role="doc-biblioref">2005</a>)</span> suggests that the minimizer of <span class="math inline">\(\ell_1\)</span> loss, i.e.&nbsp;absolute value loss, gives a good approximation to the minimizer of <span class="math inline">\(\ell_0\)</span> loss, at least in a number of special cases. In MDS we do not have linearity or convexity, but nevertheless the theoretical results in simpler cases are suggestive. We have seen that at least in the one-dimensional MDS case a number of residuals will indeed be zero at the optimum LAV solution.</p>
<p>There is an excellent review of the use of <span class="math inline">\(\ell_1\)</span> in various sparse recovery fields in <span class="citation" data-cites="candes_wakin_boyd_08">Candes, Wakin, and Boyd (<a href="#ref-candes_wakin_boyd_08" role="doc-biblioref">2008</a>)</span>. In that paper they also propose an iteratively reweighted LAV algorithm, which solves <span class="math inline">\(\ell_1\)</span> problems between weight updates. Maybe because of that they go so far as calling <span class="math inline">\(\ell_1\)</span> “the modern least squares”. But let’s not get carried away, in actual ease and frequency of use <span class="math inline">\(\ell_1\)</span> still has a long way to go if it wants to replace <span class="math inline">\(\ell_2\)</span>.</p>
</section>
</section>
<section id="generalizing-strife" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Generalizing Strife</h1>
<p>We have seen that <span class="citation" data-cites="heiser_88">Heiser (<a href="#ref-heiser_88" role="doc-biblioref">1988</a>)</span> applied majorization to minimize strife, using the AM/GM inequality. We now generalize this approach so that it can deal with other robust loss functions. A great number of different loss functions will be discussed. My intention is certainly not to confuse the reader and potential user by presenting a large number of alternatives with rather limited information. We show all these loss functions as examples of a general principle of algorithm construction and as examples of loss functions that have been used in statistics, location analysis, image analysis, and engineering over the years. They are all implemented in the function smacofRobust(), written in R (<span class="citation" data-cites="r_core_team_24">R Core Team (<a href="#ref-r_core_team_24" role="doc-biblioref">2024</a>)</span>), and listed in the appendix of this paper.</p>
<section id="sharp-quadratic-majorization" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sharp-quadratic-majorization"><span class="header-section-number">4.1</span> Sharp Quadratic Majorization</h2>
<p>The AM/GM inequality was used in <a href="#sec-majorization" class="quarto-xref">Section&nbsp;3</a> to construct a quadratic majorization of strife. In this paper we are specifically interested in sharp quadratic majorization, in which <span class="math inline">\(\mathfrak{H}\)</span> is the set of all (not necessarily convex) quadratics that majorize <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span>. This case has been studied in detail (in the case of real-valued functions on the line) in <span class="citation" data-cites="deleeuw_lange_A_09">De Leeuw and Lange (<a href="#ref-deleeuw_lange_A_09" role="doc-biblioref">2009</a>)</span>, and much of this section is taken from their paper. We added some minor extensions and reformulations.</p>
<p>For the loss functions we study in this paper there are two problems that have to be solved. First, we want a general procedure to construct quadratic majorizers. Second, we what to show that some of our majorizers are sharp.</p>
<p>If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(y\)</span>, then all quadratics <span class="math inline">\(g\)</span> that majorize <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> are of the form <span id="eq-qmaj"><span class="math display">\[
g_a(x):=f(y)+f'(y)(x-y)+\frac12a(x-y)^2
\tag{13}\]</span></span> for some <span class="math inline">\(a=g_a''(x)\)</span>, not necessarily positive. If <span class="math inline">\(f\)</span> is twice differentiable at <span class="math inline">\(y\)</span> then <span class="math inline">\(g_a''(y)\geq f''(y)\)</span> by <a href="#thm-diff" class="quarto-xref">Theorem&nbsp;1</a>, and thus we have the necessary condition <span class="math inline">\(a\geq f''(y)\)</span>. Note that not all functions have quadratic majorizations. If <span class="math inline">\(f\)</span> is a non-trivial cubic and <span class="math inline">\(g\)</span> is quadratic, then <span class="math inline">\(h=g-f\)</span> is a non-trivial cubic, and consequently we cannot have <span class="math inline">\(h\geq 0\)</span> on the whole real line.</p>
<p>We now look more closely at <span class="math inline">\(a\)</span> in <a href="#eq-qmaj" class="quarto-xref">Equation&nbsp;13</a>, initially concentrating on necessary conditions for majorization. For <span class="math inline">\(x\not = y\)</span> define <span id="eq-alpdef"><span class="math display">\[
\alpha(x):=\frac{f(x)-f(y)-f'(y)(x-y)}{\frac12(x-y)^2}.
\tag{14}\]</span></span> Of course <span class="math inline">\(\alpha\)</span> is a different function of <span class="math inline">\(x\)</span> for each <span class="math inline">\(y\)</span>, but since we are dealing with majorization at one fixed <span class="math inline">\(y\)</span> we suppress this dependence.</p>
<p>If <span class="math inline">\(f\)</span> is two times differentiable at <span class="math inline">\(y\)</span> then, by the definion of the second derivative, <span id="eq-alphop"><span class="math display">\[
\lim_{x\rightarrow y}\alpha(x)=f''(y),
\tag{15}\]</span></span> and thus can we define <span class="math inline">\(\alpha(y)=f''(y)\)</span> to make <span class="math inline">\(\alpha\)</span> continuous at <span class="math inline">\(y\)</span>.</p>
<p>If <span class="math inline">\(f\)</span> is convex, then <span class="math inline">\(\alpha\)</span> is the ratio of two positive convex functions of <span class="math inline">\(x\)</span>, and is thus positive. If <span class="math inline">\(f\)</span> is concave then <span class="math inline">\(\alpha\)</span> is negative. If <span class="math inline">\(f\)</span> is two times differentiable then there is a <span class="math inline">\(z\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> such that <span class="math inline">\(\alpha(x)=f''(z)\)</span>. Thus if <span class="math inline">\(f''(x)\leq K\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\alpha(x)\leq K\)</span> as well.</p>
<p>Quadratic majorization of <span class="math inline">\(f\)</span> by <span class="math inline">\(g_a\)</span> from <a href="#eq-qmaj" class="quarto-xref">Equation&nbsp;13</a> at <span class="math inline">\(y\)</span> is equivalent to <span class="math inline">\(\alpha(x)\leq a\)</span> for all <span class="math inline">\(x\)</span>. Thus <span class="math inline">\(g_a\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> if and only if <span class="math inline">\(\alpha\)</span> is bounded above by <span class="math inline">\(a\)</span>. If <span class="math inline">\(\alpha\)</span> is unbounded above there is no quadratic majorizer at <span class="math inline">\(y\)</span>. We can also define <span id="eq-aplus"><span class="math display">\[
a_+:=\sup_x\alpha(x),
\tag{16}\]</span></span> and say that sharp quadratic majorization at <span class="math inline">\(y\)</span> is possible if <span class="math inline">\(a_+&lt;+\infty\)</span>. We have majorization at <span class="math inline">\(y\)</span> by <span class="math inline">\(g_a\)</span> if <span class="math inline">\(a\geq a_+\)</span>, we have sharp majorization if <span class="math inline">\(a=a_+\)</span>. It follows that sharp quadratic majorizations exist if <span class="math inline">\(f''\)</span> is bounded above.</p>
<p>We illustrate these concepts by applying them to low-degree polynomials. First a cubic. Expand the cubic around <span class="math inline">\(y\)</span> as <span id="eq-cubexp"><span class="math display">\[
f(x)=f(y)+f'(y)(x-y)+\frac12f''(y)(x-y)^2+\frac16f'''(y)(x-y)^3.
\tag{17}\]</span></span> Thus <span id="eq-cubalp"><span class="math display">\[
\alpha(x)=f''(y)+\frac13f'''(y)(x-y).
\tag{18}\]</span></span> Since <span class="math inline">\(\alpha\)</span> is always unbounded above, no quadratic majorizer exists at any <span class="math inline">\(y\)</span>.</p>
<p>Now apply the same reasoning to a non-trivial quartic. We find <span id="eq-quaalp"><span class="math display">\[
\alpha(x)=f''(y)+\frac13f'''(y)(x-y)+\frac{1}{12}f^{iv}(y)(x-y)^2,
\tag{19}\]</span></span> a quadratic in <span class="math inline">\(x\)</span>. Of course for a quartic <span class="math inline">\(f^{iv}(y)\)</span> is the same for every <span class="math inline">\(y\)</span> and we may as well write <span class="math inline">\(f^{iv}\)</span>. If <span class="math inline">\(f^{iv}\)</span> is strictly positive the quadratic in <a href="#eq-quaalp" class="quarto-xref">Equation&nbsp;19</a> is unbounded above, and no quadratic majorization exists at any <span class="math inline">\(y\)</span>. If <span class="math inline">\(f^{iv}\)</span> is strictly negative then <span class="math inline">\(\alpha\)</span> has a maximum at <span class="math inline">\(x=y-2f'''(y)/f^{iv}\)</span>, and a sharp quadratic majorization exists at any <span class="math inline">\(y\)</span>.</p>
<p>We can get some more information about <span class="math inline">\(a_+\)</span> by differentiating <span class="math inline">\(\alpha\)</span>.</p>
<div id="thm-locmax" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> Suppose there is an <span class="math inline">\(x\)</span> where <span class="math inline">\(\alpha\)</span> is two times differentiable and <span class="math inline">\(a_+=\alpha(x)\)</span>. Then If <span class="math inline">\(\eqref{eq-loc1}\)</span> and the inequality in <span class="math inline">\(\eqref{eq-loc2}\)</span> is strict then <span class="math inline">\(\alpha\)</span> has a local maximum at <span class="math inline">\(x\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>After some manipulation <span class="math inline">\(\eqref{eq-loc1}\)</span> and <span class="math inline">\(\eqref{eq-loc2}\)</span> are the necessary conditions <span class="math inline">\(\alpha'(x)=0\)</span> and <span class="math inline">\(\alpha''(x)\leq 0\)</span> for a local maximum. If <span class="math inline">\(\alpha'(x)=0\)</span> and <span class="math inline">\(\alpha''(x)&gt;0\)</span> the conditions are sufficient. If <span class="math inline">\(x\)</span> satisfies <span class="math inline">\(\eqref{eq-loc1}\)</span> then substitution in <a href="#eq-alpdef" class="quarto-xref">Equation&nbsp;14</a> gives <span class="math inline">\(\eqref{eq-loc3}\)</span>.</p>
</div>
<p>Note that we have not shown that <span class="math inline">\(\alpha\)</span> always attains its maximum. <span class="citation" data-cites="deleeuw_lange_A_09">De Leeuw and Lange (<a href="#ref-deleeuw_lange_A_09" role="doc-biblioref">2009</a>)</span> give the example of the differentiable function <span class="math display">\[\begin{equation}
f(x)=\begin{cases}
x^2&amp;\text{ if }x\leq 1,\\
2x-1&amp;\text{ otherwise},
\end{cases}\label{eq:dllexam}
\end{equation}\]</span> which has <span class="math inline">\(\alpha(x)=0\)</span> for <span class="math inline">\(x&gt;1\)</span> and <span class="math inline">\(\alpha(x)&lt;2\)</span> for <span class="math inline">\(x\leq 1\)</span>, so that <span class="math inline">\(a_+=\sup_{x\leq 1}\alpha(x)=2\)</span> and the maximum does not exist.</p>
<p>Also, the conditions of <a href="#thm-locmax" class="quarto-xref">Theorem&nbsp;2</a> cannot possibly show that <span class="math inline">\(\alpha\)</span> has a <em>global</em> maximum at <span class="math inline">\(x\)</span>, and that consequently <span class="math inline">\(g_a\)</span> of <a href="#eq-qmaj" class="quarto-xref">Equation&nbsp;13</a> with <span class="math inline">\(a\)</span> given by <span class="math inline">\(\eqref{eq-loc3}\)</span> is a sharp quadratic majorizer.</p>
<p>Differentiation of <span class="math inline">\(\alpha\)</span> gives the following result.</p>
<div id="cor-incdec" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1</strong></span> Suppose <span class="math inline">\(\alpha\)</span> is differentiable. Then it is strictly increasing if and only if for all <span class="math inline">\(x\)</span> <span id="eq-strinc"><span class="math display">\[
\frac{f(x)-f(y)}{x-y}&lt;\frac12(f'(x)+f'(y)),
\tag{20}\]</span></span> and strictly decreasing if and only if for all <span class="math inline">\(x\)</span> <span id="eq-strdec"><span class="math display">\[
\frac{f(x)-f(y)}{x-y}&lt;\frac12(f'(x)+f'(y)).
\tag{21}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>These are the conditions <span class="math inline">\(\alpha'(x)&gt;0\)</span> and <span class="math inline">\(\alpha'(x)&lt;0\)</span> for all <span class="math inline">\(x\)</span>.</p>
</div>
<p>If <a href="#eq-strinc" class="quarto-xref">Equation&nbsp;20</a> or <a href="#eq-strdec" class="quarto-xref">Equation&nbsp;21</a> is true then <span class="math inline">\(\alpha\)</span> does not have a maximum, and the supremum (possibly infinite) is the limit of <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(-\infty\)</span> or <span class="math inline">\(+\infty\)</span>.</p>
<p>Because of our robust loss functions we are especially interested in the case that <span class="math inline">\(f\)</span> is even. Setting <span class="math inline">\(x=-y\)</span> makes both sides of <span class="math inline">\(\eqref{eq-loc1}\)</span> equal to zero. Thus <span class="math inline">\(\alpha\)</span> has a stationary point at <span class="math inline">\(x=-y\)</span>. If in addition <span id="eq-miny"><span class="math display">\[
f''(y)&lt;\frac{f'(y)}{y}
\tag{22}\]</span></span> then <span class="math inline">\(\alpha\)</span> has a local maximum at <span class="math inline">\(x=-y\)</span>.</p>
<div id="thm-evena" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> Suppose <span class="math inline">\(f\)</span> is even and twice-differentiable. If <span class="math inline">\(f'(x)/x\)</span> is decreasing on the positive real line then <span class="math inline">\(\alpha\)</span> has a local maximum at <span class="math inline">\(x=-y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(f'(x)/x\)</span> is strictly decreasing its derivative is strictly negative. Thus <span class="math inline">\(xf''(x)-f'(x)&lt;0\)</span> for <span class="math inline">\(x&gt;0\)</span>. For an even function <span class="math inline">\(f'(x)/x\)</span> is strictly decreasing on the positive real line if and only if it is strictly increasing on the negative real line. Thus <span class="math inline">\(xf''(x)-f'(x)&gt;0\)</span> for <span class="math inline">\(x&lt;0\)</span>. In both cases <a href="#eq-miny" class="quarto-xref">Equation&nbsp;22</a> follows.</p>
</div>
</section>
<section id="sec-twosupport" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-twosupport"><span class="header-section-number">4.2</span> Two Support Points</h2>
<p>We can say more if it is known that the quadratic majorization has more than one support point.</p>
<div id="thm-twop" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> Suppose the quadratic <span class="math inline">\(g_a\)</span> of <a href="#eq-qmaj" class="quarto-xref">Equation&nbsp;13</a> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and at <span class="math inline">\(z\not= y\)</span>. Then <span id="eq-atwop"><span class="math display">\[
a=\frac{f'(z)-f'(y)}{z-y}
\tag{23}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From <a href="#eq-qmaj" class="quarto-xref">Equation&nbsp;13</a> we have <span class="math inline">\(g_a'(x)=f'(y)+a(x-y)\)</span>. But because of <a href="#thm-diff" class="quarto-xref">Theorem&nbsp;1</a> we must also have <span class="math inline">\(g_a'(z)=f'(z)\)</span>. Thus <span class="math inline">\(g_a'(z)=f'(y)+a(z-y)=f'(z)\)</span>.</p>
</div>
<p>Again, we have not shown that <span class="math inline">\(g_a\)</span> with <span class="math inline">\(a\)</span> from <a href="#eq-atwop" class="quarto-xref">Equation&nbsp;23</a> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>. Only the reverse implication, which is that if <span class="math inline">\(g_a\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> then <span class="math inline">\(g_a\)</span> is uniquely determined by <a href="#eq-atwop" class="quarto-xref">Equation&nbsp;23</a>. In practice, even if one knows the support points <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>, one still has to prove majorization. This is precisely how <span class="citation" data-cites="heiser_88">Heiser (<a href="#ref-heiser_88" role="doc-biblioref">1988</a>)</span>, <span class="citation" data-cites="verboon_heiser_94">Verboon and Heiser (<a href="#ref-verboon_heiser_94" role="doc-biblioref">1994</a>)</span>, and <span class="citation" data-cites="groenen_giaquinto_kiers_03">Groenen, Giaquinto, and Kiers (<a href="#ref-groenen_giaquinto_kiers_03" role="doc-biblioref">2003</a>)</span> establish their majorizations.</p>
<p>In his master’s thesis <span class="citation" data-cites="vanruitenburg_05">Van Ruitenburg (<a href="#ref-vanruitenburg_05" role="doc-biblioref">2005</a>)</span> takes us one step further down the necessary conditions road. To present his main result we need two lemmas.</p>
<div id="lem-fan" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> If different quadratics <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> majorize <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> then either <span class="math inline">\(g\)</span> strictly majorizes <span class="math inline">\(h\)</span> or <span class="math inline">\(h\)</span> strictly majorizes <span class="math inline">\(g\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have Thus <span class="math inline">\(g(x)-h(x)=\frac12(a_1-a_2)(x-y)^2\)</span>, which is either strictly positive or strictly negative for <span class="math inline">\(x\not= y\)</span>.</p>
</div>
<div id="lem-ruit" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2</strong></span> Suppose quadratics <span class="math inline">\(g\)</span> and <span class="math inline">\(h\not= g\)</span> majorize <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span>. Suppose, in addition, that <span class="math inline">\(g\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(z\not=y\)</span>. Then <span class="math inline">\(h\)</span> strictly majorizes <span class="math inline">\(g\)</span> at <span class="math inline">\(y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By <a href="#lem-fan" class="quarto-xref">Lemma&nbsp;1</a> one of <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> has to strictly majorize the other. If <span class="math inline">\(g\)</span> strictly majorizes <span class="math inline">\(h\)</span> then <span class="math inline">\(h(z)&lt;g(z)=f(z)\)</span> and thus <span class="math inline">\(h\)</span> does not majorize <span class="math inline">\(f\)</span>, contrary to assumption. It follows that <span class="math inline">\(h\)</span> strictly majorizes <span class="math inline">\(g\)</span> at <span class="math inline">\(y\)</span>.</p>
</div>
<div id="thm-ruit" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5</strong></span> If the quadratic <span class="math inline">\(g\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and at <span class="math inline">\(z\not= y\)</span>, then <span class="math inline">\(g\)</span> is a sharp majorizer of <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Directly from <a href="#lem-ruit" class="quarto-xref">Lemma&nbsp;2</a>.</p>
</div>
<p>Again our results simplify if the function <span class="math inline">\(f\)</span> is even.</p>
<div id="thm-evqu" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6</strong></span> If <span class="math inline">\(f\)</span> is even and the quadratic <span class="math inline">\(g\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and <span class="math inline">\(-y\)</span>, where <span class="math inline">\(y\not= 0\)</span>, then <span class="math inline">\(g\)</span> is the even quadratic given by <span id="eq-geven"><span class="math display">\[
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
\tag{24}\]</span></span> Moreover <span class="math inline">\(g\)</span> is the sharp quadratic majorization of <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> and <span class="math inline">\(-y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(f\)</span> is even then <span class="math inline">\(f'\)</span> is odd. Consequently <a href="#eq-atwop" class="quarto-xref">Equation&nbsp;23</a> becomes <span id="eq-aeven"><span class="math display">\[
a=\frac{f'(y)}{y},
\tag{25}\]</span></span> which is even. Moreover because <span class="math inline">\(g\)</span> majorizes <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span> Averaging the two equations <span class="math inline">\(\eqref{eq-gatplusy}\)</span> and <span class="math inline">\(\eqref{eq-gatminy}\)</span> for <span class="math inline">\(g\)</span>, and simplifying, gives the required result. That the majorization is sharp follows from <a href="#thm-ruit" class="quarto-xref">Theorem&nbsp;5</a>.</p>
</div>
</section>
<section id="sufficient-conditions" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sufficient-conditions"><span class="header-section-number">4.3</span> Sufficient Conditions</h2>
<p><span class="citation" data-cites="deleeuw_lange_A_09">De Leeuw and Lange (<a href="#ref-deleeuw_lange_A_09" role="doc-biblioref">2009</a>)</span> gives a way to construct quadratic majorizers of a differentiable function on the real line. We give a slightly more general version of their basic theorem that provides a convenient way to deal with loss functions that are not differentiable everywhere. Our proofs are different.</p>
<div id="thm-wght" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7</strong></span> Suppose there is a function <span class="math inline">\(h\)</span>, concave on <span class="math inline">\(\mathbb{R}^+\)</span>, such that <span class="math inline">\(f(x)=h(x^2)\)</span>. Then <span id="eq-given"><span class="math display">\[
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
\tag{26}\]</span></span> is a sharp quadratic majorizer of <span class="math inline">\(f\)</span> at <span class="math inline">\(y\)</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By concavity for all <span class="math inline">\(u\in\mathbb{R}^+\)</span> and for all <span class="math inline">\(v\in\mathbb{R}^+\)</span> for which <span class="math inline">\(h'(v)\)</span> exists <span class="math display">\[
h(u)\leq h(v)+h'(v)(u-v).
\]</span> Substituting <span class="math inline">\(u=x^2\)</span> and <span class="math inline">\(v=y^2\)</span> gives the quadratic majorization <span class="math display">\[
f(x)\leq f(y)+h'(y^2)(x^2-y^2)
\]</span> This is true for all real <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, provided <span class="math inline">\(h\)</span> is differentiable at <span class="math inline">\(y^2\)</span>.</p>
<p>Using <span class="math inline">\(f'(y)=2yh'(y^2)\)</span> … becomes <span class="math display">\[
f(x)\leq f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
\]</span></p>
</div>
<div id="cor:a">
<p>Equivalently <span class="math inline">\(f(\sqrt{x})\)</span> is concave on <span class="math inline">\(\mathbb{R}^+\)</span>. Proof: Substituting <span class="math inline">\(y=x^2\)</span> in <span class="math inline">\(f(x)=h(x^2)\)</span> gives <span class="math inline">\(f(\sqrt{y})=h(y)\)</span> for all <span class="math inline">\(y\geq 0\)</span> and thus <span class="math inline">\(f(\sqrt{y})\)</span> is concave.</p>
</div>
<div id="cor:b">
<p>Equivalently <span class="math inline">\(f'(x)/x\)</span> is decreasing on <span class="math inline">\(\mathbb{R}^+\)</span>. This mean that A differentiable function is convex if and only if its derivative is decreasing. More precisely: if <span class="math inline">\(x&gt;y\)</span> and <span class="math inline">\(f\)</span> is concave and differentiable at both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> then <span class="math inline">\(f'(x)\leq f'(y)\)</span>.</p>
</div>
<p>Suppose <span class="math inline">\(f\)</span> is the absolute value function, so that <span class="math inline">\(f'\)</span> is the sign function for <span class="math inline">\(x\not= 0\)</span>, and <span class="math inline">\(f'\)</span> is not defined at <span class="math inline">\(x=0\)</span>. Then <span class="math inline">\(w\)</span> is the even function with <span class="math inline">\(w(x)=1/|x|\)</span> for all <span class="math inline">\(x\not=0\)</span>. This satisfies the conditions in <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a>, and thus for <span class="math inline">\(y\not= 0\)</span> we have the quadratic majorization <span class="math display">\[
|x|\leq|y|+\frac12\frac{1}{|y|}(x^2-y^2)=\frac12\frac{1}{|y|}(x^2+y^2),
\]</span> which is the AM/GM inequality.</p>
<div id="thm-sqrt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8</strong></span> The ratio <span class="math inline">\(f'(x)/x\)</span> is decreasing on <span class="math inline">\((0,\infty)\)</span> if and only <span class="math inline">\(f(\sqrt{x})\)</span> is concave. The set of functions satisfying this condition is closed under the formation of (a) positive multiples, (b) convex combinations, (c) limits, and (d) composition with a concave increasing function <span class="math inline">\(g(x)\)</span>.</p>
</div>
<p>We now apply <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> to functions of the form <span id="eq-fstressdef"><span class="math display">\[
\sigma_f(X):=\sum \omega_k\ f(\delta_k-d_k(X)),
\tag{27}\]</span></span> where <span class="math inline">\(f\)</span> satisfies the conditions in the theorem. If <span id="eq-fstressmaj"><span class="math display">\[
\eta(X):=\sum \omega_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),
\tag{28}\]</span></span> then <span class="math inline">\(\eta\)</span> is a sharp quadratic majorization at <span class="math inline">\(Y\)</span>.</p>
<p>In iteration <span class="math inline">\(k\)</span> the robust smacof algorithm does a smacof step towards minimization of <span class="math inline">\(\eta\)</span> over <span class="math inline">\(X\)</span>. We can ignore the parts of <a href="#eq-fstressmaj" class="quarto-xref">Equation&nbsp;28</a> that only depend on <span class="math inline">\(Y\)</span>, and minimize <span id="eq-fstressaux"><span class="math display">\[
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X))^2,
\tag{29}\]</span></span> with <span id="eq-wkdef"><span class="math display">\[
\omega_k(X^{(\nu)}):=\omega_k\frac{f'(\delta_k-d_k(X^{(\nu)}))}{2(\delta_k-d_k(Y))}.
\tag{30}\]</span></span> We then recomputes the weights <span class="math inline">\(\omega_k(X^{(\nu+1)})\)</span> and go to the smacof step again. This can be thought of as iteratively reweighted least squares (IRLS), and also as nested majorization, with the smacof majorization based on the Cauchy-Schwartz inequality within the sharp quadratic majorization of the loss function based on <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a>.</p>
</section>
</section>
<section id="power-smoothers" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Power Smoothers</h1>
<p>We first discuss a class of smoothers of the absolute value function that maintain most of its structure. They have a shift parameter <span class="math inline">\(c\)</span> that takes care of the non-differentiability. Although different smoothers have different scales and interpretations for <span class="math inline">\(c\)</span>, we will use the same symbol throughout. Also some smoothers have a power parameter <span class="math inline">\(q\)</span> that determines the shape of the loss function bowl.</p>
<section id="sec-charb" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-charb"><span class="header-section-number">5.1</span> Charbonnier</h2>
<p>The first, and perhaps most obvious, choice for smoothing the absolute value function is <span id="eq-charf"><span class="math display">\[
f_c(x)=\sqrt{x^2 + c^2}.
\tag{31}\]</span></span> In the engineering literature <a href="#eq-charf" class="quarto-xref">Equation&nbsp;31</a> is known as Charbonnier loss, after <span class="citation" data-cites="charbonnier_blanc-feraud_aubert_barlaud_94">Charbonnier et al. (<a href="#ref-charbonnier_blanc-feraud_aubert_barlaud_94" role="doc-biblioref">1994</a>)</span>, who were possibly the first researchers to use it in image restauration. <span class="citation" data-cites="ramirez_sanchez_kreinovich_argaez_14">Ramirez et al. (<a href="#ref-ramirez_sanchez_kreinovich_argaez_14" role="doc-biblioref">2014</a>)</span> count the number of computer operations and conclude that <a href="#eq-charf" class="quarto-xref">Equation&nbsp;31</a> is also the “most computationally efficient smooth approximation to <span class="math inline">\(|x|\)</span>”.</p>
<p>For <span class="math inline">\(c&gt;0\)</span> we have <span class="math inline">\(f_c(x)&gt;|x|\)</span>. If <span class="math inline">\(c\rightarrow 0\)</span> then <span class="math inline">\(f_c(x)\)</span> decreases monotonically to <span class="math inline">\(|x|\)</span>. Also <span class="math inline">\(\max_x|f_c(x)-|x||=c\)</span> attained at <span class="math inline">\(x=0\)</span>, which implies uniform convergence of <span class="math inline">\(f_c\)</span> to <span class="math inline">\(|x|\)</span>.</p>
<p>By l’Hôpital Thus if <span class="math inline">\(x\)</span> is much smaller than <span class="math inline">\(c\)</span> then loss is approximately a quadratic in <span class="math inline">\(x\)</span>, and if <span class="math inline">\(x\)</span> is much larger than <span class="math inline">\(c\)</span> then loss is approximately the absolute value.</p>
<p>Loss function <a href="#eq-charf" class="quarto-xref">Equation&nbsp;31</a> is infinitely many times differentiable. Its first derivative is <span class="math display">\[\begin{equation}
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,\label{eq:charg}
\end{equation}\]</span> which converges, again in the sup-norm and uniformly, to the sign function if <span class="math inline">\(c\rightarrow 0\)</span>. The IRLS weights are <span class="math display">\[\begin{equation}
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}\label{eq:charw}
\end{equation}\]</span> which is clearly a decreasing function of <span class="math inline">\(x\)</span> on <span class="math inline">\(\mathbb{R}^+\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-char" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-char-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Charbonnier Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="generalized-charbonnier" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="generalized-charbonnier"><span class="header-section-number">5.2</span> Generalized Charbonnier</h2>
<p>The loss function <span class="math inline">\((x^2+c^2)^\frac12\)</span> smoothes <span class="math inline">\(|x|\)</span>. In the same way generalized Charbonnier loss smoothes <span class="math inline">\(\ell_p\)</span> loss <span class="math inline">\(|x|^q\)</span>. We have a two-parameter family of loss functions in this case. <span id="eq-gcharf"><span class="math display">\[
f_{c,q}(x):=(x^2+c^2)^{\frac12 q}
\tag{32}\]</span></span> <span class="math display">\[\begin{equation}
w_{c,q}(x)=q(x^2+c^2)^{\frac12 q-1}\label{eq:gcharw}
\end{equation}\]</span> which is non-increasing for <span class="math inline">\(q\leq 2\)</span>. Note that we do not assume that <span class="math inline">\(q&gt;0\)</span>, and consequently generalized Charbonnier loss provides us with more flexibility than Charbonnier loss from <a href="#eq-charf" class="quarto-xref">Equation&nbsp;31</a>. Of course if <span class="math inline">\(q&lt;0\)</span> “loss” becomes “gain”, with a maximum at zero instead of a minimum. To get a proper loss function, take the negative. <a href="#fig-gchar" class="quarto-xref">Figure&nbsp;2</a> plots generalized Charbonnier loss for some negative values of <span class="math inline">\(q\)</span>. We see that for <span class="math inline">\(\alpha\rightarrow-\infty\)</span> generalized Charbonnier loss aproximates <span class="math inline">\(\ell_0\)</span> loss.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gchar" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gchar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-gchar-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gchar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Generalized Charbonnier Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="barron" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="barron"><span class="header-section-number">5.3</span> Barron</h2>
<p>There are a fair number of generalizations of the power smoother loss functions in the engineering literature. We will discuss one nice generalization from <span class="citation" data-cites="barron_19">Barron (<a href="#ref-barron_19" role="doc-biblioref">2019</a>)</span>.</p>
<p><span id="eq-barron"><span class="math display">\[
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).\
\tag{33}\]</span></span></p>
<p>To quote Barron</p>
<blockquote class="blockquote">
<p>Here <span class="math inline">\(\alpha\in\mathbb{R}\)</span> is a shape parameter that controls the robustness of the loss and <span class="math inline">\(c&gt;0\)</span> is a scale parameter that controls the size of the loss’s quadratic bowl near <span class="math inline">\(x=0\)</span>.</p>
</blockquote>
<p>A number of interesting special cases of <a href="#eq-barron" class="quarto-xref">Equation&nbsp;33</a> are obtained by selecting various values of the <span class="math inline">\(\alpha\)</span> parameters. For <span class="math inline">\(\alpha=1\)</span> it becomes Charbonnier loss, and for <span class="math inline">\(\alpha=-2\)</span> it is Geman-McClure loss. There are also some limiting cases. For <span class="math inline">\(\alpha\rightarrow 2\)</span> Barron loss becomes squared error loss, for <span class="math inline">\(\alpha\rightarrow 0\)</span> it becomes Cauchy loss, and for <span class="math inline">\(\alpha\rightarrow-\infty\)</span> it becomes Welsch loss. Accordingly <span id="eq-barronf"><span class="math display">\[
f'_{\alpha,c}(x)=\begin{cases}
\frac{x}{c^2}&amp;\text{ if }\alpha=2,\\
\frac{2x}{x^2+2c^2}&amp;\text{ if }\alpha=0,\\
\frac{x}{c^2}\exp\left(-\frac12(x/c)^2\right)&amp;\text{ if }\alpha\rightarrow-\infty,\\
\frac{x}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&amp;\text{ otherwise}.
\end{cases}
\tag{34}\]</span></span> and thus <span id="eq-barronh"><span class="math display">\[
h_{\alpha,c}(x)=\begin{cases}
\frac{1}{c^2}&amp;\text{ if }\alpha=2,\\
\frac{2}{x^2+2c^2}&amp;\text{ if }\alpha=0,\\
\frac{1}{c^2}\exp\left(-\frac12(x/c)^2\right)&amp;\text{ if }\alpha\rightarrow-\infty,\\
\frac{1}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&amp;\text{ otherwise}.
\end{cases}
\tag{35}\]</span></span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barron" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-barron-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Barron Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="convolution-smoothers" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Convolution Smoothers</h1>
<p>Suppose <span class="math inline">\(\pi\)</span> is a probability density, symmetric around zero, with finite or infinite support, expectation zero, and variance one. Define the convolution <span class="math display">\[
f_c(x):=\frac{1}{c}\int_{-\infty}^{+\infty}|x-y|\ \pi(\frac{y}{c})dy.
\]</span> Now <span class="math inline">\(c^{-1}\pi(y/c)\)</span> is still a symmetric probability density integrating to one, with expectation zero, but it now has variance <span class="math inline">\(c^2\)</span>. Thus if <span class="math inline">\(c\rightarrow 0\)</span> it becomes more and more like the Dirac delta function and <span class="math inline">\(f_c(x)\)</span> converges to the absolute value function.</p>
<p>It is clear that we can use any scale family of probability densities to define convolution smoothers. There is an infinite number of possible choices, with finite or infinite support, smooth or nonsmooth, using splines or wavelets, and so on. We give two quite different examples.</p>
<section id="huber" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="huber"><span class="header-section-number">6.1</span> Huber</h2>
<p>Take <span class="math display">\[
\pi(x)=\begin{cases}\frac12 &amp;\text{ if }|x|\leq 1,\\0&amp;\text{ otherwise.}\end{cases}
\]</span> Then <span id="eq-hubera"><span class="math display">\[
f_c(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&amp;\text{ if }|x|\leq c,\\
|x|&amp;\text{ otherwise}.
\end{cases}
\tag{36}\]</span></span></p>
<p>The Huber function (<span class="citation" data-cites="huber_64">Huber (<a href="#ref-huber_64" role="doc-biblioref">1964</a>)</span>) is traditionally transformed linearly so that it is zero for <span class="math inline">\(x=0\)</span>. This gives <span id="eq-huberb"><span class="math display">\[
f_c(x)=\begin{cases}
\frac12x^2&amp;\text{ if }|x|&lt;c,\\
c|x|-\frac12 c^2&amp;\text{ otherwise}.
\end{cases}
\tag{37}\]</span></span> For robust estimation and IRLS it does not matter if we use <a href="#eq-hubera" class="quarto-xref">Equation&nbsp;36</a> or <a href="#eq-huberb" class="quarto-xref">Equation&nbsp;37</a>. Our discussion in the introduction suggests that if we just want a smoother of the absolute value function, then <a href="#eq-hubera" class="quarto-xref">Equation&nbsp;36</a> is the natural choice, if we want a robust loss function that combines the advantages of least squares and least absolute value then that leads us to <a href="#eq-huberb" class="quarto-xref">Equation&nbsp;37</a>.</p>
<p>Because Charbonnier loss behaves the same way as Huber loss, as absolute value loss for large <span class="math inline">\(x\)</span> and as squared loss for small <span class="math inline">\(x\)</span>, it is also known as Pseudo-Huber loss.</p>
<p>The Huber function is differentiable, although not twice diffentiable. Its derivative is <span class="math display">\[
f'(x)=\begin{cases}
c&amp;\text{ if }x\geq c,\\
x&amp;\text{ if }|x|\leq c,\\
-c&amp;\text{ if }x\leq -c.
\end{cases}
\]</span> <span class="math display">\[
\omega(x)=
\begin{cases}
\hfill\frac{c}{x}&amp;\text{ if }x\geq c,\\
\hfill1&amp;\text{ if }|x|\leq c,\\
-\frac{c}{x}&amp;\text{ if }x\leq -c.
\end{cases}
\]</span> The Huber function is even and differentiable. Moreover <span class="math inline">\(f'(x)/x\)</span> decreases from. Thus <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> applies.</p>
<p>The MDS majorization algorithm for the Huber loss is to update <span class="math inline">\(Y\)</span> by minimizing (or by performing one smacof step to decrease) <span class="math display">\[
\sum \omega_k(Y)(\delta_k-d_k(X))^2
\]</span> where <span class="math display">\[
\omega_k(Y)=\begin{cases}
\omega_k&amp;\text{ if }|\delta_k-d_k(Y)|&lt;c,\\
\frac{c\omega_k}{|\delta_k-d_k(Y)|}&amp;\text{ otherwise}.
\end{cases}
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-huber" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-huber-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-huber-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-huber-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Huber Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="gaussian" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="gaussian"><span class="header-section-number">6.2</span> Gaussian</h2>
<p>In <span class="citation" data-cites="deleeuw_E_18f">De Leeuw (<a href="#ref-deleeuw_E_18f" role="doc-biblioref">2018</a>)</span> we also discussed the convolution smoother proposed by <span class="citation" data-cites="voronin_ozkaya_yoshida_15">Voronin, Ozkaya, and Yoshida (<a href="#ref-voronin_ozkaya_yoshida_15" role="doc-biblioref">2014</a>)</span>. The idea is to use the convolution of the absolute value function and a Gaussian pdf. <span class="math display">\[
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
\]</span></p>
<p>Carrying out the integration gives</p>
<p><span class="math display">\[
f_c(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
\]</span> The derivative is <span class="math display">\[
f'_c(x)=2\Phi(x/c)-1
\]</span> It may not be immediately obvious in this case that the weight function <span class="math inline">\(f'(x)/x\)</span> is non-increasing on <span class="math inline">\(\mathbb{R}^+\)</span>. We prove that its derivative is negative on <span class="math inline">\((0,+\infty)\)</span>. The derivative of <span class="math inline">\(f'(x)/x\)</span> has the sign of <span class="math inline">\(xf''(x)-f'(x)\)</span>, which is <span class="math inline">\(z\phi(z)-\Phi(z)+1/2\)</span>, with <span class="math inline">\(z=x/c\)</span>. It remains to show that <span class="math inline">\(\Phi(z)-z\phi(z)\geq\frac12\)</span>, or equivalently that <span class="math inline">\(\int_0^z\phi(x)dx-z\phi(z)\geq 0\)</span>. Now if <span class="math inline">\(0\leq x\leq z\)</span> then <span class="math inline">\(\phi(x)\geq\phi(z)\)</span> and thus <span class="math inline">\(\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)\)</span>, which completes the proof.</p>
<p><span class="math display">\[
\omega_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gaussian" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gaussian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-gaussian-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gaussian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Gaussian Convolution Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="a-bouquet-of-loss-functions" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> A Bouquet of Loss Functions</h1>
<p>In the early seventies, after the pioneering mostly theoretical work in robust statistics of Huber, Hampel, and Tukey, the mainframe computer allowed statisticians to make large-scale comparisons of many robust loss functions. The most impressive of such comparisons was the Princeton Robustness Study (<span class="citation" data-cites="andrews_bickel_hampel_huber_rogers_tukey_72">Andrews et al. (<a href="#ref-andrews_bickel_hampel_huber_rogers_tukey_72" role="doc-biblioref">1972</a>)</span>).</p>
<p>In <span class="citation" data-cites="holland_welsch_77">Holland and Welsch (<a href="#ref-holland_welsch_77" role="doc-biblioref">1977</a>)</span> the computer package ROSEPACK was introduced that made it relatively easy to compute robust estimators using several different loss functions. Eight different weight functions were implemented as options. Somewhat later <span class="citation" data-cites="coleman_holland_kaden_klema_peters_80">Coleman et al. (<a href="#ref-coleman_holland_kaden_klema_peters_80" role="doc-biblioref">1980</a>)</span> made an more modern computer implementation available, using the same eight weight functions, which was not limited to mainframes.</p>
<p>We have implemented the same eight weight functions in smacofRobust. Below we give formulas for the loss function, the influence function, and the weight function. One of the eight is Huber loss, which we already discussed in the convolution section. We graph the remaining seven loss functions for selected values of the “tuning constants” <span class="math inline">\(c\)</span>.</p>
<p><span class="citation" data-cites="holland_welsch_77">Holland and Welsch (<a href="#ref-holland_welsch_77" role="doc-biblioref">1977</a>)</span>, following <span class="citation" data-cites="andrews_bickel_hampel_huber_rogers_tukey_72">Andrews et al. (<a href="#ref-andrews_bickel_hampel_huber_rogers_tukey_72" role="doc-biblioref">1972</a>)</span>, distnguish between “hard redescenders” that have an influence function <span class="math inline">\(f'\)</span> equal to zero if <span class="math inline">\(x\)</span> is large enough (Andrews, Tukey, and Hinich loss), “soft redescenders” with influence functions asymptotic to zero for large <span class="math inline">\(x\)</span> (Cauchy, Welsch loss), and loss functions with a monotone influence function (Huber, Logistic, Fair loss)</p>
<section id="andrews" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="andrews"><span class="header-section-number">7.1</span> Andrews</h2>
<p>The first loss function in this section is taken from <span class="citation" data-cites="andrews_bickel_hampel_huber_rogers_tukey_72">Andrews et al. (<a href="#ref-andrews_bickel_hampel_huber_rogers_tukey_72" role="doc-biblioref">1972</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
f(x)&amp;=\begin{cases}
c^2(1-\cos(x/c))&amp;\text{ if }|x|\leq\pi c,\\
2c^2&amp;\text{ otherwise.}
\end{cases}\\
f'(x)&amp;=\begin{cases}
c\sin(x/c)&amp;\text{ if }|x|\leq\pi c,\\
0&amp;\text{ otherwise.}
\end{cases}\\
\omega(x)&amp;=\begin{cases}
(x/c)^{-1}\sin(x/c)&amp;\text{ if }|x|\leq\pi c,\\
0&amp;\text{ otherwise.}
\end{cases}
\end{align}\]</span></p>
<p>Because <span class="math inline">\(\cos\)</span> is even and <span class="math inline">\(\sin(x)/x\)</span> decreases on <span class="math inline">\([0,\pi]\)</span> <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> applies.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-andrews" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-andrews-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-andrews-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-andrews-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Andrews Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="tukey" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="tukey"><span class="header-section-number">7.2</span> Tukey</h2>
<p>The usual reference for Tukey loss is <span class="citation" data-cites="beaton_tukey_74">Beaton and Tukey (<a href="#ref-beaton_tukey_74" role="doc-biblioref">1974</a>)</span>, although closely related hard redescenders are also in <span class="citation" data-cites="andrews_bickel_hampel_huber_rogers_tukey_72">Andrews et al. (<a href="#ref-andrews_bickel_hampel_huber_rogers_tukey_72" role="doc-biblioref">1972</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
f(x)&amp;=\begin{cases}
\frac{c^2}{6}\left(1-\left(1-(x/c)^2\right)^3\right)&amp;\text{ if } |x|\leq c,\\
\frac{c^2}{6}&amp;\text{ otherwise}.
\end{cases}\\
f'(x)&amp;=\begin{cases}
x\left(1-\left(1-(x/c)^2\right)^2\right)&amp;\text{ if } |x|\leq c,\\
0&amp;\text{ otherwise}.
\end{cases}\\
\omega(x)&amp;=\begin{cases}
\left(1-\left(1-(x/c)^2\right)^2\right)&amp;\text{ if } |x|\leq c,\\
0&amp;\text{ otherwise}.
\end{cases}
\end{align}\]</span></p>
<p>The conditions of <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> are clearly satisfied.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-tukey" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tukey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-tukey-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tukey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Tukey Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="hinich" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="hinich"><span class="header-section-number">7.3</span> Hinich</h2>
<p>Hinich loss, from <span class="citation" data-cites="hinich_talwar_75">Hinich and Talwar (<a href="#ref-hinich_talwar_75" role="doc-biblioref">1975</a>)</span>, is somewhat special because it is not differentiable at <span class="math inline">\(c\)</span>. For <span class="math inline">\(x\not= c\)</span> and <span class="math inline">\(x&gt;0\)</span> the function <span class="math inline">\(f'(x)/x\)</span> is discontinuous, but non-increasing on <span class="math inline">\([0,+\infty)\)</span>.</p>
<p><span class="math display">\[\begin{align}
f(x)&amp;=\begin{cases}
\frac12 x^2&amp;\text{ if } |x|\leq c,\\
\frac12 c^2&amp;\text{ otherwise}.
\end{cases}\\
g(x)&amp;=\begin{cases}
x&amp;\text{ if } |x|\leq c,\\
0&amp;\text{ otherwise}.
\end{cases}\\
h(x)&amp;=\begin{cases}
1&amp;\text{ if } |x|\leq c,\\
0&amp;\text{ otherwise}.
\end{cases}
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hinich" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hinich-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-hinich-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hinich-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Hinich Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="cauchy" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="cauchy"><span class="header-section-number">7.4</span> Cauchy</h2>
<p>Cauchy loss seems to have many names. <span class="citation" data-cites="black_anandan_96">Black and Anandan (<a href="#ref-black_anandan_96" role="doc-biblioref">1996</a>)</span> call it Lorentzian loss, and <span class="citation" data-cites="holland_welsch_77">Holland and Welsch (<a href="#ref-holland_welsch_77" role="doc-biblioref">1977</a>)</span> call it t-likelihood loss. It is related to the Cauchy distribution, which is Student’s t distribution with one degree of freedom.</p>
<p><span class="citation" data-cites="mlotshwa_vandeventer_bosman_23">Mlotshwa, Van Deventer, and Sergeevna Bosman (<a href="#ref-mlotshwa_vandeventer_bosman_23" role="doc-biblioref">2023</a>)</span></p>
<p><span class="math display">\[\begin{align}
f(x)&amp;=\frac12c^2\log(1+\{\frac{x}{c}\}^2),\\
f'(x)&amp;=x\frac{1}{\{1+\frac{x}{c}\}^2},\\
\omega(x)&amp;=\frac{1}{\{1+\frac{x}{c}\}^2}
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cauchy" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cauchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-cauchy-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cauchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Cauchy Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="welsch" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="welsch"><span class="header-section-number">7.5</span> Welsch</h2>
<p><span class="citation" data-cites="dennis_welsch_78">Dennis Jr and Welsch (<a href="#ref-dennis_welsch_78" role="doc-biblioref">1978</a>)</span></p>
<p>Leclerc loss</p>
<p><span class="math display">\[\begin{align}
f(x)&amp;=\frac12c^2[1-\exp(-\{\frac{x}{c}\}^2)],\\
f'(x)&amp;=x\exp(-\{\frac{x}{c}\}^2,\\
\omega(x)&amp;=\exp(-\{\frac{x}{c}\}^2),
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-welsch" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-welsch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-welsch-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-welsch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Welsch Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="logistic"><span class="header-section-number">7.6</span> Logistic</h2>
<p><span class="math display">\[\begin{align}
f(x)&amp;=c^2[\log(\cosh(x/c))],\\
f'(x)&amp;=c\tanh(x/c),\\
\omega(x)&amp;=(x/c)^{-1}\tanh(x/c).
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-logistic" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-logistic-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Logistic Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="fair" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="fair"><span class="header-section-number">7.7</span> Fair</h2>
<p><span class="math display">\[\begin{align}
f(x)&amp;=c^2\{|x|/c-\log(1+|x|/c)\},\\
f'(x)&amp;=x(1+(|x|/c))^{-1},\\
\omega(x)&amp;=(1+(|x|/c))^{-1}.
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fair" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="av_files/figure-html/fig-fair-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Fair Loss
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="examples" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Examples</h1>
<section id="gruijter" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="gruijter"><span class="header-section-number">8.1</span> Gruijter</h2>
<p>The example we use are dissimilarities between nine Dutch political parties, collected by <span class="citation" data-cites="degruijter_67">De Gruijter (<a href="#ref-degruijter_67" role="doc-biblioref">1967</a>)</span>. They are averages over a politically heterogenous group of 100 introductory psychology students, and consequently they regress to the mean. Any reasonable MDS analysis of these data would at least allow for an additive constant.</p>
<p>Some background on Dutch politics around that time may be useful.</p>
<ul>
<li>CPN - Communists.</li>
<li>PSP - Pacifists, left-wing.</li>
<li>PvdA - Labour, Democratic Socialists.</li>
<li>D’66 - Pragmatists, nether left-wing nor right-wing, brand new in 1967.</li>
<li>KVP - Christian Democrats, catholic.</li>
<li>ARP - Christian Democrats, protestant.</li>
<li>CHU - Christian Democrats, protestant.</li>
<li>VVD - Liberals, European flavour, conservative.</li>
<li>BP - Farmers, protest party, right-wing.</li>
</ul>
<p>The dissimilarities are in the table below.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">KVP</th>
<th style="text-align: right;">PvdA</th>
<th style="text-align: right;">VVD</th>
<th style="text-align: right;">ARP</th>
<th style="text-align: right;">CHU</th>
<th style="text-align: right;">CPN</th>
<th style="text-align: right;">PSP</th>
<th style="text-align: right;">BP</th>
<th style="text-align: right;">D66</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">KVP</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">5.63</td>
<td style="text-align: right;">5.27</td>
<td style="text-align: right;">4.60</td>
<td style="text-align: right;">4.80</td>
<td style="text-align: right;">7.54</td>
<td style="text-align: right;">6.73</td>
<td style="text-align: right;">7.18</td>
<td style="text-align: right;">6.17</td>
</tr>
<tr class="even">
<td style="text-align: left;">PvdA</td>
<td style="text-align: right;">5.63</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">6.72</td>
<td style="text-align: right;">5.64</td>
<td style="text-align: right;">6.22</td>
<td style="text-align: right;">5.12</td>
<td style="text-align: right;">4.59</td>
<td style="text-align: right;">7.22</td>
<td style="text-align: right;">5.47</td>
</tr>
<tr class="odd">
<td style="text-align: left;">VVD</td>
<td style="text-align: right;">5.27</td>
<td style="text-align: right;">6.72</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">5.46</td>
<td style="text-align: right;">4.97</td>
<td style="text-align: right;">8.13</td>
<td style="text-align: right;">7.55</td>
<td style="text-align: right;">6.90</td>
<td style="text-align: right;">4.67</td>
</tr>
<tr class="even">
<td style="text-align: left;">ARP</td>
<td style="text-align: right;">4.60</td>
<td style="text-align: right;">5.64</td>
<td style="text-align: right;">5.46</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">3.20</td>
<td style="text-align: right;">7.84</td>
<td style="text-align: right;">6.73</td>
<td style="text-align: right;">7.28</td>
<td style="text-align: right;">6.13</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CHU</td>
<td style="text-align: right;">4.80</td>
<td style="text-align: right;">6.22</td>
<td style="text-align: right;">4.97</td>
<td style="text-align: right;">3.20</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">7.80</td>
<td style="text-align: right;">7.08</td>
<td style="text-align: right;">6.96</td>
<td style="text-align: right;">6.04</td>
</tr>
<tr class="even">
<td style="text-align: left;">CPN</td>
<td style="text-align: right;">7.54</td>
<td style="text-align: right;">5.12</td>
<td style="text-align: right;">8.13</td>
<td style="text-align: right;">7.84</td>
<td style="text-align: right;">7.80</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">4.08</td>
<td style="text-align: right;">6.34</td>
<td style="text-align: right;">7.42</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PSP</td>
<td style="text-align: right;">6.73</td>
<td style="text-align: right;">4.59</td>
<td style="text-align: right;">7.55</td>
<td style="text-align: right;">6.73</td>
<td style="text-align: right;">7.08</td>
<td style="text-align: right;">4.08</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">6.88</td>
<td style="text-align: right;">6.36</td>
</tr>
<tr class="even">
<td style="text-align: left;">BP</td>
<td style="text-align: right;">7.18</td>
<td style="text-align: right;">7.22</td>
<td style="text-align: right;">6.90</td>
<td style="text-align: right;">7.28</td>
<td style="text-align: right;">6.96</td>
<td style="text-align: right;">6.34</td>
<td style="text-align: right;">6.88</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">7.36</td>
</tr>
<tr class="odd">
<td style="text-align: left;">D66</td>
<td style="text-align: right;">6.17</td>
<td style="text-align: right;">5.47</td>
<td style="text-align: right;">4.67</td>
<td style="text-align: right;">6.13</td>
<td style="text-align: right;">6.04</td>
<td style="text-align: right;">7.42</td>
<td style="text-align: right;">6.36</td>
<td style="text-align: right;">7.36</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The reason we have chosen this example is partly because CPN and BP are outliers, and we can expect the robust loss functions to handle outlying dissimilarities differently from the bulk of the data.</p>
<p>Unless otherwise indicated we run smacofRobust() with a maximum of 10,000 iterations, and we decide that we have convergence if the difference between consecutive stress values is less than 10^{-15}. We perform one single smacof iteration between the updates of the weights. For each analysis we show the configuration plot, the Shepard plot, and a histogram of the absolute values of the residuals. In the Shepard plot points corresponding to the eight CPN-dissimilarities are labeled “C”, while BP-dissimilarities are “B”.</p>
<section id="least-squares" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="least-squares"><span class="header-section-number">8.1.1</span> Least Squares</h3>
<p>We start with a least squares analysis, actually with Huber loss with <span class="math inline">\(c=10\)</span>, which for these data is equivalent to least squares. The process converges in 859 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pxls-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Configuration Least Squares</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pdls-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Grujter Shepard Plot Least Squares</figcaption>
</figure>
</div>
</div>
</div>
<p>The Shepard plot clearly shows why an additive constant would be very beneficial in this case.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/phlsh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Histogram Least Squares Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="least-absolute-value" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="least-absolute-value"><span class="header-section-number">8.1.2</span> Least Absolute Value</h3>
<p>For our LAV smacof we use engine smacofCharbonnier with <span class="math inline">\(c=.001\)</span>. We have convergence in 637 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pxav-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Configuration Least Absolute Value</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pdav-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Shepard Plot Least Absolute Value</figcaption>
</figure>
</div>
</div>
</div>
<p>In the Shepard plot we see that there are a number of dissimilarities which are fitted exactly. If we count them there are about 15-20. Note that configurations in two dimensions have <span class="math inline">\((n-1)+(n-2)=2n-3\)</span> degrees of freedom, which is 15 in this case. Thus if we take the 15 dissimilarities which are fitted exactly, give them weight one, give all other 21 dissimilarities weight zero, and do a regular non-robust smacof analysis using these weights, then we will have perfect fit in two dimensions, and the solution will be the LAV solution. All this is easier said than done, because it presumes that we use Charbonnier loss with <span class="math inline">\(c=0\)</span> and that we are able to decide which residuals are exactly equal to zero. The LAV analysis also suggests the possibility of a huge number of local minima, because there are so many ways to pick 15 out of 36 dissimilarities.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/phavh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Histogram Least Absolute Value Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="huber-1" class="level3" data-number="8.1.3">
<h3 data-number="8.1.3" class="anchored" data-anchor-id="huber-1"><span class="header-section-number">8.1.3</span> Huber</h3>
<p>smacofHuber with <span class="math inline">\(c=1\)</span> converges in 165 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pxhme-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Configuration Huber c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pdhme-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Shepard Plot Huber c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/phmeh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Histogram Huber Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="tukey-1" class="level3" data-number="8.1.4">
<h3 data-number="8.1.4" class="anchored" data-anchor-id="tukey-1"><span class="header-section-number">8.1.4</span> Tukey</h3>
<p>smacofTukey with <span class="math inline">\(c=2\)</span> converges in 180 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pxtmed-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Configuration Tukey c = 2</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pdtmed-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Shepard Plot Tukey c = 2</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/phtuh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Gruijter Histogram Tukey Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="rothkopf" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="rothkopf"><span class="header-section-number">8.2</span> Rothkopf</h2>
<p>Our second example are the Rothkopf Morse data (<span class="citation" data-cites="rothkopf_57">Rothkopf (<a href="#ref-rothkopf_57" role="doc-biblioref">1957</a>)</span>), which have a better fit and have fewer outliers than the Gruijter data. We used the asymetric confusion matrix from the smacof package (<span class="citation" data-cites="deleeuw_mair_A_09c">De Leeuw and Mair (<a href="#ref-deleeuw_mair_A_09c" role="doc-biblioref">2009</a>)</span>) and defined dissimilarities by the Shepard-Luce formula <span class="math display">\[
\delta_{ij}=-\log\frac{p_{ij}p_{ji}}{p_{ii}p_{jj}}.
\]</span></p>
<section id="least-squares-1" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="least-squares-1"><span class="header-section-number">8.2.1</span> Least Squares</h3>
<p>For least squares we use the smacofHuber engine with <span class="math inline">\(c=25\)</span>, well outside the range of the residuals. We have convergence in 213 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpxls-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Configuration Least Squares</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mkpdls-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Shepard Plot Least Squares</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mphlsh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Histogram Least Squares Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="least-absolute-value-1" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="least-absolute-value-1"><span class="header-section-number">8.2.2</span> Least Absolute Value</h3>
<p>For least absolute value we use Chardonnier loss with <span class="math inline">\(c=.001\)</span>. We have convergence in 2291 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpxav-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Configuration Least Absolute Value</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpdav-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Shepard Plot Least Absolute Value</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mphlavh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Histogram Least Absolute Value Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="huber-2" class="level3" data-number="8.2.3">
<h3 data-number="8.2.3" class="anchored" data-anchor-id="huber-2"><span class="header-section-number">8.2.3</span> Huber</h3>
<p>smacofHuber with <span class="math inline">\(c=1\)</span> converges in 680 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpxhme-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Configuration Huber c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpdhme-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Shepard Plot Huber c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mphmeh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Histogram Huber Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="tukey-2" class="level3" data-number="8.2.4">
<h3 data-number="8.2.4" class="anchored" data-anchor-id="tukey-2"><span class="header-section-number">8.2.4</span> Tukey</h3>
<p>Tukey with <span class="math inline">\(c=1\)</span> converges in 812 iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mpxtu-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Configuration Tukey c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/pdttu-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Shepard Plot Tukey c = 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="av_files/figure-html/mphtuh-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Rothkopf Histogram Tukey Residuals</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-literature" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Literature</h1>
<p>The literature on results like <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> and <a href="#thm-sqrt" class="quarto-xref">Theorem&nbsp;8</a> is difficult to review. There are various reasons for that. Relevant results have been published in robust statistics, computational statistics, optimization, location analysis, image restoration, sparse recovery. As is often the case, there are not many references between fields, almost everything is within. Even the names of the loss functions differ between fields. Much of it is hard to find in conference proceedings. Also, in most cases, the authors have specific applications in mind, which they then embed in a likelihood, Bayesian, linear regression, logistic regression, facility location, or EM framework and language.</p>
<p><span class="citation" data-cites="deleeuw_lange_A_09">De Leeuw and Lange (<a href="#ref-deleeuw_lange_A_09" role="doc-biblioref">2009</a>)</span> give some references to previous work on results like <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a>, notably <span class="citation" data-cites="groenen_giaquinto_kiers_03">Groenen, Giaquinto, and Kiers (<a href="#ref-groenen_giaquinto_kiers_03" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="jaakkola_jordan_00">Jaakkola and Jordan (<a href="#ref-jaakkola_jordan_00" role="doc-biblioref">2000</a>)</span>, and <span class="citation" data-cites="hunter_li_05">Hunter and Li (<a href="#ref-hunter_li_05" role="doc-biblioref">2005</a>)</span>. In these earlier papers we do not find <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> in its full generality. In <span class="citation" data-cites="groenen_giaquinto_kiers_03">Groenen, Giaquinto, and Kiers (<a href="#ref-groenen_giaquinto_kiers_03" role="doc-biblioref">2003</a>)</span> majorization of the log logistic function is considered. Besides requiring equality of the function and the majorizing quadratic at the support point <span class="math inline">\(y\)</span> they also require equality at <span class="math inline">\(-y\)</span> and then check that the resulting quadratic is indeed a majorizer. In <span class="citation" data-cites="jaakkola_jordan_00">Jaakkola and Jordan (<a href="#ref-jaakkola_jordan_00" role="doc-biblioref">2000</a>)</span> also consider a symmetrized version of the log logistic function. They note that the resulting function is a convex funcion of <span class="math inline">\(x^2\)</span>, and use a linear majorizer at <span class="math inline">\(x^2\)</span> to obtain a quadratic majorization. <span class="citation" data-cites="hunter_li_05">Hunter and Li (<a href="#ref-hunter_li_05" role="doc-biblioref">2005</a>)</span> come closest to <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a>. In their proposition 3.1 they approximate the general penalty function they use for varable selection at <span class="math inline">\(y\)</span> by a quadratic with coefficient <span class="math inline">\(f'(y)/2y\)</span>, and then show that it provides a quadratic majorization. In neither of the three papers there is a notion of sharp quadratic majorization.</p>
<p>I will discuss some of the literature under the headings “robust statistics”, “location analysis”, and “sparse recovery”. Since I most definitely am not an expert in either of these three fields the literature reviews will be biased and incomplete. A final section, where I am somewhat more sure-footed, is “multivariate analysis”.</p>
<section id="robust-statistics" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="robust-statistics"><span class="header-section-number">9.1</span> Robust Statistics</h2>
<p>In robust statistics it has been known for a long time that iterative reweighted least squares (IRLS) with weights <span class="math inline">\(f'(x)/x\)</span> gives a quadratic majorization algorithm. This result, and the corresponding IRLS algorithm, is often attributed to <span class="citation" data-cites="beaton_tukey_74">Beaton and Tukey (<a href="#ref-beaton_tukey_74" role="doc-biblioref">1974</a>)</span>.</p>
</section>
<section id="location-analysis" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="location-analysis"><span class="header-section-number">9.2</span> Location Analysis</h2>
<p>In location analysis the first majorization/IRLS method is generally attributed to a 16-year old Hungarian mathematics prodigy (<span class="citation" data-cites="weiszfeld_37">Weiszfeld (<a href="#ref-weiszfeld_37" role="doc-biblioref">1937</a>)</span>). His algorithm avant-la-lettre was intended to find the minimum of a function of the form <span id="eq-web"><span class="math display">\[
\sigma(x)=\sum_k w_kd_k(x),
\tag{38}\]</span></span> where <span class="math inline">\(d_k(x)=\|x-y_k\|\)</span>, over <span class="math inline">\(x\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>. The <span class="math inline">\(y_k\)</span> are known locations, called <em>anchors</em> in the literature, and the norm is Euclidean. Actually <span class="citation" data-cites="weiszfeld_37">Weiszfeld (<a href="#ref-weiszfeld_37" role="doc-biblioref">1937</a>)</span> did not use weights <span class="math inline">\(w_k\)</span> and worked in three-dimensional space. There is an English translation of Weiszfeld’s paper, with bibliography and comments, in <span class="citation" data-cites="weiszfeld_plastria_09">Weiszfeld and Plastria (<a href="#ref-weiszfeld_plastria_09" role="doc-biblioref">2009</a>)</span>.</p>
<p>The problem of minimizing <a href="#eq-web" class="quarto-xref">Equation&nbsp;38</a> is known under various names, usually consisting of one of the seven different non-empty selections from the triple (Torricelli, Fermat, Weber). The history of the problem is discussed, for example, in <span class="citation" data-cites="plastria_11">Plastria (<a href="#ref-plastria_11" role="doc-biblioref">2011</a>)</span>.</p>
<p>Weiszfeld first acknowledges that <span class="citation" data-cites="sturm_84">Sturm (<a href="#ref-sturm_84" role="doc-biblioref">1884</a>)</span> has already established the existence and uniqueness of the minimum point. He then proceeds to give three new proofs. We are interested in the first one, described in his first theorem. It defines the iterative sequence <span id="eq-wup"><span class="math display">\[
x^{(\nu+1)}=\frac{\sum\omega_k(x^{(\nu)})y_k}{\sum\omega_k(x^{(\nu)})}
\tag{39}\]</span></span> with weights <span id="eq-ww"><span class="math display">\[
\omega_\nu(x)=\frac{1}{d_k(x^{(\nu)})}.
\tag{40}\]</span></span> The proof then consists of showing that the sequence converges to the unique minimum point of <a href="#eq-web" class="quarto-xref">Equation&nbsp;38</a>. It would have been nice if we were told where <a href="#eq-wup" class="quarto-xref">Equation&nbsp;39</a> came from, but it is simply taken as the starting point.</p>
<p>We can guess what suggested this particular sequence. Observe first that the correponding problem with <span class="math inline">\(d_k(x)\)</span> replaced by <span class="math inline">\(d_k^2(x)\)</span> is easy to solve. The solution is simply the weighted mean of the <span class="math inline">\(y_k\)</span>. This suggests the rewrite <span id="eq-rew"><span class="math display">\[
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k(x)}d_k^2(x),
\tag{41}\]</span></span> which in turn suggests <a href="#eq-wup" class="quarto-xref">Equation&nbsp;39</a>. Alternatively, differentiate the loss in <a href="#eq-web" class="quarto-xref">Equation&nbsp;38</a> and set the partials equal to zero. This gives <span id="eq-wstatio1"><span class="math display">\[
\sum_k w_k\frac{1}{d_k(x)}(x-y_k)=0,
\tag{42}\]</span></span> or <span id="eq-wstatio2"><span class="math display">\[
x=\frac{\sum\omega_k(x)y_k}{\sum\omega_k(x)}.
\tag{43}\]</span></span> <span class="citation" data-cites="sturm_84">Sturm (<a href="#ref-sturm_84" role="doc-biblioref">1884</a>)</span> mentions that he derived his existence and uniqueness theorem without using differentiation, but he mentions a paper by Lorenz Lindelöf from 1866 which derives his necessary conditions using differentiation (I have not been able to find a copy of that paper). The discussion is a clear example of the nineteenth century tension between using synthetic (geometric) methods or analytic (calculus) methods.</p>
<p>Even more synthetic were the methods in a paper by Lamé and Clapeyron of 1829. (I have not been able to find a copy of that paper either). There is a partial translation in <span class="citation" data-cites="franksen_grattan-guinness_89">Franksen and Grattan-Guinness (<a href="#ref-franksen_grattan-guinness_89" role="doc-biblioref">1989</a>)</span>. Lamé and Clapeyron suggest solving their “moindre distances” problems, which generalize the single facility location problem in various ways, by ingeneous systems of pulleys. There is a translation of the general principles section of the Lamé and Clapeyron paper in <span class="citation" data-cites="franksen_grattan-guinness_89">Franksen and Grattan-Guinness (<a href="#ref-franksen_grattan-guinness_89" role="doc-biblioref">1989</a>)</span>. From that translation we read in General Principle 9 their defense of the mechanical method they propose.</p>
<blockquote class="blockquote">
<p>But if one considers that the proposed problem is totally insoluble, in its entire generality, by the current means of analysis and geometry; that it is only in very special and very simple cases that one may obtain a complete graphical solution; that finally in the applications the data themselves are only approximate; one will be forced to adrmt that in the state of imperfection in which algebraic analysis is still found today, the manner of solution in question here is the only one which obtains for the proposed problem.</p>
</blockquote>
<p>In General Principle 21 they suggest an iterative method of “trial and error” to solve the weighted location problem, where the weights in each iteration are adjusted by multiplying them with the inverse of the distances in the previous iteration. Thus they propose the Weiszfeld algorithm, albeit in a version using pulleys.</p>
<p>Over the years the location problem has been generalized in numerous directions, to multiple locations, to using different norms, to unknown anchors, to nonlinear manifolds, and to obnoxious anchors you want to be far from. A good recent overview is <span class="citation" data-cites="beck_sabach_15">Beck and Sabach (<a href="#ref-beck_sabach_15" role="doc-biblioref">2015</a>)</span>. A paper close in spirit to our paper is <span class="citation" data-cites="aftab_hartley_trumpf_15">Aftab, Hartley, and Trumpf (<a href="#ref-aftab_hartley_trumpf_15" role="doc-biblioref">2015</a>)</span>, which has generalizations to <span class="math inline">\(\ell_q\)</span> norms and to Riemannian manifolds of non-negative curvature.</p>
<p>There is a huge literature on the convergence of the Weiszfeld algorithm. As in our <a href="#sec-amgm" class="quarto-xref">Section&nbsp;3.1</a> we can simply use AM/GM inequality. Thus <span id="eq-webamgm"><span class="math display">\[
\|x-y_\nu\|\leq\frac12\frac{1}{\|x^{(\nu)}-y_\nu\|}(\|x-y_\nu\|^2+\|x^{(\nu)}-y_\nu\|^2),
\tag{44}\]</span></span> which immediately gives <a href="#eq-wup" class="quarto-xref">Equation&nbsp;39</a>. Convergence follows from the general majorization of MM theory.</p>
<p>Unlike robust smacof the Toricelli-Fermat-Weber problem is convex, and consequently has no problems with non-global local minima. A most elegant proof of convergence using the tools of modern convex analysis is in <span class="citation" data-cites="mordukhovich_nam_19">Mordukhovich and Nam (<a href="#ref-mordukhovich_nam_19" role="doc-biblioref">2019</a>)</span>. Older proofs sometimes have difficulty dealing with cases in which the iterates coincide with one of the anchors or in which the solution is actually one of the anchors. This creates problems similar to the problems in our <a href="#sec-zero" class="quarto-xref">Section&nbsp;3.2</a>, but in this simple case the problem is can be completely resolved using convexity and has no serious algorithmic consequences.</p>
<p>In a straightforward generalization of the Toricelli-Fermat-Weber problem , which is particularly relevant for the developments in our paper, <span class="citation" data-cites="katz_69">Katz (<a href="#ref-katz_69" role="doc-biblioref">1969</a>)</span> proposes to minimize <span id="eq-katzp"><span class="math display">\[
\sigma(x)=\sum_{k=1}^mw_kd_k^q(x),
\tag{45}\]</span></span> and even <span id="eq-katzf"><span class="math display">\[
\sigma(x)=\sum_{k=1}^mw_kf_k(d_k(x))
\tag{46}\]</span></span> with Euclidean distances and <span class="math inline">\(f_k\)</span> functions defined on the non-negative reals. Note that if <span class="math inline">\(f\)</span> is convex ands increasing then <span class="math inline">\(\sigma\)</span> of <a href="#eq-katzf" class="quarto-xref">Equation&nbsp;46</a> is convex. If <span class="math inline">\(q\geq 1\)</span> then <span class="math inline">\(\sigma\)</span> of <a href="#eq-katzp" class="quarto-xref">Equation&nbsp;45</a> is convex.</p>
<p>The algorithm Katz suggests for minimizinf <span class="math inline">\(\sigma\)</span> of <a href="#eq-katzp" class="quarto-xref">Equation&nbsp;45</a> generalizes the decompostion in <a href="#eq-rew" class="quarto-xref">Equation&nbsp;41</a> to <span id="eq-katzdec"><span class="math display">\[
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k^{2-q}(x)}d_k^2(x),
\tag{47}\]</span></span> which leads to <span id="eq-katzupdp"><span class="math display">\[
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)}}}.
\tag{48}\]</span></span> For <a href="#eq-katzf" class="quarto-xref">Equation&nbsp;46</a>, analogous with <a href="#eq-wstatio1" class="quarto-xref">Equation&nbsp;42</a>, we set the derivative of <a href="#eq-katzf" class="quarto-xref">Equation&nbsp;46</a> equal to zero. Thus we solve <span id="eq-katzder"><span class="math display">\[
\sum_{k=1}^mw_k\frac{f_k'(d_k(x))}{d_k(x)}(x-y_k)=0,
\tag{49}\]</span></span> and the iteration becomes <span id="eq-katzupdf"><span class="math display">\[
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}}.
\tag{50}\]</span></span> The conditions in <span class="citation" data-cites="katz_69">Katz (<a href="#ref-katz_69" role="doc-biblioref">1969</a>)</span> on <span class="math inline">\(f\)</span> needed for the convergence proof, and also the proof itself, are rather complicated. We can simply use the conditions of <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> that <span class="math inline">\(f'(x)/x\)</span> is non-increasing on the non-negatives reals to construct a quadratic majorization algorithm in which we minimize <span id="eq-katzmajor"><span class="math display">\[
\sum_{k=1}^m\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)})}{d_k(x^{(\nu)})}d_k^2(x)
\tag{51}\]</span></span> to find <span class="math inline">\(x^{k+1}\)</span>. This gives directly the update <a href="#eq-katzupdp" class="quarto-xref">Equation&nbsp;48</a>.</p>
</section>
<section id="sparse-recovery" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sparse-recovery"><span class="header-section-number">9.3</span> Sparse Recovery</h2>
<p>This is a field which is difficult to delineate. A somewhat ad-hoc definition is recovering complete information from incomplete information, often in the context of specific engineering problems. There is overlap with signal detection, image analysis, matrix completion, … But “sparse recovery” scientific activities “sparse recovery” could be extended far beyond these boundaries. Since classical statistics infers properties of the population from those of a sample it is a form of sparse recovery. Since science infers properties of the real world from outcomes of experiments it is sparse recovery too.</p>
</section>
<section id="multivariate-analysis" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="multivariate-analysis"><span class="header-section-number">9.4</span> Multivariate Analysis</h2>
<p>The smacof majorization method for multidimensional scaling was first presented at the <em>US-Japan Seminar on Theory, Methods and Applications of Multidimensional Scaling and Related Techniques</em> at UCSD in La Jolla, August 1975. Shortly after that I read the basic EM paper by <span class="citation" data-cites="dempster_laird_rubin_77">Dempster, Laird, and Rubin (<a href="#ref-dempster_laird_rubin_77" role="doc-biblioref">1977</a>)</span>, and shortly after that I realized that smacof and EM were both special cases of a general minimization strategy, which I called majorization at the time. In June 1978 both Nan Laird and I attended the <em>Fifth International Symposium on Multivariate Analysis</em> at the University of Pittsburgh. I remember mentioning majorization, excitedly, to Nan on the conference bus.</p>
<p>The smacof majorization method was fully discussed in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>, <span class="citation" data-cites="deleeuw_heiser_C_77">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_77" role="doc-biblioref">1977</a>)</span>, and <span class="citation" data-cites="deleeuw_heiser_C_80">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_80" role="doc-biblioref">1980</a>)</span>. The familiar picture illustrating two steps of the general majorization algorithm first appears in <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="#ref-deleeuw_A_88b" role="doc-biblioref">1988a</a>)</span>. But unlike EM, which took off as a rocket in 1977, the general idea of majorization remained unpublished, until <span class="citation" data-cites="deleeuw_C_94c">De Leeuw (<a href="#ref-deleeuw_C_94c" role="doc-biblioref">1994</a>)</span> and <span class="citation" data-cites="heiser_95">Heiser (<a href="#ref-heiser_95" role="doc-biblioref">1995</a>)</span>. Majorization was used regularly in the Gifi project. The book <span class="citation" data-cites="gifi_B_90">Gifi (<a href="#ref-gifi_B_90" role="doc-biblioref">1990</a>)</span>, which is a version of 1981 lecture notes, mentions majorization only once, but since then a stream of papers and dissertations from the Data Theory department in Leiden using majorization appeared. <span class="citation" data-cites="heiser_95">Heiser (<a href="#ref-heiser_95" role="doc-biblioref">1995</a>)</span> mentions most of them. In <span class="citation" data-cites="deleeuw_C_88b">De Leeuw (<a href="#ref-deleeuw_C_88b" role="doc-biblioref">1988b</a>)</span> another large majorization subfield, the <em>aspect approach</em> to multivariate analysis, was developed. In section 7 of that paper the general majorization/minorization approach to optimization is outlined, possibly for the first time in print.</p>
<p>Robust versions of low rank matrix approximation, a.k.a. principal component analysis, were first considered by <span class="citation" data-cites="gabriel_odoroff_84">Gabriel and Odoroff (<a href="#ref-gabriel_odoroff_84" role="doc-biblioref">1984</a>)</span>. They start by discussing the alternating least squares algorithm for least squares weighted matrix approximation of <span class="citation" data-cites="gabriel_zamir_79">Gabriel and Zamir (<a href="#ref-gabriel_zamir_79" role="doc-biblioref">1979</a>)</span>. The alternating is to compute new row scores for currently fixed column scores by linear regression, and then computing new column scores corresponding with the new row scores, again by linear regression. <span class="citation" data-cites="gabriel_odoroff_84">Gabriel and Odoroff (<a href="#ref-gabriel_odoroff_84" role="doc-biblioref">1984</a>)</span> suggest to replace the linear least squares weighted averages in each of the two stages by medians or trimmed means to get a robust PCA. There is no sign of a convergence proof, but there is the suggestion to use alternating least absolute value methods to minimize the sum of absolute residuals of the matrix approximation. This suggestion was taken up by <span class="citation" data-cites="verboon_heiser_94">Verboon and Heiser (<a href="#ref-verboon_heiser_94" role="doc-biblioref">1994</a>)</span> using the majorization approach and the Huber and Tukey robust loss functions. Their robust PCA method is very similar to our robust MDS method, but the presentation of their method has some magical elements. The Huber and Tukey majorization functions are presented without any discussion where they came from, and it is then verified that they are indeed majorizations. There is clearly nothing wrong with this, but using our <a href="#thm-wght" class="quarto-xref">Theorem&nbsp;7</a> gives a more general and more direct approach.</p>
<p><span class="citation" data-cites="heiser_86">Heiser (<a href="#ref-heiser_86" role="doc-biblioref">1986</a>)</span> was the first to connect the Weiszfeld problem with correspondence analysis and multidimensional scaling, emphasizing the majorization aspects. As we have seen in <span class="citation" data-cites="heiser_87">Heiser (<a href="#ref-heiser_87" role="doc-biblioref">1987</a>)</span> and <span class="citation" data-cites="heiser_88">Heiser (<a href="#ref-heiser_88" role="doc-biblioref">1988</a>)</span> he constructed majorization algorithms for multidimensional scaling and correspondence analysis.</p>
<p>The IRLS approach to robustifying multivariate matrix approximation techniques could easily lead to a large and varied number of publications. There are some excellent examples making their way through the usual publication channels. I will just give two recent examples, with good bibliographies. They are Huber Principal Component Analysis (<span class="citation" data-cites="he_li_liu_zhou_23">He et al. (<a href="#ref-he_li_liu_zhou_23" role="doc-biblioref">2023</a>)</span>) and Cauchy Factor Analysis (<span class="citation" data-cites="li_24">Li (<a href="#ref-li_24" role="doc-biblioref">2024</a>)</span>).</p>
</section>
</section>
<section id="discussion" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Discussion</h1>
<section id="bounding-the-second-derivative" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="bounding-the-second-derivative"><span class="header-section-number">10.1</span> Bounding the Second Derivative</h2>
<p>In some cases our basic theorems may not apply, but there may be an alternative way to majorize loss. In fact, this is classic quadratic bounding as in <span class="citation" data-cites="vosz_eckhardt_80">Vosz and Eckhardt (<a href="#ref-vosz_eckhardt_80" role="doc-biblioref">1980</a>)</span> or <span class="citation" data-cites="boehning_lindsay_88">Böhning and Lindsay (<a href="#ref-boehning_lindsay_88" role="doc-biblioref">1988</a>)</span>. As before, we want to minimize <span class="math inline">\(\sum \omega_k\ f(\delta_k-d_k(X))\)</span>, but now we suppose that there is a <span class="math inline">\(K&gt;0\)</span> such that <span class="math inline">\(f''(x)\leq K\)</span>. We then have the majorization</p>
<p><span id="eq-qbound"><span class="math display">\[
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
\tag{52}\]</span></span> and in iteration <span class="math inline">\(k\)</span> we minimize, or at least decrease, <span id="eq-qiter"><span class="math display">\[
\sum \omega_k\left[d_k(X)-\{d_k(X^{(\nu)})-K^{-1}f'(\delta_k-d_k(X^{(\nu)}))\}\right]^2
\tag{53}\]</span></span> Note that in this algorithm the weights do not change. Instead of fitting a fixed target with moving weights, we fit a moving target with fixed weights.</p>
<p>We can apply bounding the second derivative, for example, to Charbonnier loss, using the inequality <span id="eq-chark"><span class="math display">\[
f_c''(x)=(x^2 + c^2)^{-\frac12}-x^2(x^2 + c^2)^{-\frac32}\leq(x^2 + c^2)^{-\frac12}\leq c^{-1},
\tag{54}\]</span></span> Of course this method requires that the second derivative exists at <span class="math inline">\(x\)</span>. Although I have not done any comparisons it will probably require more iterations and take longer than the method in <a href="#sec-charb" class="quarto-xref">Section&nbsp;5.1</a>.</p>
<p>The paper by <span class="citation" data-cites="vosz_eckhardt_80">Vosz and Eckhardt (<a href="#ref-vosz_eckhardt_80" role="doc-biblioref">1980</a>)</span> deserves some special mention here.</p>
<p><span class="math display">\[
\mathcal{D}^2\sigma(x)=\sum\frac{1}{d_i(x)}\left\{I-\frac{(x-y_i)(x-y_i)'}{d_i^2(x)}\right\}
\]</span></p>
</section>
<section id="fixed-weights" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="fixed-weights"><span class="header-section-number">10.2</span> Fixed Weights</h2>
<p>One could also consider using the fixed weights in regular non-robust smacof to achieve some form of robustness. Redefine stress as <span id="eq-wstress"><span class="math display">\[
\sigma(X):=\sum_k \omega_kf(\delta_k)(\delta_k-d_k(X))^2
\tag{55}\]</span></span> For example, we can choose a negative power for <span class="math inline">\(f\)</span>, so that it downweights the large dissimilarities. If the dissimilarities is large, then it should have less influence on the fit, and thus on the solution <span class="math inline">\(X\)</span>. This type of fixed power-weighting is used in various places (<span class="citation" data-cites="deleeuw_heiser_C_80">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_80" role="doc-biblioref">1980</a>)</span>, <span class="citation" data-cites="groenen_vandevelden_16">Groenen and Van de Velden (<a href="#ref-groenen_vandevelden_16" role="doc-biblioref">2016</a>)</span>) to approximate loss functions such the one with logarithmic residuals in <span class="citation" data-cites="ramsay_77">Ramsay (<a href="#ref-ramsay_77" role="doc-biblioref">1977</a>)</span>.</p>
<p>But we have to keep in mind that downweighting large dissimilarities is not the same thing as downweighting large residuals. The residuals depend on <span class="math inline">\(X\)</span>, and it is perfectly possible that some small dissimilarities have large residuals. On the other hand emphasizing small dissimilarities in the loss function means that we want small dissimilarities to be fitted relatively well, which means that on average we want small dissimilarities to have small residuals. The Shepard plot will tend to fan out at the high end.</p>
<p>Despite these reservations, it will be useful to study if and how fixed weights can be used to improve robustness of smacof. If only because fixed weights correspond with a simpler and presumably more efficient algorithm.</p>
</section>
<section id="residual-definition" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="residual-definition"><span class="header-section-number">10.3</span> Residual Definition</h2>
<p>In our examples and in our code we use the residuals <span class="math inline">\(\delta_k-d_k(X)\)</span> are arguments of our loss functions. From the statistical point of view we have to remember, however, that most of these loss functions were designed for the robust estimation of a location parameter or a linear regression function. The error distributions were explicitly or implicitly assumed to be symmetric around zero, and defined on the whole real line, which was reflected in the fact that loss functions were even and had infinite support. In MDS, however, distances and dissimilarities are non-negative and reasonable error functions are not symmetric. One could follow the example of <span class="citation" data-cites="ramsay_77">Ramsay (<a href="#ref-ramsay_77" role="doc-biblioref">1977</a>)</span> and measure residuals as <span class="math inline">\(\log\delta_{ij}-\log d_{ij}(X)\)</span>. This does not have any effect on the majorization of the loss functions, but it means that in the smacof step to find <span class="math inline">\(X^{(\nu+1)}\)</span> we have to minimize <span class="math display">\[
\sigma(X)=\sum \omega_k(X^{(\nu)})(\log\delta_{ij}-\log d_{ij}(X))^2,
\]</span> which is considerably more complicated (<span class="citation" data-cites="deleeuw_groenen_mair_E_16a">De Leeuw, Groenen, and Mair (<a href="#ref-deleeuw_groenen_mair_E_16a" role="doc-biblioref">2016</a>)</span>).</p>
</section>
<section id="robust-nonmetric-mds" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="robust-nonmetric-mds"><span class="header-section-number">10.4</span> Robust Nonmetric MDS</h2>
<p>Our discussion and our software is all about metric MDS. It seems easy to extend the discussion to non-linear and non-metric MDS by adding an alternating least squares step optimally scaling the dissimilarities. This would take place between two majorizations of the robust loss function, so one or more transformation and smacof steps can be taken between updating the weights. But this paradigm does not work for robust smacof.</p>
<p>Consider any hard redescender, such as Tukey or Hinich. At iteration <span class="math inline">\(\nu\)</span>, for current weights, first first improve the configuration, then compute the optimal transformation of the dissimilarities, and then compute new weights. This is a recipe for disaster. At some point we minimize <span class="math display">\[
\sigma(\hat d)=\sum \omega_k(X^{(\nu)})(\hat d_k-d_k(X^{(\nu+1)}))^2
\]</span>{#eq:zero} over the disparities <span class="math inline">\(\hat d\)</span>, which must be monotone with the dissimilarities. Because of the hard redescending some of the weights, for current absolute residuals larger than <span class="math inline">\(c\)</span>, will be zero. The monotone regression is done for the observations with non-zero weights, and the disparities corresponding with zero weights are only determined by the order they are required to have. Thus they can be freely chosen in an interval between two disparities obtained from the monotone regression. That interval can be large, in fact if one of the zero weights corresponds with the largest dissimilarity it can be infinite. What we choose in the interval will determine the new residual and thus the next set of weights.</p>
<p>In the unfortunate situation that the current absolute residuals are all larger than <span class="math inline">\(c\)</span>, even after choosing the optimal <span class="math inline">\(\hat d\)</span>, the next weights will all be zero and the algorithm stops with zero stress.</p>
</section>
<section id="practicalities" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="practicalities"><span class="header-section-number">10.5</span> Practicalities</h2>
<p>Recommending one particular loss function from the many we have discussed is not easy. In some cases, for example for Cauchy loss, one can justify the choice of a loss function by assuming a particular error distribution and using the maximum likelihood principle. But in general perhaps the best way to proceed for a given MDS problem is to take what we could call a <em>trajectory approach</em>. Choose one particular parametric family, for example the Huber one, and compute the robust smacof solution <span class="math inline">\(X(c)\)</span> for a number of increasing positive <span class="math inline">\(c\)</span> values. For small <span class="math inline">\(c\)</span> we start with a close approximation of the LAV solution, increasing <span class="math inline">\(c\)</span> will eventually take us to the LS solution. The starting point for computing each solution will be the solution for the previous <span class="math inline">\(c\)</span>. We can plot the trajectory of the points in the configurations <span class="math inline">\(X(c)\)</span>, and even make an animation. It seems that the Huber family is a good candidate for such a study, with the generalized Charbonnier a good second. If the main concern is to suppress the influence of outliers then trying some of the hard redescenders, such as the Tukey family, makes sense. Studying trajectories for some of robust loss functions is clearly interesting, but it is not something we can or will explore in this paper.</p>
</section>
</section>
<section id="code" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Code</h1>
<p>The function smacofRobust has a parameter “engine”, which can be equal to smacofCharbonnier, smacofGeneralizedCharbonnier, smacofBarron, smacofHuber, smacofTukey, smacofHinnich, smacofCauchy, smacofFair, smacofAndrews, smacofLogistic, smacofWelsch, or smacofGaussian. These thirteen small modules compute the respective loss function values and weights for the IRLS procedure. This makes it easy for interested parties to add additional robust loss functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>smacofRobust <span class="ot">&lt;-</span> <span class="cf">function</span>(delta,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">weights =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">diag</span>(<span class="fu">nrow</span>(delta)),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                         <span class="at">ndim =</span> <span class="dv">2</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">xold =</span> <span class="fu">smacofTorgerson</span>(delta, ndim),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">engine =</span> smacofAV,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">cons =</span> <span class="dv">0</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                         <span class="at">itmax =</span> <span class="dv">1000</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                         <span class="at">eps =</span> <span class="fl">1e-15</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">verbose =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  nobj <span class="ot">&lt;-</span> <span class="fu">nrow</span>(delta)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  wmax <span class="ot">&lt;-</span> <span class="fu">max</span>(weights)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  dold <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">dist</span>(xold))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> <span class="fu">engine</span>(nobj, weights, delta, dold, cons)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  rold <span class="ot">&lt;-</span> h<span class="sc">$</span>resi</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  wold <span class="ot">&lt;-</span> h<span class="sc">$</span>wght</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  sold <span class="ot">&lt;-</span> h<span class="sc">$</span>strs</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  itel <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">repeat</span> {</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    vmat <span class="ot">&lt;-</span> <span class="sc">-</span>wold</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">diag</span>(vmat) <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">rowSums</span>(vmat)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    vinv <span class="ot">&lt;-</span> <span class="fu">solve</span>(vmat <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">/</span> nobj)) <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">/</span> nobj)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    bmat <span class="ot">&lt;-</span> <span class="sc">-</span>wold <span class="sc">*</span> delta <span class="sc">/</span> (dold <span class="sc">+</span> <span class="fu">diag</span>(nobj))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">diag</span>(bmat) <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">rowSums</span>(bmat)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    xnew <span class="ot">&lt;-</span> vinv <span class="sc">%*%</span> (bmat <span class="sc">%*%</span> xold)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    dnew <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">dist</span>(xnew))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    h <span class="ot">&lt;-</span> <span class="fu">engine</span>(nobj, weights, delta, dnew, cons)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    rnew <span class="ot">&lt;-</span> h<span class="sc">$</span>resi</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    wnew <span class="ot">&lt;-</span> h<span class="sc">$</span>wght</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    snew <span class="ot">&lt;-</span> h<span class="sc">$</span>strs</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (verbose) {</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cat</span>(</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"itel "</span>,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="fu">formatC</span>(itel, <span class="at">width =</span> <span class="dv">4</span>, <span class="at">format =</span> <span class="st">"d"</span>),</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">"sold "</span>,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="fu">formatC</span>(sold, <span class="at">digits =</span> <span class="dv">10</span>, <span class="at">format =</span> <span class="st">"f"</span>),</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">"snew "</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="fu">formatC</span>(snew, <span class="at">digits =</span> <span class="dv">10</span>, <span class="at">format =</span> <span class="st">"f"</span>),</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"</span><span class="sc">\n</span><span class="st">"</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ((itel <span class="sc">==</span> itmax) <span class="sc">||</span> ((sold <span class="sc">-</span> snew) <span class="sc">&lt;</span> eps)) {</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    xold <span class="ot">&lt;-</span> xnew</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    dold <span class="ot">&lt;-</span> dnew</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    sold <span class="ot">&lt;-</span> snew</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    wold <span class="ot">&lt;-</span> wnew</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    rold <span class="ot">&lt;-</span> rnew</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    itel <span class="ot">&lt;-</span> itel <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> xnew,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="at">s =</span> snew,</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">d =</span> dnew,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="at">r =</span> rnew,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">itel =</span> itel</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>smacofTorgerson <span class="ot">&lt;-</span> <span class="cf">function</span>(delta, ndim) {</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>  dd <span class="ot">&lt;-</span> delta<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>  rd <span class="ot">&lt;-</span> <span class="fu">apply</span>(dd, <span class="dv">1</span>, mean)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>  md <span class="ot">&lt;-</span> <span class="fu">mean</span>(dd)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>  sd <span class="ot">&lt;-</span> <span class="sc">-</span>.<span class="dv">5</span> <span class="sc">*</span> (dd <span class="sc">-</span> <span class="fu">outer</span>(rd, rd, <span class="st">"+"</span>) <span class="sc">+</span> md)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  ed <span class="ot">&lt;-</span> <span class="fu">eigen</span>(sd)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(ed<span class="sc">$</span>vectors[, <span class="dv">1</span><span class="sc">:</span>ndim] <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="fu">sqrt</span>(ed<span class="sc">$</span>values[<span class="dv">1</span><span class="sc">:</span>ndim])))</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>smacofCharbonnier <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((delta <span class="sc">-</span> dmat)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> cons)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(resi <span class="sc">&lt;</span> <span class="fl">1e-10</span>, <span class="dv">2</span> <span class="sc">*</span> <span class="fu">max</span>(wmat), resi)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>  rmin <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(cons)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">/</span> (resi <span class="sc">+</span> <span class="fu">diag</span>(nobj))</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi) <span class="sc">-</span> rmin <span class="sc">*</span> <span class="fu">sum</span>(wmat)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>smacofGeneralizedCharbonnier <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> ((delta <span class="sc">-</span> dmat) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> cons[<span class="dv">1</span>]) <span class="sc">^</span> cons[<span class="dv">2</span>]</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>  rmin <span class="ot">&lt;-</span> cons[<span class="dv">1</span>] <span class="sc">^</span> cons[<span class="dv">2</span>]</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> ((delta <span class="sc">-</span> dmat) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> cons[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">diag</span>(nobj)) <span class="sc">^</span> (cons[<span class="dv">2</span>] <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi) <span class="sc">-</span> rmin <span class="sc">*</span> <span class="fu">sum</span>(wmat)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>smacofBarron <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>  f1 <span class="ot">&lt;-</span> <span class="fu">abs</span>(cons[<span class="dv">2</span>] <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">/</span> cons[<span class="dv">2</span>]</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>  f2 <span class="ot">&lt;-</span> ((((delta <span class="sc">-</span> dmat) <span class="sc">/</span> cons[<span class="dv">1</span>]) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="fu">abs</span>(cons[<span class="dv">2</span>] <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> f1 <span class="sc">*</span> (f2 <span class="sc">^</span> (cons[<span class="dv">2</span>] <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> f2 <span class="sc">^</span> (cons[<span class="dv">2</span>] <span class="sc">/</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>smacofGauss <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> difi <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">*</span> <span class="fu">pnorm</span>(difi <span class="sc">/</span> cons) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> cons <span class="sc">*</span> <span class="fu">dnorm</span>(difi <span class="sc">/</span> cons)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>  rmin <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> cons <span class="sc">*</span> <span class="fu">dnorm</span>(<span class="dv">0</span>)</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> (<span class="fu">pnorm</span>(difi <span class="sc">/</span> cons) <span class="sc">-</span> <span class="fl">0.5</span>) <span class="sc">/</span> (difi <span class="sc">+</span> <span class="fu">diag</span>(nobj))</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi) <span class="sc">-</span> rmin <span class="sc">*</span> <span class="fu">sum</span>(wmat)</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>smacofHuber <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons, (difi <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span>, cons <span class="sc">*</span> <span class="fu">abs</span>(difi) <span class="sc">-</span> ((cons <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span>))</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons,</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>                 wmat,</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>                 wmat <span class="sc">*</span> <span class="fu">sign</span>(difi <span class="sc">-</span> cons) <span class="sc">*</span> cons <span class="sc">/</span> (difi <span class="sc">+</span> <span class="fu">diag</span>(nobj)))</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>smacofTukey <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>  cans <span class="ot">&lt;-</span> (cons <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">6</span></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons, cans <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> (difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">3</span>), cans)</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons, (<span class="dv">1</span> <span class="sc">-</span> (difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>smacofCauchy <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">log</span>((difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">/</span> ((difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>))</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>smacofWelsch <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>(difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>(difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>smacofAndrews <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> pi <span class="sc">*</span> cons, </span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>                 (cons <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">cos</span>(x <span class="sc">/</span> cons)), </span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>                 <span class="dv">2</span> <span class="sc">*</span> (cons<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> pi <span class="sc">*</span> cons, <span class="fu">sin</span>(x <span class="sc">/</span> cons) <span class="sc">/</span> (x <span class="sc">/</span> cons), <span class="dv">0</span>)</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>smacofHinich <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons, (difi<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span>, (cons<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(difi) <span class="sc">&lt;</span> cons, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>smacofLogistic <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> (cons <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">cosh</span>(x <span class="sc">/</span> cons))</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> <span class="fu">tanh</span>(x <span class="sc">/</span> cons) <span class="sc">/</span> (x <span class="sc">/</span> cons)</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>smacofFair <span class="ot">&lt;-</span> <span class="cf">function</span>(nobj, wmat, delta, dmat, cons) {</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>  difi <span class="ot">&lt;-</span> delta <span class="sc">-</span> dmat</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>  resi <span class="ot">&lt;-</span> <span class="fu">log</span>((difi <span class="sc">/</span> cons)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>  wght <span class="ot">&lt;-</span> wmat <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">/</span> ((difi <span class="sc">/</span> cons) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>))</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>  strs <span class="ot">&lt;-</span> <span class="fu">sum</span>(wmat <span class="sc">*</span> resi)</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>    <span class="at">resi =</span> resi,</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>    <span class="at">wght =</span> wght,</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>    <span class="at">strs =</span> strs</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="references" class="level1 unnumbered" data-number="12">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">12 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-aftab_hartley_15" class="csl-entry" role="listitem">
Aftab, K., and R. Hartley. 2015. <span>“Convergence of Iteratively Re-Weighted Least Squares to Robust m-Estimators.”</span> In <em>2015 IEEE Winter Conference on Applications of Computer Vision,</em> 480–87. <a href="https://doi.org/10.1109/WACV.2015.70">https://doi.org/10.1109/WACV.2015.70</a>.
</div>
<div id="ref-aftab_hartley_trumpf_15" class="csl-entry" role="listitem">
Aftab, K., R. Hartley, and J. Trumpf. 2015. <span>“Generalized Weiszfeld Algorithms for Lq Optimization.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 37 (4): 728–44. <a href="https://doi.org/10.1109/TPAMI.2014.2353625">https://doi.org/10.1109/TPAMI.2014.2353625</a>.
</div>
<div id="ref-andrews_bickel_hampel_huber_rogers_tukey_72" class="csl-entry" role="listitem">
Andrews, D. F., P. J. Bickel, F. R. Hampel, P. J. Huber, W. H. Rogers, and J. W. Tukey. 1972. <em>Robust Estimators of Location: Survey and Advances</em>. Princeton University Press.
</div>
<div id="ref-barron_19" class="csl-entry" role="listitem">
Barron, J. T. 2019. <span>“A General and Adaptive Robust Loss Function.”</span> In <em>Proceedings 2019 IEEE/CVF Conferehce on Computer Vision and Pattern Recognition</em>, 4331–39. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf</a>.
</div>
<div id="ref-beaton_tukey_74" class="csl-entry" role="listitem">
Beaton, A. E., and W. Tukey J. 1974. <span>“The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.”</span> <em>Technometrics</em> 16 (147–185).
</div>
<div id="ref-beck_sabach_15" class="csl-entry" role="listitem">
Beck, A., and S. Sabach. 2015. <span>“Weiszfeld’s Method: Old and New Results.”</span> <em>Journal of Optimization Theory and Applications</em> 164: 1–40.
</div>
<div id="ref-black_anandan_96" class="csl-entry" role="listitem">
Black, M. J., and P. Anandan. 1996. <span>“The Robust Estimation of Multiple Motions: Parametric and Piecewise-Smooth Flow Fields.”</span> <em>Computer Vision and Image Understanding</em> 63 (1): 75–104.
</div>
<div id="ref-boehning_lindsay_88" class="csl-entry" role="listitem">
Böhning, D., and B. G. Lindsay. 1988. <span>“<span class="nocase">Monotonicity of Quadratic-approximation Algorithms</span>.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 40 (4): 641–63.
</div>
<div id="ref-candes_tao_05" class="csl-entry" role="listitem">
Candes, E. J., and T. Tao. 2005. <span>“Decoding by Linear Programming.”</span> <em>IEEE Transactions on Information Theory</em> 51 (12): 4203–15.
</div>
<div id="ref-candes_wakin_boyd_08" class="csl-entry" role="listitem">
Candes, E. J., M. B. Wakin, and S. P. Boyd. 2008. <span>“Enhancing Sparsity by Reweighted l_1 Minimization.”</span> <em>Journal of Fourier Analysis and Applications</em> 14: 877–905. <a href="https://doi.org/10.1007/s00041-008-9045-x">https://doi.org/10.1007/s00041-008-9045-x</a>.
</div>
<div id="ref-charbonnier_blanc-feraud_aubert_barlaud_94" class="csl-entry" role="listitem">
Charbonnier, P., L. Blanc-Feraud, G. Aubert, and M. Barlaud. 1994. <span>“<span class="nocase">Two deterministic half-quadratic regularization algorithms for computed imaging</span>.”</span> <em>Proceedings of 1st International Conference on Image Processing</em> 2: 168–72. <a href="https://doi.org/10.1109/icip.1994.413553">https://doi.org/10.1109/icip.1994.413553</a>.
</div>
<div id="ref-coleman_holland_kaden_klema_peters_80" class="csl-entry" role="listitem">
Coleman, D., P. Holland, N. Kaden, V. Klema, and S. C. Peters. 1980. <span>“A System of Subroutines for Iteratively Reweighted Least Squares Computations.”</span> <em>ACM Transactions on Mathematical Software</em> 6 (3): 327–36.
</div>
<div id="ref-degruijter_67" class="csl-entry" role="listitem">
De Gruijter, D. N. M. 1967. <span>“<span class="nocase">The Cognitive Structure of Dutch Political Parties in 1966</span>.”</span> Report E019-67. Psychological Institute, University of Leiden.
</div>
<div id="ref-deleeuw_C_77" class="csl-entry" role="listitem">
De Leeuw, J. 1977. <span>“Applications of Convex Analysis to Multidimensional Scaling.”</span> In <em>Recent Developments in Statistics</em>, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_A_84f" class="csl-entry" role="listitem">
———. 1984. <span>“<span class="nocase">Differentiability of Kruskal’s Stress at a Local Minimum</span>.”</span> <em>Psychometrika</em> 49: 111–13.
</div>
<div id="ref-deleeuw_A_88b" class="csl-entry" role="listitem">
———. 1988a. <span>“Convergence of the Majorization Method for Multidimensional Scaling.”</span> <em>Journal of Classification</em> 5: 163–80.
</div>
<div id="ref-deleeuw_C_88b" class="csl-entry" role="listitem">
———. 1988b. <span>“<span class="nocase">Multivariate Analysis with Optimal Scaling</span>.”</span> In <em>Proceedings of the International Conference on Advances in Multivariate Statistical Analysis</em>, edited by S. Das Gupta and J. K. Ghosh, 127–60. Calcutta, India: Indian Statistical Institute.
</div>
<div id="ref-deleeuw_C_94c" class="csl-entry" role="listitem">
———. 1994. <span>“<span class="nocase">Block Relaxation Algorithms in Statistics</span>.”</span> In <em>Information Systems and Data Analysis</em>, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. <a href="https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf">https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf</a>.
</div>
<div id="ref-deleeuw_E_18f" class="csl-entry" role="listitem">
———. 2018. <span>“<span class="nocase">MM Algorithms for Smoothed Absolute Values</span>.”</span> 2018. <a href="https://jansweb.netlify.app/publication/deleeuw-e-18-f/deleeuw-e-18-f.pdf">https://jansweb.netlify.app/publication/deleeuw-e-18-f/deleeuw-e-18-f.pdf</a>.
</div>
<div id="ref-deleeuw_groenen_mair_E_16a" class="csl-entry" role="listitem">
De Leeuw, J., P. Groenen, and P. Mair. 2016. <span>“<span class="nocase">Minimizing rStress Using Majorization</span>.”</span> 2016. <a href="https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-a/deleeuw-groenen-mair-e-16-a.pdf">https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-a/deleeuw-groenen-mair-e-16-a.pdf</a>.
</div>
<div id="ref-deleeuw_heiser_C_77" class="csl-entry" role="listitem">
De Leeuw, J., and W. J. Heiser. 1977. <span>“Convergence of Correction Matrix Algorithms for Multidimensional Scaling.”</span> In <em>Geometric Representations of Relational Data</em>, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.
</div>
<div id="ref-deleeuw_heiser_C_80" class="csl-entry" role="listitem">
———. 1980. <span>“Multidimensional Scaling with Restrictions on the Configuration.”</span> In <em>Multivariate Analysis, Volume <span>V</span></em>, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_lange_A_09" class="csl-entry" role="listitem">
De Leeuw, J., and K. Lange. 2009. <span>“Sharp Quadratic Majorization in One Dimension.”</span> <em>Computational Statistics and Data Analysis</em> 53: 2471–84.
</div>
<div id="ref-deleeuw_mair_A_09c" class="csl-entry" role="listitem">
De Leeuw, J., and P. Mair. 2009. <span>“<span class="nocase">Multidimensional Scaling Using Majorization: SMACOF in R</span>.”</span> <em>Journal of Statistical Software</em> 31 (3): 1–30. <a href="https://www.jstatsoft.org/article/view/v031i03">https://www.jstatsoft.org/article/view/v031i03</a>.
</div>
<div id="ref-dempster_laird_rubin_77" class="csl-entry" role="listitem">
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. <span>“<span class="nocase">Maximum Likelihood for Incomplete Data via the EM Algorithm</span>.”</span> <em>Journal of the Royal Statistical Society</em> B39: 1–38.
</div>
<div id="ref-dennis_welsch_78" class="csl-entry" role="listitem">
Dennis Jr, J. E., and R. E. Welsch. 1978. <span>“Techniques for Nonlinear Least Squares and Robust Regression.”</span> <em>Communications in Statistics - Simulation and Computation</em> 7 (4): 345–59.
</div>
<div id="ref-donoho_elad_03" class="csl-entry" role="listitem">
Donoho, D. L., and M. Elad. 2003. <span>“Optimally Sparse Representation in General (Nonorthogonal) Dictionaries via ℓ_1 Minimization.”</span> <em>Proceedings of the National Academy of Sciences</em> 100 (5): 2197–2202.
</div>
<div id="ref-franksen_grattan-guinness_89" class="csl-entry" role="listitem">
Franksen, O. I., and I. Grattan-Guinness. 1989. <span>“The Eraliest Contribution to Location Theory ? Spatio-Economic Equilibrium with Lam<span>é</span> and Clapeyron.”</span> <em>Mathematics and Computers in Simulation</em> 3w1: 195–220.
</div>
<div id="ref-gabriel_odoroff_84" class="csl-entry" role="listitem">
Gabriel, K. R., and Ch. L. Odoroff. 1984. <span>“Resistant Lower Rank Approximation of Matrices.”</span> In <em>Data Analysis and Informatics</em>, edited by E. Diday, M. Jambu, L. Lebart, J. Pages, and R. Tomassone, 3:23–30. North Holland Publishing Company.
</div>
<div id="ref-gabriel_zamir_79" class="csl-entry" role="listitem">
Gabriel, K. R., and S. Zamir. 1979. <span>“<span class="nocase">Lower Rank Approximation of Matrices by Least Squares with Any Choize of Weights</span>.”</span> <em>Technometrics</em> 21 (4): 489–98.
</div>
<div id="ref-gifi_B_90" class="csl-entry" role="listitem">
Gifi, A. 1990. <em>Nonlinear Multivariate Analysis</em>. New York, N.Y.: Wiley.
</div>
<div id="ref-groenen_giaquinto_kiers_03" class="csl-entry" role="listitem">
Groenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. <span>“<span class="nocase">Weighted Majorization Algorithms for Weighted Least Squares Decomposition Models</span>.”</span> Econometric Institute Report EI 2003-09. Econometric Institute, Erasmus University Rotterdam. <a href="https://repub.eur.nl/pub/1700">https://repub.eur.nl/pub/1700</a>.
</div>
<div id="ref-groenen_heiser_meulman_99" class="csl-entry" role="listitem">
Groenen, P. J. F., W. J. Heiser, and J. J. Meulman. 1999. <span>“<span class="nocase">Global Optimization in Least-Squares Multidimensional Scaling by Distance Smoothing</span>.”</span> <em>Journal of Classification</em> 16: 225–54.
</div>
<div id="ref-groenen_vandevelden_16" class="csl-entry" role="listitem">
Groenen, P. J. F., and M. Van de Velden. 2016. <span>“<span class="nocase">Multidimensional Scaling by Majorization: A Review</span>.”</span> <em>Journal of Statistical Software</em> 73 (8): 1–26. <a href="https://www.jstatsoft.org/index.php/jss/article/view/v073i08">https://www.jstatsoft.org/index.php/jss/article/view/v073i08</a>.
</div>
<div id="ref-he_li_liu_zhou_23" class="csl-entry" role="listitem">
He, Y., L. Li, D. Liu, and W.-Z. Zhou. 2023. <span>“Huber Principal Component Analysis for Large-Dimensional Factor Models.”</span> <a href="https://arxiv.org/abs/2303.02817">https://arxiv.org/abs/2303.02817</a>.
</div>
<div id="ref-heiser_86" class="csl-entry" role="listitem">
Heiser, W. J. 1986. <span>“<span class="nocase">A Majorization Algorithm for the Reciprocal Location Problem</span>.”</span> RR-86-12. Department of Data Theory, University of Leiden.
</div>
<div id="ref-heiser_87" class="csl-entry" role="listitem">
———. 1987. <span>“<span class="nocase">Correspondence Analysis with Least Absolute Residuals</span>.”</span> <em>Computational Statistics and Data Analysis</em> 5: 337–56.
</div>
<div id="ref-heiser_88" class="csl-entry" role="listitem">
———. 1988. <span>“<span class="nocase">Multidimensional Scaling with Least Absolute Residuals</span>.”</span> In <em>Classification and Related Methods of Data Analysis</em>, edited by H. H. Bock, 455–62. North-Holland Publishing Co.
</div>
<div id="ref-heiser_95" class="csl-entry" role="listitem">
———. 1995. <span>“<span class="nocase">Convergent Computing by Iterative Majorization: Theory and Applications in Multidimensional Data Analysis</span>.”</span> In <em>Recent Advantages in Descriptive Multivariate Analysis</em>, edited by W. J. Krzanowski, 157–89. Oxford: Clarendon Press.
</div>
<div id="ref-hinich_talwar_75" class="csl-entry" role="listitem">
Hinich, M. J., and P. P. Talwar. 1975. <span>“A Simple Method for Robust Regression.”</span> <em>Journal of the American Statistical Association</em> 70: 113–19.
</div>
<div id="ref-holland_welsch_77" class="csl-entry" role="listitem">
Holland, P. W., and R. E. Welsch. 1977. <span>“Robust Regression Using Iteratively Reweighted Least-Squares.”</span> <em>Communications in Statistics - Theory and Methods</em> 6 (9): 813–27. <a href="https://doi.org/10.1080/03610927708827533">https://doi.org/10.1080/03610927708827533</a>.
</div>
<div id="ref-huber_64" class="csl-entry" role="listitem">
Huber, P. J. 1964. <span>“Robust Estimation of a Location Parameter.”</span> <em>Annals of Mathematical Statistics</em> 35 (1): 73–101.
</div>
<div id="ref-hunter_li_05" class="csl-entry" role="listitem">
Hunter, D. R., and R. Li. 2005. <span>“<span>Variable Selection Using MM Algorithms</span>.”</span> <em>The Annals of Statistics</em> 33: 1617–42.
</div>
<div id="ref-jaakkola_jordan_00" class="csl-entry" role="listitem">
Jaakkola, T. S., and M. I. Jordan. 2000. <span>“<span class="nocase">Bayesian Parameter Estimation via Variational Methods</span>.”</span> <em>Statistics and Computing</em> 10: 25–37.
</div>
<div id="ref-katz_69" class="csl-entry" role="listitem">
Katz, I. N. 1969. <span>“On the Convergence of a Numerical Scheme for Solving Some Locational Equilibrium Problems.”</span> <em><span>SIAM</span> Journal on Applied Mathematics</em> 17 (6): 1224–31.
</div>
<div id="ref-lange_16" class="csl-entry" role="listitem">
Lange, K. 2016. <em>MM Optimization Algorithms</em>. SIAM.
</div>
<div id="ref-li_24" class="csl-entry" role="listitem">
Li, J. 2024. <span>“Robust Matrix Factor Analysis Method with Adaptive Parameter Adjustment Using Cauchy Weighting.”</span> <em>Computational Statistics</em>. <a href="https://doi.org/10.1007/s00180-024-01548-4">https://doi.org/10.1007/s00180-024-01548-4</a>.
</div>
<div id="ref-mlotshwa_vandeventer_bosman_23" class="csl-entry" role="listitem">
Mlotshwa, T., H. Van Deventer, and Sergeevna Bosman A. 2023. <span>“Cauchy Loss Function: Robustness Under Gaussian and Cauchy Noise.”</span> <a href="https://arxiv.org/abs/2302.07238">https://arxiv.org/abs/2302.07238</a>.
</div>
<div id="ref-mordukhovich_nam_19" class="csl-entry" role="listitem">
Mordukhovich, B. S., and N. M. Nam. 2019. <span>“The Fermat-Torricelli Problem and Weiszfeld’s Algorithm in the Light of Convex Analysis.”</span> <em>Journal of Applied Numerical Optimization</em> 1 (3): 205–19. https://doi.org/<a href="https://doi.org/10.23952/jano.1.2019.3.02">https://doi.org/10.23952/jano.1.2019.3.02</a>.
</div>
<div id="ref-phillips_02" class="csl-entry" role="listitem">
Phillips, R. F. 2002. <span>“Least Absolute Deviations Estimation via the EM Algorithm.”</span> <em>Statistics and Computing</em> 12: 281–85.
</div>
<div id="ref-plastria_11" class="csl-entry" role="listitem">
Plastria, F. 2011. <span>“The Weiszfeld Algorithm: Proof, Amendments and Extensions.”</span> In <em>Foundations of Location Analysis</em>, edited by H. A. Eiselt and V. Marianov, 155:357–89. International Series inOperations Research and Management Science. Springer.
</div>
<div id="ref-pliner_96" class="csl-entry" role="listitem">
Pliner, V. 1996. <span>“<span class="nocase">Metric Unidimensional Scaling and Global Optimization</span>.”</span> <em>Journal of Classification</em> 13: 3–18.
</div>
<div id="ref-r_core_team_24" class="csl-entry" role="listitem">
R Core Team. 2024. <em>R: A Language and Environment for Statistical Computing</em>. <span>Vienna, Austria</span>: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-ramirez_sanchez_kreinovich_argaez_14" class="csl-entry" role="listitem">
Ramirez, C., R. Sanchez, V. Kreinovich, and M. Argaez. 2014. <span>“<span class="nocase"><span class="math inline">\(\sqrt{x^2+\mu}\)</span> is the Most Computationally Efficient Smooth Approximation to x</span>.”</span> <em>Journal of Uncertain Systems</em> 8: 205–10.
</div>
<div id="ref-ramsay_77" class="csl-entry" role="listitem">
Ramsay, J. O. 1977. <span>“<span class="nocase">Maximum Likelihood Estimation in Multidimensional Scaling</span>.”</span> <em>Psychometrika</em> 42: 241–66.
</div>
<div id="ref-rothkopf_57" class="csl-entry" role="listitem">
Rothkopf, E. Z. 1957. <span>“<span class="nocase">A Measure of Stimulus Similarity and Errors in some Paired-associate Learning</span>.”</span> <em>Journal of Experimental Psychology</em> 53: 94–101.
</div>
<div id="ref-schlossmacher_73" class="csl-entry" role="listitem">
Schlossmacher, E. J. 1973. <span>“An Iterative Technique for Absolute Deviations Curve Ftting.”</span> <em>Journal of the American Statistical Association</em> 68: 857–59.
</div>
<div id="ref-sturm_84" class="csl-entry" role="listitem">
Sturm, R. 1884. <span>“Ueber Den Punkt Kleinster Entferningssumme von Gegebenen Punkten.”</span> <em>Journal f<span>ü</span>r Die Reine Und Angewandte Mathematik</em> 97: 49–61.
</div>
<div id="ref-vanruitenburg_05" class="csl-entry" role="listitem">
Van Ruitenburg, J. 2005. <span>“<span class="nocase">Algorithms for Parameter Estimation in the Rasch Model</span>.”</span> Measurement and Research Department Reports 2005-04. Arnhem, Netherlands: CITO. <a href="https://www.researchgate.net/publication/355568984_Algorithms_for_parameter_estimation_in_the_Rasch_model_Measurement_and_Research_Report_05-04#fullTextFileContent">https://www.researchgate.net/publication/355568984_Algorithms_for_parameter_estimation_in_the_Rasch_model_Measurement_and_Research_Report_05-04#fullTextFileContent</a>.
</div>
<div id="ref-verboon_heiser_94" class="csl-entry" role="listitem">
Verboon, P., and W. J. Heiser. 1994. <span>“<span class="nocase">Resistant Lower Rank Approximation of Matrices by Iterative Majorization</span>.”</span> <em>Computational Statistics and Data Analysis</em> 18: 457–67.
</div>
<div id="ref-voronin_ozkaya_yoshida_15" class="csl-entry" role="listitem">
Voronin, S., G. Ozkaya, and Y. Yoshida. 2014. <span>“<span class="nocase">Convolution Based Smooth Approximations to the Absolute Value Function with Application to Non-smooth Regularization</span>.”</span> 2014. <a href="https://arxiv.org/abs/1408.6795">https://arxiv.org/abs/1408.6795</a>.
</div>
<div id="ref-vosz_eckhardt_80" class="csl-entry" role="listitem">
Vosz, H., and U. Eckhardt. 1980. <span>“<span class="nocase">Linear Convergence of Generalized <span>W</span>eiszfeld’s Method</span>.”</span> <em>Computing</em> 25: 243–51.
</div>
<div id="ref-weiszfeld_37" class="csl-entry" role="listitem">
Weiszfeld, E. 1937. <span>“<span class="nocase">Sur le Point par lequel la Somme des Distances de n Points Donnés est Minimum</span>.”</span> <em>Tohoku Mathematics Journal</em> 43: 355–86.
</div>
<div id="ref-weiszfeld_plastria_09" class="csl-entry" role="listitem">
Weiszfeld, E., and F. Plastria. 2009. <span>“On the Point for Which the Sum of the Distances to n Given Points Is Minimum.”</span> <em>Annals of Operations Research</em> 167: 7–41.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>