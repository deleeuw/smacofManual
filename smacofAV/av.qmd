---
title: "Robust Least Squares Multidimensional Scaling"
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: Combining different loss functions with linear models and minimizing   loss with iteratively reweighted least squares (IRLS) has a long history in
  robust statistics. In this paper we use an IRLS version of the smacof
  algorithm to minimize various robust multidimensional scaling loss functions.
  Our results use a general theorem on sharp quadratic majorization of
  @deleeuw_lange_A_09. We relate this theorem to earlier results in robust
  statistics, location theory, and sparse recovery. Code in R is included. 
---

\sectionbreak

\listoffigures

\sectionbreak


```{r gruijterdata, echo = FALSE}
gruijter <-
  structure(
    c(
      5.63,
      5.27,
      4.6,
      4.8,
      7.54,
      6.73,
      7.18,
      6.17,
      6.72,
      5.64,
      6.22,
      5.12,
      4.59,
      7.22,
      5.47,
      5.46,
      4.97,
      8.13,
      7.55,
      6.9,
      4.67,
      3.2,
      7.84,
      6.73,
      7.28,
      6.13,
      7.8,
      7.08,
      6.96,
      6.04,
      4.08,
      6.34,
      7.42,
      6.88,
      6.36,
      7.36
    ),
    names = c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66"),
    Size = 9L,
    call = quote(as.dist.default(m = polpar)),
    class = "dist",
    Diag = FALSE,
    Upper = FALSE
  )
delta <- as.matrix(gruijter)
names <- c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66")
row.names(delta) <- colnames(delta) <- names
index <- matrix(1:81, 9, 9)
jndex <- index[-c(6, 8), -c(6, 8)][outer(1:7, 1:7, "<")]
icpn <- c(46:50, 52:54)
ibp <- c(64:70, 72)
iall <- index[outer(1:9, 1:9, "<")]
data(morse2, package = "smacof")
morse2 <- 1 - morse2
morse2 <- ifelse(morse2 == 0, .01, morse2)
dm <- diag(morse2)
morse <- -log((morse2 * t(morse2)) / (outer(dm, dm)))
kndex <- matrix(1:1296, 36, 36)[outer(1:36, 1:36, "<")]
```

```{r loadcode, echo = FALSE}
library(ggplot2)
source("smacofRobust.R")
```


# Introduction

The title of this paper is somewhat paradoxical. Least squares estimation
is typically not robust, it is sensitive to outliers and pays too much  attention to minimizing the largest residuals. What we mean by robust least squares multidimensional scaling (MDS), however, is using the smacof machinery designed to minimize least squares loss functions of the form
\begin{equation}
\sigma_2(X):=\sum \omega_k(\delta_k-d_k(X))^2\label{eq:stressdef},
\end{equation}
to minimize robust loss functions. 

The prototypical robust loss function is least absolute value loss
\begin{equation}
\sigma_1(X):=\sum \omega_k|\delta_k-d_k(X)|\label{eq:stradddef},
\end{equation}
which we will call
*strife*, because the names *stress*, *sstress*, and *strain* are already taken.

Strife is not differentiable at configurations $X$ for which there is at least one $k$ for which either $d_k(X)=\delta_k$ or $d_k(X)=0$ (or both). This lack of differentiability complicates the minimization problem. Moreover experience
with one-dimensional and city block MDS suggests that having areas
where the loss function is not differentiable leads to (many) additional local minima.

In this paper we will discuss (and implement) various variations of 
$\sigma_1$ from \eqref{eq:stradddef}. They can be interpreted in two different
ways. On the one hand we use smoothers of the absolute value
function, and consequently of strife. We want to eliminate the
problems with differentiability, at least the ones caused by
$\delta_k=d_k(X)$. If this is our main goal, then we want to choose
the smoother in such a way that it is close to the absolute value
function. This is not unlike the distance
smoothing used by @pliner_96 and @groenen_heiser_meulman_99 in 
the global minimization of $\sigma_2$ from \eqref{eq:stressdef}.

On the other hand our modified loss function can be interpreted
as more robust versions of the least squares loss function, and
consequently of stress. Our goal then is to combines the robustness of the
absolute value function with the efficiency and computational ease 
of least squares. If robustness is our goal then there is no reason to
stay that close to the absolute value function.

Our robust or smooth loss functions are all of the form
\begin{equation}
\sigma(X):=\sum \omega_k\ f_c(\delta_k-d_k(X))\label{eq:strifedef},
\end{equation}
for a suitable choice of the real valued function $f$. We will
define what we mean by "suitable" later on. The subscript $c$
of $f_c$ is meant to indicate that the loss function may depend
on one or more real-valued tuning parameters $c$ that regulate the degree
of smoothness and/or robustness. For now, note that loss
\eqref{eq:stressdef} is the special case with $f_c(x)=x^2$ and loss \eqref{eq:stradddef}is the special case with $f_c(x)=|x|$. There is no tuning in both these cases.

\sectionbreak

# Majorizing Strife {#sec-majorization}

The idea of minimizing a least absolute value (LAV) to obtain parameter estimates dates back to the work of Boskovitch in the middle of the
eighteenth century. Until recently it has been applied mainly to fit
linear models, so that we can actually use standard linear programming
algorithms to obtain optimal solutions.

The pioneering work in MDS strife minimization using smacof is @heiser_88, which builds on earlier work of @heiser_87. It is based on a creative use of the Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of the absolute
value function. For the general theory of majorization algorithms (now more commonly known as MM algorithms) we refer to their original introduction in @deleeuw_C_94c and to the excellent recent book by @lange_16.

The AM/GM inequality says that for all non-negative $x$ and $y$ we have 
\begin{equation}
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),\label{eq:amgm}
\end{equation}
with equality if and only if $x^2=y^2$. If $y>0$ we can write \eqref{eq:amgm} as
\begin{equation}
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\label{eq:amgmmaj}
\end{equation}
and this provides a quadratic majorization of $|x|$ at $y$. There is no quadratic
majorization of $|x|$ at $y=0$, which is a problem we will have to deal with.

Using the majorization \eqref{eq:amgmmaj}, and assuming $\delta_k\not=d_k(Y)$ for all $k$, we define
\begin{equation}
\omega_1(X):=\frac12\sum \omega_k\frac{1}{|\delta_k-d_k(Y)|}((\delta_k-d_k(Y))^2+(\delta_k-d_k(X))^2).\label{eq:omegadef}
\end{equation}
Now $\sigma_1(X)\leq\omega_1(X)$ for all $X$ and $\sigma_1(Y)=\omega_1(Y)$, and thus
$\omega_1$ majorizes $\sigma_1$ at $Y$.

## Algorithm {#sec-amgm}

Define
\begin{equation}
\omega_k(Y):=\omega_k\frac{1}{|\delta_k-d_k(Y)|}.\label{eq:wk1def}
\end{equation}
Reweighted smacof to minimize strife computes $X^{(k+1)}$ by decreasing
\begin{equation}
\sum \omega_k(X^{(k)})(\delta_k-d_k(X^{(k)}))^2,\label{eq:sstrf}
\end{equation}
using a standard smacof step. It then computes the new weights $\omega_k(X^{(k+1)})$ from \eqref{eq:wk1def}
and uses them in the next smacof step to update $X^{(k+1)}$. And so on, until convergence.

A straightforward variation of the algorithm does a number of smacof steps before
upgrading the weights. This still leads to a monotone, and thus convergent, algorithm.
How many smacof steps we have to take in the inner iterations is something that needs further study. It is likely to depend on the fit of the data, on the shape of the function near the local minimum, and on how far the iterations are from the local minimum.

## Zero Residuals {#sec-zero}

It may happen that for some $k$ we have $d_k(X^{(k)})=\delta_k$ while iterating. There have been various proposals to deal with such an 
unfortunate event, and we will discuss some of them further on. Even more
importantly we will see that that the minimizer of the absolute value
loss usually satisfies $d_k(X)=\delta_k$ for quite a few elements,
which means that near convergence the algorithm will become unstable
because the weights from \eqref{eq:wk1def} become very large. 

A large number of somewhat ad-hoc solutions have been proposed to deal with the problem of zero residuals, both in location analysis and in the statistical literature. We tend to agree with the assessment of @aftab_hartley_15.

> .. attempts to analyze this difficulty [caused by infinite weights of IRLS for the $\ell_p$-loss] have a long history of proofs and counterexamples to incorrect claims.

@schlossmacher_73 is the first discussion of the majorization method in the statistical literature (for LAV linear regression). His proposal is to simply set a weight equal to zero if the corresponding residual is
less than some small positive value $\epsilon$. A similar approach, also used in location analysis, is to cap the weights at some large positive value.
In @heiser_88 all residuals smaller than this epsilon get a weight equal to the weighted average of all these small residuals. @phillips_02 assumes double-exponential
errors in LAV regression and then concludes that the EM algorithm gives the 
majorization method we have discussed. He uses \eqref{eq:wk1def} throughout if all residuals are larger than $\epsilon$. If one of more residuals are smaller than epsilon then the weight for those residuals is set equal to one, while for the remaining residuals the weight is set to epsilon divided by the absolute value of the
residual. Often we get the assurance in these papers that the problem is not really important in practice, because it is very rare, and by just wiggling we will get to the unique solution anyway. But both in location analysis and in 
LAV regression the loss function is convex, however, which guarantees a unique minimum. This is certainly not the case in robust MDS. In this paper we try to follow a more systematic approach that uses smooth parametric approximations to the absolute value function, where the parameter can be used to make the 
approximation as precise as necessary.

To illustrate the problems with differentiability we compute the directional derivatives of strife. 

Let $s_k(X):=\omega_k|d_k(X)-\delta_k|$.

1. If $\delta_k=0$ and $d_k(X)=0$ then $ds_k(X;Y)=\omega_kd_k(Y)$.
2. If $\delta_k>0$ and $d_k(X)=0$ then $ds(X;Y)=-\omega_kd_k(Y)$.
3. If $d_k(X)>0$ and $d_k(X)-\delta_k>0$ then 
$ds_k(X;Y)=\omega_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
4. If $d_k(X)>0$ and $d_k(X)-\delta_k<0$ then $ds_k(X;Y)=-\omega_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
5. If $d_k(X)>0$ and $d_k(X)-\delta_k=0$ then $ds_k(X;Y)=\omega_k\frac{1}{d_k(X)}\text{tr}\ |X'A_kY|$.

The directional derivative of $\sigma_1$ is consequently the sum of
five terms, corresponding with each of these five cases.

In the case of stress the directional derivatives could be used to prove
that if $\omega_k\delta_k>0$ for all $k$ then stress is differentiable at each local minimum (@deleeuw_A_84f). For strife to be differentiable we would have to prove that at a local minimum both $d_k(X)>0$ and $(d_k(X)-\delta_k)\not= 0$ for all $k$ with $\omega_k>0$. But this is impossible by the following argument. In the one-dimensional
case we can partition $\mathbb{R}^n$ into $n!$ polyhedral convex cones
corresponding with the permutations of $x$. Within each cone the distances
are a linear function of $x$. Each cone can be partitioned by intersecting it
with the polyhedra defined by the linear inequalities $\delta_k-d_k(x)\geq 0$ or $\delta_k-d_k(x)\leq 0$. Some of these intersections can and will obviously be empty. Within each of these non-empty polyhedral regions strife is a linear function of $x$. Thus it attains its minimum for the region at a vertex, which is a solution for which some
distances are zero and/or some residuals are zero. There can be no  
minima, local or global, in the interior of one of these polyhedral regions.
We have thus shown that in one dimension strife is not differentiable at a local minimum, and that there is presumably a large number of them. Even for moderate $n$ the number of regions is of course too large to actually compute or draw. 

In the multidimensional case linearity goes out the window. The
set of configurations $d_k(X)=\delta_k$ is an ellipsoid and $d_k(X)=0$ defines a hyperplane. Strife is not differentiable at all intersections of these
ellipsoids and hyperplanes. The partitioning of $\mathbb{R}^n$ by these
ellipsoids and hyperplanes is not simple to describe. It has convex
and non-convex cells, and within each cell strife is the difference
of two weighted sums of distances. Anything can happen. 

## $\ell_0$ loss

A somewhat extreme special case of Equation \eqref{eq:strifedef} has
$$
f(x)=\begin{cases}
0&\text{ if }x = 0,\\
1&\text{ otherwise}.
\end{cases}
$$
This is $\ell_0$ loss. Minimizing $\ell_0$ loss means maximizing the number 
of cases with perfect fit, i.e. with $\delta_k=d_k(X)$. The reason we
mention it here is that the work of @donoho_elad_03 and @candes_tao_05   suggests that the minimizer of $\ell_1$ loss, i.e. absolute value loss, gives a good approximation to the minimizer of $\ell_0$ loss, at least in a number of
special cases. In MDS we do not have linearity or convexity, but nevertheless
the theoretical results in simpler cases are suggestive. By computing the directional derivatives we have seen that at least in the one-dimensional MDS case a number of residuals will indeed be zero at the optimum LAV solution.

There is an excellent review of the use of $\ell_1$ in various sparse recovery fields in @candes_wakin_boyd_08. In that paper they also propose an iteratively
reweighted LAV algorithm, which solves $\ell_1$ problems between weight updates. Maybe because of that they go so far as calling $\ell_1$ "the modern least squares". But let's not get carried away, in actual ease and frequency of use $\ell_1$ still has a long way to go if it wants to replace $\ell_2$.

\sectionbreak

# Generalizing Strife

We have seen that @heiser_88 applied majorization to minimize strife, using the AM/GM inequality. We now generalize this approach so that it can easily deal with other robust loss functions. A great number of different loss functions will be discussed. The intention is not to confuse the reader by presenting a large number of alternatives with rather limited information. We show all these loss functions as examples of a general principle of algorithm construction and as examples of loss functions that have been used in
statistics, location analysis, image analysis and engineering over the years.
They are all implemented in the function smacofRobust(), written in R (@r_core_team_24).

## Majorization

First some definitions. 

::: {#def-majorize}
A function $g$ *majorizes* a function $f$ at $y$ if $g(x)\geq f(x)$ for all $x$ and $g(y)=f(y)$. The point $y$ is the *support point* of the majorization. Majorization of $f$ at $y$ is *strict* if $g(x)>f(x)$ for all $x\not= y$.
:::
 
::: {#def-sharp}
If $\mathfrak{H}$ is a family of functions that all majorize $f$ at $y$ then $h\in\mathfrak{H}$ is a *sharp majorization* in $\mathfrak{H}$ if $h(x)\leq g(x)$ for all $g\in\mathfrak{H}$. The sharp majorization, if it exists, is by definition unique.
:::

::: {#thm-diff}
Suppose $f$ and $g$ are two functions defined on an open interval of the
real line and $y$ is a point of the interval where $g$ majorizes $f$.

* If $f$ and $g$ are differentiable at $y$ then $f'(y)=g'(y)$.
* If $f$ and $g$ are twice-differentiable at $y$ then $f''(y)\leq g''(y)$.
* If $f''(y)<g''(y)$ then $g$ strictly majorizes $f$ in a neighborhood of $y$.
:::
::: {.proof}
$h=g-f$ is non-negative and has a minimum equal to zero at $y$. Thus the derivative of $h$ vanishes at $y$ and the second derivative is non-negative at $y$. If the second derivative is negative then we use the sufficient 
condition for a local minimum.
:::


### Sharp Quadratic Majorization

The AM/GM inequality was used in @sec-majorization to construct a quadratic majorization of strife. In this paper we are specifically interested in sharp quadratic majorization, in which $\mathfrak{H}$ is the set of all convex quadratics that majorize $f$ at $y$. This case has been studied in detail (in the case of real-valued functions on the line) in @deleeuw_lange_A_09. For the
loss functions we study there are two problems that have to be solved. First,
we need a general procedure to construct quadratic majorizers. Second, we need
to show that some of our majorizers are sharp.

If $f$ is differentiable at $y$, then all quadratics $g$ that majorize $f$ at $y$ have
$$
g(x)=f(y)+f'(y)(x-y)+\frac12a(x-y)^2
$${#eq-qmaj}
for some $a$, not necessarily positive.
Since $g''(y)\geq f''(y)$ by @thm-diff we must have $a\geq f''(y)$. Note that not all functions have quadratic majorizations. If $f$ is a non-trivial cubic
then so is $h=g-f$, and consequently we cannot have $h\geq 0$ on the whole real line.

We now look more closely at $a$ in @eq-qmaj. For $x\not = y$ define
$$
\alpha(x):=\frac{f(x)-f(y)-f'(y)(x-y)}{\frac12(x-y)^2}
$${#eq-alpdef}
If $f$ is two times differentiable at $y$ then by l' Hôpital
$$
\lim_{x\rightarrow y}\alpha(x)=f''(y),
$${\#eq-alphop}
and thus we define $\alpha(y)=f''(y)$ to make $\alpha$ continuous at $y$.
Of course $\alpha$ is a different function of $x$ for each $y$, but since we are dealing with only one fixed $y$ here we suppress this dependence. 

If $f$ is convex, then $\alpha$ is the ratio of two non-negative convex functions of $x$. If $f$ is two times differentiable then
there is a $z$ between $x$ and $y$ such that $\alpha(x)=f''(z)$.
If $f''(x)\leq K$ for all $x$, then $\alpha(x)\leq K$ as well.

Quadratic majorization is equivalent to $a\geq\alpha(x)$ for all $x$. 
Sharp quadratic majorization is possible if and only 
$$
A:=\sup_x\alpha(x)<+\infty,
$${#eq-Adef}
in which case we get sharp quadratic majorization by setting $a=A$ in @eq-qmaj. The quadratic $g$ of @eq-qmaj majorizes $f$ if and only if $a\geq A$.

We illustrate these concepts by using low-degree polynomials. First a cubic. Expand the function around $y$ as $f(x)=f(y)+f'(y)(x-y)+\frac12f''(y)(x-y)^2+\frac16f'''(y)(x-y)^3$. Thus
$\alpha(x)=f''(y)+3f'''(y)(x-y)$
and we have majorization at $y$ if the linear function $\alpha$ is everywhere
non-negative. Since this is impossible, no quadratic majorizer exists at any $y$. Now apply the same reasoning to a quartic. We find
$\alpha(x)=f''(y)+3f'''(y)(x-y)+\frac{1}{12}f^{iv}(y)(x-y)^2$, a quadratic in $x$. We have quadratic majorization at $y$ if this quadratic is non-negative in  $x-y$. It is clearly necessary that $f^{iv}(y)$ is positive.
This implies the quadratic is unbounded above, and thus a sharp quadratic majorization does not exist at any $y$. Non-sharp quadratic majorizations at $y$ exist if the discriminant of the quadratic is non-positive, and this inequality constraint on $(f''(y),f'''(y),f^{iv}(y))$ defines a non-empty cone in $\mathbb{R}^3$. It is sufficient for quadratic majorization at $y$, for example, that both $f''(y)$ and $f^{iv}(y)$ are positive and that $f'''(y)=0$. It is necessary that both $f''(y)$ and $f^{iv}(y)$ are positive.


::: {#thm-locmax}
Suppose $\alpha$ has a local maximum at a point $x$ where $\alpha$ is two times differentiable. Then 
\begin{subequations}
\begin{align}
\frac{f(x)-f(y)}{x-y}&=\frac12(f'(x)+f'(y)),\label{eq-loc1}\\
\alpha(x)&=\frac{f'(x)-f'(y)}{x-y},\label{eq-loc2}\\
f''(x)&\leq\frac{f'(x)-f'(y)}{x-y}.\label{eq-loc3}
\end{align}
\end{subequations}
If the inequality in \eqref{eq-loc3} is strict then $\alpha$ has a local maximum at $x$.
:::
::: {.proof}
After some manipulation \eqref{eq-loc1} and \eqref{eq-loc3} are the necessary conditions $\alpha'(x)=0$ and 
$\alpha''(x)\leq 0$ for a local maximum, and the sufficient condition $\alpha''(x)<0$. If $x$ satisfies \eqref{eq-loc1} then substitution in
@eq-alpdef gives \eqref{eq-loc2}.
:::

Note that we have not shown that $\alpha$ always attains its maximum. 
@deleeuw_lange_A_09 give the example of the differentiable function
$$
f(x)=\begin{cases}
x^2&\text{ if }x\leq 1,\\
2x-1&\text{ otherwise},
\end{cases}
$$(#eq-dllexam)
which has $\alpha(x)=0$ for $x>1$ and $\alpha(x)<2$ for $x\leq 1$, so that
$A=\sup_{x\leq 1}\alpha(x)=2$ and the maximum does not exist.

Also, the conditions of @thm-locmax cannot show
that $\alpha$ has a *global* maximum at $x$, and that consequently $g$ of @eq-qmaj with $a$ given by \eqref{eq-loc2} is a sharp quadratic majorizer.

### Regulated Functions

Theorem 4.5 in @deleeuw_lange_A_09 gives a way to construct quadratic
majorizers of a differentiable function on an interval. We generalize it
here to regularized functions, which may not be differentiable at all
points in the interval. First a short introduction to regularized 
functions (@dieudonne_69, sections 7.6 and 8.7).

::: {#def-regulated}
A real-valued function on a closed interval $[a,b]$ is *regulated* if it has a limit from the right for each $x$ in $[a,b)$ and a limit from the left for each $x$ in $(a,b]$.
:::

Note that the end-points of the interval can be $\pm\infty$. Regularized functions can only have discontinuities of the first kind (jump discontinuities). Step functions, monotone functions, functions of bounded variation, and continuous functions are all regulated functions. An alternative definition is that regulated functions are limits of sequences of step functions, where the convergence is uniform on compact sets. 

::: {#def-primitive}
A real-valued function $f$ on $[a,b]$ is a *primitive* of a real-valued function $g$ on $[a,b]$ if $f$ is differentiable with $f'(x)=g(x)$, except possibly at a denumerable number of points.
:::

A regulated function has at least one primitive, and primitives are unique
up to addition of a constant function. The primitive $f$ of a continuous function $g$ is differentiable everywhere, with $f'(x)=g(x)$.

::: {#def-integral}
The *integral* of a regulated function $f$ on $[a,b]$, written as $\int_a^b f(x)dx$, is equal to $g(b)-g(a)$, where $g$ is one of the primitives of $f$.
:::


::: {#thm-dll}
Suppose $g$ is a regulated function on $[x,y]$ and $f$ is one of its primitives. Define $h(z):=g(z)/z$ and assume $h$ is non-increasing on $[x,y]$. Then we have the quadratic majorization
$$
f(x)\leq f(y)+\frac12h(y)(x^2-y^2).
$${#eq-amajor}
of $f$ at $y$.
:::
::: {.proof}
$$
f(y)-f(x)=\int_x^y g(z)dz=\int_x^y h(z)zdz\geq h(y)\int_x^yzdz=\frac12(y^2-x^2).
$${#eq:dllproof}
:::

@thm-dll implicitly assumes that if $[x,y]$ contains zero then 
$h(z)/z$ is defined at $z=0$. Maybe $h$ is of the form $h(z)=zu(z)$
for some function $u$, or maybe l'Hôpital applies. 
Since the value of the integral does not change if we change the integrand at a single point this causes no loss of generality.

Suppose $g$ is the sign function, which is obviously regulated, and 
$f$ is the absolute value function, a primitive of $g$. Then for
$y>0$
$$
|x|\leq|y|+\frac12\frac{1}{|y|}(x^2-y^2)=\frac12\frac{1}{|y|}(x^2+y^2),
$$
which is the AM/GM inequality.

### Two Support Points {#sec-twosupport}

::: {#thm-twop}
Suppose the quadratic $g$ majorizes $f$ at $y$ and at $z\not= y$. Then
$$
a=\frac{f'(z)-f'(y)}{z-y}
$${#eq-atwop}
:::
::: {.proof}
From @eq-qmaj we have $g'(x)=f'(y)+a(x-y)$. But because of @thm-diff
we must also have $g'(z)=f'(z)$. Thus $g'(z)=f'(y)+a(z-y)=f'(z)$.
:::

Again, we have not shown that $g$ with $a$ from @eq-atwop majorizes
$f$ at $y$ and $z$. Only the reverse implication, which is that if $g$
majorizes $f$ at $y$ and $z$ then $g$ is uniquely determined
by @eq-atwop. In practice, even if one knows the support points $y$ and $z$, 
one still has to prove
majorization. This is precisely how @heiser_88, @verboon_heiser_94, and 
@groenen_giaquinto_kiers_03 establish their majorizations.
@vanruitenburg_05 takes us a step further down that road.

::: {#lem-fan}
If different quadratics $g$ and $h$ majorize $f$ at $y$ then either
$g$ strictly majorizes $h$ or $h$ strictly majorizes $g$.
:::
::: {.proof}
We have
\begin{subequations}
\begin{align}
g(x)&=f(y)+f'(y)(x-y)+\frac12a_1(x-y)^2,\label{eq-gfunc}\\
h(x)&=f(y)+f'(y)(x-y)+\frac12a_2(x-y)^2.\label{eq-hfunc}.
\end{align}
\end{subequations}
Thus $g(x)-h(x)=\frac12(a_1-a_2)(x-y)^2$, which is either
strictly positive or strictly negative for $x\not= y$ and
zero for $x=y$.
:::

::: {#lem-ruit}
Suppose quadratics $g$ and $h\not= g$ majorize $f$ at $y$. 
Suppose, in addition, that $g$ majorizes $f$ at $z\not=y$.
Then $h$ strictly majorizes $g$ at $y$. 
:::
::: {.proof}
If $g$ strictly majorizes $h$ at $y$ then $h(z)<g(z)=f(z)$
and thus $h$ does not majorize $f$. By @lem-fan
$h$ strictly majorizes $g$ at $y$.
:::

::: {#thm-ruit} 
If the quadratic $g$ majorizes  $f$ at $y$ and at $z\not= y$, then $g$ is a
sharp majorizer of $f$ at $y$.
:::
::: {.proof}
Directly from @lem-ruit.
:::



### Even Functions

::: {#thm-evqu}
If $f$ is even and the quadratic $g$ majorizes $f$ at $y$ and $-y$, where $y\not= 0$, then $g$ is the even quadratic given by
$$
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
$${#eq-geven}
Moreover $g$ is the sharp quadratic majorization of $f$ at $y$ and $-y$.
:::
::: {.proof}
If $f$ is even then $f'$ is odd. Consequently @eq-atwop becomes
$$
a=\frac{f'(y)}{y},
$${#eq-aeven}
which is even. Moreover because $g$ majorizes $f$ at $y$
\begin{subequations}
\begin{equation}
g(x)=f(y)+f'(y)(x-y)+\frac12\frac{f'(y)}{y}(x-y)^2,\label{eq-gatplusy}
\end{equation}
and because $g$ majorizes $f$ at $-y$
\begin{equation}
g(x)=f(y)-f'(y)(x+y)+\frac12\frac{f'(y)}{y}(x+y)^2.\label{eq-gatminy}
\end{equation}
\end{subequations}
Averaging the two equations \eqref{eq-gatplusy} and \eqref{eq-gatminy} for $g$, and simplifying, gives the required result. That the majorization is sharp follows from @thm-ruit.
:::

If $f$ is even then $x=-y$ makes both sides of \eqref{eq-loc1} equal to zero.
Thus $\alpha$ has a stationary point at $-y$.

If in addition
$$
f''(y)<\frac{f'(y)}{y}
$${#eq-miny}
then $-y$ is a local maximum of $\alpha$. 

If $y>0$ then @eq-miny can be written as $yf''(y)-f'(y)<0$. 

Now the sign of the
derivative of $f'(x)/x$ is the sign of $xf''(x)-f'(x)$ and thus ... 
is equivalent to $f'(x)/x$ is decreasing for $x>0$.

::: {lem-decre}
On the positive real line $f'(x)/x$ is decreasing if and only if $$
f''(x)\leq\frac{f'(x)}{x}.
$$
:::
$$
\left(\frac{f'(x)}{x}\right)'=\frac{xf''(x)-f'(x)}{x^2}
$$
If $f'(x)/x$ is decreasing then $xf''(x)-f'(x)<0$. Thus if $x>0$ 
$f'(x)/x$ is decreasing is equivalent with 
$$
f''(x)\leq\frac{f'(x)}{x}
$$


### Mis


::: {#thm-wght}
Suppose 

1. $f$ is the primitive of a regulated function $h$ on $\mathbb{R}$
2. the ratio  $h(x)/x$ is non-increasing on $(0,\infty)$
3. $f$ is even

Then the even quadratic
$$
g(x)=\frac{f'(y)}{2y}(x^2-y^2)+f(y)\label{eq:sharp}
$$
is a sharp quadratic majorizer of $f$ at the point $y$.
:::

::: {.proof}
Suppose $0<y<x$. Then
\begin{align}
f(x)-f(y)&=\int_y^x h(z)dz=\notag\\
&=\int_y^x \frac{h(z)}{z}zdz\leq\frac{h(y)}{y}\int_y^xzdz=\notag\\
&=\frac12\frac{h(y)}{y}(x^2-y^2).
\end{align}
If $0<x<y$ then
\begin{align}
f(y)-f(x)&=\int_x^y h(z)dz=\\
&=\int_x^y \frac{h(z)}{z}zdz\geq\frac{h(y)}{y}\int_y^xzdz=\\
&=\frac12\frac{h(y)}{y}(y^2-x^2).
\end{align}
If $x<0<y$ then
$$
f(y)-f(x)=\int_x^y h(z)dz=
\int_x^0 h(z)dz+\int_0^y h(z)dz=
\int_0^{-x} h(z)dz+\int_0^y h(z)dz
\leq\frac{h(-x)}{-x}
$$
:::

::: {#thm-sqrt} 
The ratio $f'(x)/x$ is decreasing on $(0,\infty)$ if and only 
$f(\sqrt{x})$ is concave. The set of functions satisfying this condition is closed under the formation of (a) positive multiples, (b) convex combinations, (c) limits, and (d) composition with a concave increasing function $g(x)$.
:::

Note that these theorems give a sufficient condition for quadratic
majorization (in fact, for sharp quadratic majorization) and not a necessary one. Quadratic majorization, and even sharp quadratic majorization, may still be possible if the conditions in the theorem are violated.




We now apply @thm-wght to functions of the form
\begin{equation}
\sigma_f(X):=\sum \omega_k\ f(\delta_k-d_k(X)),\label{eq:fstressdef}
\end{equation}
where $f$ satisfies the conditions in the theorem. If
\begin{equation}
\omega_f(X):=\sum \omega_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),\label{eq:fstressmaj}
\end{equation}
then $\omega_f$ is a sharp quadratic majorization at $Y$.



In iteration $k$ the robust smacof algorithm does a smacof step towards minimization of
 $\omega_f$ over $X$. We can ignore the parts of \eqref{eq:fstressmaj} that only depend on $Y$, and 
minimize
\begin{equation}
\sum \omega_k(X^{(k)})(\delta_k-d_k(X))^2,\label{eq:fstressaux}
\end{equation}
with 
\begin{equation}
\omega_k(X^{(k)}):=\omega_k\frac{f'(\delta_k-d_k(X^{(k)}))}{2(\delta_k-d_k(Y))}.\label{eq:wkdef}
\end{equation}
It then recomputes the weights $\omega_k(X^{(k+1)})$ and goes to the smacof step again. This can be thought of as iteratively reweighted least squares (IRLS), and also as nested majorization, with the smacof majorization based on the Cauchy-Schwartz inequality within the sharp quadratic majorization of the loss function based on the AM/GM inequality.



\sectionbreak

# Power Smoothers

We first discuss a class of smoothers of the absolute value function that
maintain most of its structure. They have a shift parameter $c$ that takes
care of the non-differentiability. Although different smoothers have different scales and interpretations for $c$, we will use the same symbol throughout. Also some smoothers have a power 
parameter $q$ that determines the shape of the loss function bowl. 

## Charbonnier {#sec-charb}

The first, and perhaps most obvious, choice for smoothing the absolute
value function is
$$
f_c(x)=\sqrt{x^2 + c^2}.
$${#eq-charf}

For $c>0$ we have $f_c(x)>|x|$.
If $c\rightarrow 0$ then  $f_c(x)$ decreases monotonically to $|x|$. Also $\max_x|f_c(x)-|x||=c$ attained at $x=0$, which implies
uniform convergence of $f_c$ to $|x|$.

In the engineering literature @eq-gcharf is known as Charbonnier loss, after @charbonnier_blanc-feraud_aubert_barlaud_94, who were possibly
the first researchers to use it in image restauration. 
@ramirez_sanchez_kreinovich_argaez_14 count the number of elementary 
computer operations and argue @eq-gcharf is also the "most computationally efficient smooth approximation to $|x|$". 

```{r}
#| label: charfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: "Charbonnier Loss"
fcharbonnier <- function(x, c) {sqrt(x ^ 2 + c ^ 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1"), fun = fcharbonnier, args = list(c = 1))
base <- base + labs(x = "variable", y = "value")
base <- base + geom_function(aes(colour = "c = .1"), fun = fcharbonnier, args = list(c = .1)) 
base <- base + geom_function(aes(colour = "c = .01"), fun = fcharbonnier, args = list(c = .01)) 
base + geom_function(aes(colour = "c = .001"), fun = fcharbonnier, args = list(c = .001)) 
```


By l'Hôpital
\begin{subequations}
\begin{equation}
\lim_{x\rightarrow 0}\frac{\sqrt{x^2+c^2}-c}{\frac12x^2}=1.
\end{equation}
Of course also
\begin{equation}
\lim_{x\rightarrow\infty}\frac{\sqrt{x^2+c^2}}{|x|}=1
\end{equation}
and
\begin{equation}
\lim_{x\rightarrow\pm\infty}\sqrt{x^2+c^2}-|x|=0
\end{equation}
\end{subequations}
Thus if $x$ is much smaller than $c$ then loss is approximately a quadratic in $x$, 
and if $x$ is much larger than $c$ then loss is approximately the
absolute value. 

Loss function @eq-charf is infinitely many times differentiable. Its first derivative is
\begin{equation}
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,\label{eq:charg}
\end{equation}
which converges, again in the sup-norm and uniformly, to the sign function if $c\rightarrow 0$. The IRLS weights are
\begin{equation}
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}\label{eq:charw}
\end{equation}
which is clearly a decreasing function of $x$ on $\mathbb{R}^+$.

## Generalized Charbonnier

The loss function $(x^2+c^2)^\frac12$ smoothes $|x|$. In the same way generalized Charbonnier loss smoothes $\ell_p$ loss $|x|^q$. We have
a two-parameter family of loss functions in this case.
$$
f_{c,q}(x):=(x^2+c^2)^{\frac12 q}
$${#eq-gcharf}
\begin{equation}
w_{c,q}(x)=q(x^2+c^2)^{\frac12 q-1}\label{eq:gcharw}
\end{equation}
which is non-increasing for $q\leq 2$. Note that we do not assume that
$q>0$, and consequently generalized Charbonnier loss provides us with more flexibility than Charbonnier loss from @eq-charf. Of course if
$q<0$ "loss" becomes "gain", with a maximum at zero instead of a
minimum. To get a proper loss function, take the negative. @fig-gchar
plots generalized Charbonnier loss for some negative values of $q$.

```{r}
#| label: gchar
#| fig.align: center
#| echo: FALSE 
#| fig.cap: Generalized Charbonnier Loss
gfcharbonnier <- function(x, c, q) {
  s <- sqrt(x ^ 2 + c ^ 2) ^ q
  if (q < 0) return(1 - s)
  return(s)
}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1, q = -5"), fun = gfcharbonnier, args = list(c = 1, q = -5)) 
base <- base + geom_function(aes(colour = "c = 1, q = -1"), fun = gfcharbonnier, args = list(c = 1, q = -1)) 
base <- base + geom_function(aes(colour = "c = 1, q = -.5"), fun = gfcharbonnier, args = list(c = 1, q = -.5)) 
base + geom_function(aes(colour = "c = 1, q = -.1"), fun = gfcharbonnier, args = list(c = 1, q = -.1)) 
```

We see that for $\alpha\rightarrow-\infty$ generalized Charbonnier loss aproximates $\ell_0$ loss.

## Barron

There are a fair number of generalizations
of the power smoother loss functions in the engineering literature. We will discuss one nice generalization from @barron_19. 

$$
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).\
$${#eq-barron}

To quote Barron

>Here $\alpha\in\mathbb{R}$ is a shape parameter that controls the robustness of the loss and $c>0$ is a scale parameter that controls the size of the loss's quadratic bowl near $x=0$.

A number of interesting special cases of @eq-barron are obtained by selecting various values of the $\alpha$ parameters. For $\alpha=1$ it becomes Charbonnier loss, and for $\alpha=-2$ it is Geman-McClure loss.
There are also some limiting cases. For $\alpha\rightarrow 2$ Barron loss becomes squared error loss, for $\alpha\rightarrow 0$ it becomes Cauchy loss, and for $\alpha\rightarrow-\infty$ it becomes Welsch loss. Accordingly
$$
f'_{\alpha,c}(x)=\begin{cases}
\frac{x}{c^2}&\text{ if }\alpha=2,\\
\frac{2x}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{x}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{x}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
$${#eq-barroni}
and thus
$$
w_{\alpha,c}(x)=\begin{cases}
\frac{1}{c^2}&\text{ if }\alpha=2,\\
\frac{2}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{1}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{1}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
$${#eq-barronw}

```{r}
#| label: barronfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: "Barron Loss"
fbarron <- function(x, c, alpha) {
  f1 <- abs(alpha - 2) / alpha
  f2 <- (((x / c) ^ 2) / abs(alpha - 2) + 1) 
  return(f1 * (f2 ^ (alpha / 2) - 1))
}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1, alpha = 2"), fun = fbarron, args = list(c = 1, alpha = 1.999))
base <- base + labs(x = "variable", y = "value")
base <- base + geom_function(aes(colour = "c = 1, alpha = 1"), fun = fbarron, args = list(c = 1, alpha = 1))
base <- base + geom_function(aes(colour = "c = 1, alpha = 0"), fun = fbarron, args = list(c = .1, alpha = .001)) 
base <- base + geom_function(aes(colour = "c = 1, alpha = -2"), fun = fbarron, args = list(c = 1, alpha = -2)) 
base + geom_function(aes(colour = "c = 1, alpha = -10"), fun = fbarron, args = list(c = 1, alpha = -10))
```

\sectionbreak

# Convolution Smoothers

Suppose $\pi$ is a probability density, symmetric around zero, with finite or infinite support, expectation zero, and variance one. Define the convolution
$$
f_c(x):=\frac{1}{c}\int_{-\infty}^{+\infty}|x-y|\ \pi(\frac{y}{c})dy.
$$
Now $c^{-1}\pi(y/c)$ is still a symmetric probability density integrating to one, with expectation zero, but it now has variance $c^2$. Thus if $c\rightarrow 0$ it becomes more and more like the Dirac delta function and $f_c(x)$ converges to  the absolute value function.

It is  clear that we can use any scale family of probability densities to define convolution smoothers. There is an infinite number of possible choices, with finite or infinite support, smooth or nonsmooth, using splines or wavelets, and so on. We give two quite different examples.

## Huber

Take
$$
\pi(x)=\begin{cases}\frac12 &\text{ if }|x|\leq 1,\\0&\text{ otherwise.}\end{cases}
$$
Then
$$
f_c(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&\text{ if }|x|\leq c,\\
|x|&\text{ otherwise}.
\end{cases}
$${#eq-hubera}

The Huber function (@huber_64) is traditionally transformed linearly so that it is zero for $x=0$. This gives
$$
f_c(x)=\begin{cases}
\frac12x^2&\text{ if }|x|<c,\\
c|x|-\frac12 c^2&\text{ otherwise}.
\end{cases}
$${#eq-huberb}
For robust estimation and IRLS it does not matter if we use @eq-hubera or
@eq-huberb. Our discussion in the introduction suggests that if we just want
a smoother of the absolute value function, then @eq-hubera is the natural choice, if we want a robust loss function that combines the advantages of 
least squares and least absolute value then that leads us to @eq-huberb.

Because Charbonnier loss behaves the same way as Huber loss, as absolute value loss for large $x$ and as squared loss for small $x$, it is also known as Pseudo-Huber loss.

The Huber function is differentiable, although not twice diffentiable. Its derivative is
$$
f'(x)=\begin{cases}
c&\text{ if }x\geq c,\\
x&\text{ if }|x|\leq c,\\
-c&\text{ if }x\leq -c.
\end{cases}
$$
$$
\omega(x)=
\begin{cases}
\hfill\frac{c}{x}&\text{ if }x\geq c,\\
\hfill1&\text{ if }|x|\leq c,\\
-\frac{c}{x}&\text{ if }x\leq -c.
\end{cases}
$$
The Huber function is even and differentiable. Moreover
$f'(x)/x$ decreases from. Thus @{thm-wght} applies.

The MDS majorization algorithm for the Huber loss is to update $Y$ by 
minimizing (or by performing one smacof step to decrease)
$$
\sum \omega_k(Y)(\delta_k-d_k(X))^2
$$
where
$$
\omega_k(Y)=\begin{cases}
\omega_k&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac{c\omega_k}{|\delta_k-d_k(Y)|}&\text{ otherwise}.
\end{cases}
$$

```{r}
#| label: ghuberfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Huber Loss
fhuber <- function(x, c) {ifelse(abs(x) < c, (x ^ 2) / 2, c * abs(x) - ((c ^ 2) / 2))}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fhuber, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fhuber, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fhuber, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fhuber, args = list(c = .5))
```

## Gaussian

In @deleeuw_E_18f we also discussed the
convolution smoother proposed by @voronin_ozkaya_yoshida_15. The idea is
to use the convolution of the absolute value
function and a Gaussian pdf.
$$
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
$$

Carrying out the integration gives

$$
f_c(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
$$
The derivative is
$$
f'_c(x)=2\Phi(x/c)-1
$$
It may not be immediately obvious in this case that the weight function $f'(x)/x$ is non-increasing on $\mathbb{R}^+$. We prove that its
derivative is negative on $(0,+\infty)$. 
The derivative of $f'(x)/x$ has the sign of $xf''(x)-f'(x)$, which is
$z\phi(z)-\Phi(z)+1/2$, with $z=x/c$. It remains to show that $\Phi(z)-z\phi(z)\geq\frac12$,
or equivalently that $\int_0^z\phi(x)dx-z\phi(z)\geq 0$.
Now if $0\leq x\leq z$ then $\phi(x)\geq\phi(z)$ and thus $\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)$, which completes the proof.


$$
\omega_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
$$

```{r}
#| label: ggaussianfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Gaussian Convolution Loss
fgauss <- function(x, c) {x * (2 * pnorm(x / c) - 1) + 2 * c * dnorm(x / c)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fgauss, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fgauss, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fgauss, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fgauss, args = list(c = .5))
```



\sectionbreak

# A Bouquet of Loss Functions

In the early seventies, after the pioneering mostly theoretical work in robust statistics of Huber, Hampel, and Tukey, the mainframe computer allowed statisticians to make large-scale comparisons of many robust loss functions. The most impressive of such comparisons was the Princeton Robustness Study
(@andrews_bickel_hampel_huber_rogers_tukey_72).

In @holland_welsch_77 the computer package ROSEPACK was introduced that 
made it relatively easy to compute robust estimators using several different loss functions. Eight different weight functions were implemented as
options. Somewhat later @coleman_holland_kaden_klema_peters_80 made an
more modern computer implementation available, using the same eight weight
functions, which was not limited to mainframes.

We have implemented the same eight weight functions in smacofRobust. Below we give formulas for the loss function, the influence function, and the weight function. One of the eight is Huber loss, which we already discussed in the convolution section. We graph the remaining seven loss functions for selected values of the "tuning constants" $c$.

 @holland_welsch_77, following @andrews_bickel_hampel_huber_rogers_tukey_72,
 distnguish between "hard redescenders" that have an influence function $f'$
 equal to zero if $x$ is large enough (Andrews, Tukey, and Hinich loss),
 "soft redescenders" with influence functions asymptotic to zero for large $x$
 (Cauchy, Welsch loss), and loss functions with a monotone influence 
 function (Huber, Logistic, Fair loss)
 

## Andrews

The first loss function in this section is taken from 
@andrews_bickel_hampel_huber_rogers_tukey_72.

\begin{align}
f(x)&=\begin{cases}
c^2(1-\cos(x/c))&\text{ if }|x|\leq\pi c,\\
2c^2&\text{ otherwise.}
\end{cases}\\
f'(x)&=\begin{cases}
c\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}\\
\omega(x)&=\begin{cases}
(x/c)^{-1}\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}
\end{align}

Because $\cos$ is even and $\sin(x)/x$ decreases on $[0,\pi]$ @thm-wght  applies.

```{r}
#| label: gandrewsfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Andrews Loss
fandrews <- function(x, c) {ifelse(abs(x) < pi * c, 
                 (c ^ 2) * (1 - cos(x / c)), 
                 2 * (c ^ 2))}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fandrews, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fandrews, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fandrews, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fandrews, args = list(c = .5)) 
```


## Tukey 

The usual reference for Tukey loss is @beaton_tukey_74, although closely related hard redescenders are also in @andrews_bickel_hampel_huber_rogers_tukey_72.

\begin{align}
f(x)&=\begin{cases}
\frac{c^2}{6}\left(1-\left(1-(x/c)^2\right)^3\right)&\text{ if } |x|\leq c,\\
\frac{c^2}{6}&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
x\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
\omega(x)&=\begin{cases}
\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

The conditions of @thm-wght are clearly satisfied.

```{r}
#| label: gtukeyfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Tukey Loss
ftukey <- function(x, c) {ifelse(
    abs(x) < c, ((c ^ 2) / 6) * ( 1 - (1 - (x / c) ^ 2) ^ 3), 
                 (c ^ 2) / 6)}
base <- ggplot() + xlim(-2, 2)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = ftukey, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = ftukey, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = ftukey, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = ftukey, args = list(c = .5))
```


## Hinich

Hinich loss, from @hinich_talwar_75, is somewhat special because it is even but not differentiable at $c$. For $x\not= c$ and $x>0$ the function $f'(x)/x$
is non-increasing.

\begin{align}
f(x)&=\begin{cases}
\frac12 x^2&\text{ if } |x|\leq c,\\
\frac12 c^2&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
1&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
\omega(x)&=\begin{cases}
1/x&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

```{r}
#| label: ghinichfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Hinich Loss
fhinich <- function(x, c) {ifelse(abs(x) < c, (x ^ 2) / 2, (c ^ 2) / 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fhinich, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fhinich, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fhinich, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fhinich, args = list(c = .5))
```



## Cauchy 

Cauchy loss seems to have many names. @black_anandan_96 call it Lorentzian loss, and @holland_welsch_77 call it t-likelihood loss. It  is related to 
the Cauchy distribution, which is Student's t distribution with one degree of freedom.

@mlotshwa_vandeventer_bosman_23

\begin{align}
f(x)&=\frac12c^2\log(1+\{\frac{x}{c}\}^2),\\
f'(x)&=x\frac{1}{\{1+\frac{x}{c}\}^2},\\
\omega(x)&=\frac{1}{\{1+\frac{x}{c}\}^2}
\end{align}


```{r}
#| label: gcauchyfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Cauchy Loss
fcauchy <- function(x, c) {(c ^ 2) / 2 * log(1 + (x / c) ^ 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fcauchy, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fcauchy, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fcauchy, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fcauchy, args = list(c = .5))
```



## Welsch

@dennis_welsch_78

Leclerc loss

\begin{align}
f(x)&=\frac12c^2[1-\exp(-\{\frac{x}{c}\}^2)],\\
f'(x)&=x\exp(-\{\frac{x}{c}\}^2,\\
\omega(x)&=\exp(-\{\frac{x}{c}\}^2),
\end{align}

```{r}
#| label: gwelschfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Welsch Loss
fwelsch <- function(x, c) 1 - exp(-.5 * (x / c) ^ 2)
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fwelsch, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fwelsch, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fwelsch, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fwelsch, args = list(c = .5))
```


## Logistic

\begin{align}
f(x)&=c^2[\log(\cosh(x/c))],\\
f'(x)&=c\tanh(x/c),\\
\omega(x)&=(x/c)^{-1}\tanh(x/c).
\end{align}

```{r}
#| label: glogisticfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Logistic Loss
flogistic <- function(x, c) (c ^ 2) * log(cosh(x / c))
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = flogistic, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = flogistic, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = flogistic, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = flogistic, args = list(c = .5))
```

## Fair

\begin{align}
f(x)&=c^2\{|x|/c-\log(1+|x|/c)\},\\
f'(x)&=x(1+(|x|/c))^{-1},\\
\omega(x)&=(1+(|x|/c))^{-1}.
\end{align}

```{r}
#| label: gfairfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Fair Loss
ffair <- function(x, c) log((x / c) ^ 2 + 1)
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = ffair, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = ffair, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = ffair, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = ffair, args = list(c = .5))
```


\sectionbreak

# Examples

## Gruijter

The example we use are dissimilarities between nine Dutch political parties,
collected by @degruijter_67. They are averages over a politically heterogenous 
group of 100 introductory psychology students, and consequently they
regress to the mean. Any reasonable MDS analysis of these data would at
least allow for an additive constant.

Some background on Dutch politics around that time may be useful.

* CPN - Communists.
* PSP - Pacifists, left-wing.
* PvdA - Labour, Democratic Socialists.
* D'66 - Pragmatists, nether left-wing nor right-wing, brand new in 1967.
* KVP - Christian Democrats, catholic.
* ARP - Christian Democrats, protestant.
* CHU - Christian Democrats, protestant.
* VVD - Liberals, European flavour, conservative.
* BP - Farmers, protest party, right-wing.

The dissimilarities are in the table below.

```{r gruijter, echo = FALSE}
knitr::kable(delta)
```

The reason we have chosen this example is partly because CPN and BP are
outliers, and we can expect the robust loss functions to handle outlying
dissimilarities differently from the bulk of the data.

Unless otherwise indicated we run smacofRobust() with a maximum of
10,000 iterations, and we decide that we have convergence if the
difference between consecutive stress values is less than `r 1e-15`.
We perform one single smacof iteration between the updates of
the weights. For each analysis we show the configuration plot, the Shepard plot, and a histogram of the absolute values of the residuals. In the Shepard plot points corresponding to the eight CPN-dissimilarities are labeled "C", while BP-dissimilarities are "B".

### Least Squares


```{r}
#| label: ls
#| echo: FALSE
hls <- smacofRobust(delta, engine = smacofHuber, cons = 10, verbose = FALSE, itmax = 10000)
```
We start with a least squares analysis, actually with Huber loss with
$c=10$, which for these data is equivalent to least squares. The process converges in `r formatC(hls$itel, format = "d")` iterations.

```{r pxls, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Least Squares"}
df <- data.frame(dim1 = hls$x[, 1],
                 dim2 = hls$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```


```{r pdls, echo = FALSE, fig.align = "center", fig.cap = "Grujter Shepard Plot Least Squares"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hls$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hls$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hls$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

The Shepard plot clearly shows why an additive constant would be very beneficial in this case.

```{r phlsh, fig.align = "center", fig.cap = "Gruijter Histogram Least Squares Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hls$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Least Absolute Value

```{r av, echo = FALSE}
hav <- smacofRobust(delta, engine = smacofCharbonnier, cons = .001, verbose = FALSE, itmax = 100000, eps = 1e-15)
```
For our LAV smacof we use engine smacofCharbonnier with $c=.001$.
We have convergence in `r formatC(hav$itel, format = "d")` iterations.

```{r pxav, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Least Absolute Value"}
df <- data.frame(dim1 = hav$x[, 1],
                 dim2 = hav$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdav, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Least Absolute Value"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hav$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hav$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hav$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

In the Shepard plot we see that there are a number of dissimilarities which
are fitted exactly. If we count them there are about 15-20. Note that 
configurations in two dimensions have $(n-1)+(n-2)=2n-3$ degrees of freedom, which is
15 in this case. Thus if we take the 15 dissimilarities which are 
fitted exactly, give them weight one, give all other 21 dissimilarities
weight zero, and do a regular non-robust smacof analysis using these weights, then we will have perfect fit in two dimensions, and the solution will be
the LAV solution. All this is easier said than done,
because it presumes that we use Charbonnier loss with $c=0$ and that we are able to decide which residuals are exactly equal to zero. The LAV analysis also suggests the possibility of
a huge number of local minima,
because there are so many ways to pick 15 out of 36 dissimilarities.

```{r phavh, fig.align = "center", fig.cap = "Gruijter Histogram Least Absolute Value Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hav$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Huber

```{r hme, echo = FALSE}
hme <- smacofRobust(delta, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```
smacofHuber with $c=1$ converges in `r formatC(hme$itel, format = "d")` iterations.

```{r pxhme, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Huber c = 1"}
df <- data.frame(dim1 = hme$x[, 1],
                 dim2 = hme$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdhme, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Huber c = 1"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hme$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hme$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hme$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 12) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

```{r phmeh, fig.align = "center", fig.cap = "Gruijter Histogram Huber Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hme$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Tukey

```{r htu, echo = FALSE}
htu <- smacofRobust(delta, engine = smacofTukey, cons = 2, verbose = FALSE, itmax = 10000)
```
smacofTukey with $c=2$ converges in `r formatC(htu$itel, format = "d")` iterations.

```{r pxtmed, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Tukey c = 2"}
df <- data.frame(dim1 = htu$x[, 1],
                 dim2 = htu$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdtmed, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Tukey c = 2"}
df1 <- data.frame(
      delta = delta[jndex],
      d = htu$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = htu$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = htu$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

```{r phtuh, fig.align = "center", fig.cap = "Gruijter Histogram Tukey Residuals", echo = FALSE}
residuals <- abs(delta[iall] - htu$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

## Rothkopf

Our second example are the Rothkopf Morse data (@rothkopf_57), which have a better fit and have fewer outliers than the Gruijter data. We used the asymetric confusion matrix from the smacof package (@deleeuw_mair_A_09c) and defined dissimilarities by the Shepard-Luce formula
$$
\delta_{ij}=-\log\frac{p_{ij}p_{ji}}{p_{ii}p_{jj}}.
$$

### Least Squares


```{r mls, echo = FALSE}
jndex <- outer(1:36, 1:36, ">")
mhls <- smacofRobust(morse, engine = smacofHuber, cons = 25, verbose = FALSE, itmax = 10000)
```
For least squares we use the smacofHuber engine with $c=25$, well outside the range of the residuals. We have convergence in `r mhls$itel` iterations.


```{r mpxls, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Least Squares"}
df <- data.frame(dim1 = mhls$x[, 1],
                 dim2 = mhls$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mkpdls, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Least Squares"}
df <- data.frame(
      delta = morse[jndex],
      d = mhls$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphlsh, fig.align = "center", fig.cap = "Rothkopf Histogram Least Squares Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhls$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Least Absolute Value

```{r mav, echo = FALSE}
mhav <- smacofRobust(morse, engine = smacofCharbonnier, cons = .001, verbose = FALSE, itmax = 10000)
```
For least absolute value we use Chardonnier loss with $c=.001$. We have convergence in `r mhav$itel` iterations.

```{r mpxav, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Least Absolute Value"}
df <- data.frame(dim1 = mhav$x[, 1],
                 dim2 = mhav$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mpdav, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Least Absolute Value"}
df <- data.frame(
      delta = morse[jndex],
      d = mhav$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphlavh, fig.align = "center", fig.cap = "Rothkopf Histogram Least Absolute Value Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhav$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Huber

```{r mhme, echo = FALSE}
mhme <- smacofRobust(morse, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```

smacofHuber with $c=1$ converges in `r mhme$itel` iterations.

```{r mpxhme, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Huber c = 1"}
df <- data.frame(dim1 = mhme$x[, 1],
                 dim2 = mhme$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mpdhme, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Huber c = 1"}
df <- data.frame(
      delta = morse[jndex],
      d = mhme$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphmeh, fig.align = "center", fig.cap = "Rothkopf Histogram Huber Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhme$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Tukey


```{r mhtu, echo = FALSE}
mhtu <- smacofRobust(morse, engine = smacofTukey, cons = 1, verbose = FALSE, itmax = 10000)
```

Tukey with $c=1$ converges in `r mhtu$itel` iterations.


```{r mpxtu, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Tukey c = 1"}
df <- data.frame(dim1 = mhtu$x[, 1],
                 dim2 = mhtu$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdttu, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Tukey c = 1"}
df <- data.frame(
      delta = morse[jndex],
      d = mhtu$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphtuh, fig.align = "center", fig.cap = "Rothkopf Histogram Tukey Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhtu$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```
\sectionbreak

# Literature

The literature on results like @thm-wght and @thm-sqrt is difficult to review. There are various reasons for that. Relevant results have
been published in robust statistics, computational statistics, optimization, location analysis, image restoration, sparse recovery. As is often the case, there are not many references between fields, almost everything is within.
Even the names of the loss functions differ between fields.
Much of it is hard to find in conference proceedings. Also, in most cases, the authors have specific  applications in mind, which they then embed in a likelihood, Bayesian, linear regression, logistic regression, facility location, or EM framework and language.

@deleeuw_lange_A_09 give some references to previous work on results like  @thm-wght, notably @groenen_giaquinto_kiers_03, @jaakkola_jordan_00, and @hunter_li_05. In these earlier papers we do not find @thm-wght in its full
generality. In @groenen_giaquinto_kiers_03 majorization of the log logistic 
function is considered. Besides requiring equality of the function and the majorizing quadratic at the support point $y$ they also require equality
at $-y$ and then check that the resulting quadratic is indeed a majorizer.
In @jaakkola_jordan_00 also consider a symmetrized version of the log
logistic function. They note that the resulting function is a convex
funcion of $x^2$, and use a linear majorizer at $x^2$ to obtain a
quadratic majorization. @hunter_li_05 come closest to @thm-wght.
In their proposition 3.1 they approximate the general penalty function they use for varable selection at $y$ by a quadratic with coefficient $f'(y)/2y$, and
then show that it provides a quadratic majorization. In neither of the
three papers there is a notion of sharp quadratic majorization. 

I will discuss some of the literature under the headings "robust statistics", "location analysis", and "sparse recovery". Since I most definitely am not
an expert in either of these three fields the literature reviews will be biased and incomplete. A final section, where I am somewhat more sure-footed, is
"multivariate analysis".

## Robust Statistics
 
In robust statistics it has been known for a long time that iterative reweighted least squares (IRLS) with weights $f'(x)/x$ gives a quadratic majorization algorithm. This result, and the corresponding IRLS algorithm, is often attributed to @beaton_tukey_74.

## Location Analysis

In location analysis the first majorization/IRLS method is generally 
attributed to a 16-year old Hungarian mathematics prodigy (@weiszfeld_37). His 
algorithm avant-la-lettre was intended to minimize a function of
the form
$$
\sigma(x)=\sum_k \|x-y_k\|,
$${#eq-weiszfeld}
over $x$ in the plane or in three-space. The $y_i$ are known locations, called *anchors* in the literature, and the norm is Euclidean. 
There is an English translation of Weiszfeld's paper, with bibliography and comments, in @weiszfeld_plastria_09. The minimization of @eq-weiszfeld
is known under various names, usually consisting of
one of the seven different non-empty selections from the triple (Torricelli, Fermat, Weber). The history of the problem is discussed, for example, in @plastria_11.

Over the years the location problem has been
generalized in numerous directions, to multiple locations, to using different norms, to unknown anchors, to nonlinear manifolds, and to obnoxious anchors you want to be far from. A good recent overview is @beck_sabach_15. An paper close in spirit to our paper is @aftab_hartley_trumpf_15, which has generalizations to $\ell_q$ norms and to Riemannian manifolds of non-negative curvature.

There is a huge literature on the convergence of the Weiszfeld algorithm.
As in our @sec-amgm the basis is majorization based on the AM/GM inequality.
Thus
$$
\|x-y_\nu\|\leq\frac12\frac{1}{\|x^{(k)}-y_\nu\|}(\|x-y_\nu\|^2+\|x^{(k)}-y_\nu\|^2)
$${#eq-wmajw}
Thus the update is
$$
x^{(k+1)}=\frac{\sum\omega_\nu(x^{(k)})y_\nu}{\sum\omega_\nu(x^{(k)})}
$${#eq-wupdate}
with weights
$$
\omega_\nu(x)=\frac{1}{\|x-y_\nu\|}.
$${#eq-wweights}

Unlike robust smacof the Toricelli-Fermat-Weber problem is convex, 
and consequently has no problems with non-global local minima.
A most elegant proof of convergence using the tools of modern convex analysis is in @mordukhovich_nam_19. Older proofs sometimes have difficulty dealing with cases in which the iterates coincide with one of the anchors or in which the solution is actually one of the anchors. This creates problems similar to the problems in our @sec-zero, but in this simple case the problem is can be completely resolved using convexity and has no serious algorithmic consequences.

## Sparse Recovery

This is a field which is difficult to delineate. A somewhat ad-hoc definition is recovering complete information from incomplete information, often in the
context of specific engineering problems. There is overlap with signal detection, image analysis, matrix completion, ... But "sparse recovery" scientific activities "sparse recovery" could be extended far beyond these
boundaries. Since classical statistics infers properties of the population from those of a sample it is a form of sparse recovery. Since science infers properties of the real world from outcomes of experiments it is sparse recovery too.

## Multivariate Analysis

The smacof majorization method for multidimensional scaling was first presented
at the *US-Japan Seminar on Theory, Methods and Applications of Multidimensional Scaling and Related Techniques* at UCSD in La Jolla, August 1975. Shortly after that I read the basic EM paper by @dempster_laird_rubin_77,
and shirtky after that I realized that smacof and EM were both special
cases of a general minimization strategy, which I called majorization
at the time. In June 1978 both Nan Laird and I attended the *Fifth International Symposium on Multivariate Analysis* at the University of Pittsburgh. I remember mentioning majorization, excitedly, to Nan on the conference bus. 

The smacof majorization method was fully discussed in @deleeuw_C_77, @deleeuw_heiser_C_77, and @deleeuw_heiser_C_80. The familiar picture illustrating two steps of the general majorization algorithm appears in @deleeuw_A_88b. But unlike EM the general idea of majorization remained unpublished, until @deleeuw_C_94c and @heiser_95. 

Majorization was used regularly in the
Gifi project. The book @gifi_B_90, which is a version of 1981 lecture
notes, mentions majorization only once, but since then a stream of
papers and dissertations using majorization appeared. @heiser_95 
briefly mentions most of them. In @deleeuw_C_88b another large majorization
subfield, the *aspect approach* to multivariate analysis, was developed.
In section 7 of that paper the general majorization/minorization approach to optimization is outlined, maybe for the first time in print.

Robust versions of low rank matrix approximation, a.k.a. principal component
analysis, were first considered by @gabriel_odoroff_84. They start by
discussing the alternating least squares algorithm for least squares weighted matrix approximation of @gabriel_zamir_79. The alternating is to compute new row scores for currently fixed column scores by linear regression, and then computing new column scores corresponding with the new row scores, again by
linear regression. @gabriel_odoroff_84 suggest to replace the linear least
squares weighted averages in each of the two stages by medians or trimmed
means to get a robust PCA. There is no sign of a convergence proof, but there
is the suggestion to use alternating least absolute value methods to minimize
the sum of absolute residuals of the matrix approximation. This suggestion
was taken up by @verboon_heiser_94 using the majorization approach and the
Huber and Tukey robust loss functions. Their robust PCA method is very similar 
to our robust MDS method, but the presentation of their method has
some magical elements. The Huber and Tukey majorization functions are
presented without any discussion where they came from, and it is then verified
that they are indeed majorizations. There is clearly nothing wrong with this, but using our @thm-wght gives a more general and more direct approach.

@heiser_86 was the first to connect the Weiszfeld problem with correspondence analysis and multidimensional scaling, emphasizing the majorization aspects.
As we have seen in @heiser_87 and @heiser_88 he constructed majorization
algorithms for multidimensional scaling and correspondence analysis.

The IRLS approach to robustifying multivariate matrix approximation techniques could easily lead to a large and varied number of publications. There are some excellent examples making their way through the usual publication channels. I will just give two recent examples, with good bibliographies. They are Huber Principal Component Analysis (@he_li_liu_zhou_23) and Cauchy Factor Analysis (@li_24).

\sectionbreak

# Discussion

## Bounding the Second Derivative

In some cases our basic theorems may not apply, but there may be an alternative
way to majorize loss. In fact, this is classic quadratic bounding as in @vosz_eckhardt_80 or @boehning_lindsay_88. As before, we want to minimize
$\sum \omega_k\ f(\delta_k-d_k(X))$,
but now we suppose that there is a $K>0$ such that $f''(x)\leq K$. We then have
the majorization

$$
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
$${#eq-qbound}
and in iteration $k$ we minimize, or at least decrease, 
$$
\sum \omega_k\left[d_k(X)-\{d_k(X^{(k)})-K^{-1}f'(\delta_k-d_k(X^{(k)}))\}\right]^2
$${#eq-qiter}
Note that in this algorithm the weights do not change.  Instead
of fitting a fixed target with moving weights, we fit a
moving target with fixed weights.

We can apply bounding the second derivative, for example, to Charbonnier loss, using the inequality
$$
f_c''(x)=(x^2 + c^2)^{-\frac12}-x^2(x^2 + c^2)^{-\frac32}\leq(x^2 + c^2)^{-\frac12}\leq c^{-1},
$${#eq-chark}
Of course this method requires that the second derivative exists at $x$.
Although I have not done any comparisons it will probably require more
iterations and take longer than the method in @sec-charb.

The paper by @vosz_eckhardt_80 deserves some special mention here.

$$
\mathcal{D}^2\sigma(x)=\sum\frac{1}{d_i(x)}\left\{I-\frac{(x-y_i)(x-y_i)'}{d_i^2(x)}\right\}
$$

## Fixed Weights

One could also consider using the fixed weights in regular non-robust smacof
to achieve some form of robustness. Redefine stress as
$$
\sigma(X):=\sum_k \omega_kf(\delta_k)(\delta_k-d_k(X))^2
$${#eq-wstress}
For example, we can choose a negative power for $f$, so that it downweights the large dissimilarities. If the dissimilarities is large, then it should have less influence on the fit, and thus on the solution $X$. This type of fixed power-weighting is used in various places (@deleeuw_heiser_C_80, @groenen_vandevelden_16) to approximate loss functions such the one with logarithmic residuals in @ramsay_77.

But we have to keep in mind that downweighting large dissimilarities
is not the same thing as downweighting large residuals. The residuals
depend on $X$, and it is perfectly possible that some small dissimilarities
have large residuals. On the other hand emphasizing small dissimilarities
in the loss function means that we want small dissimilarities to be
fitted relatively well, which means that on average we want small dissimilarities to have small residuals. The Shepard plot will tend to 
fan out at the high end.

Despite these reservations, it will be useful to study if and how fixed weights can be used to improve robustness of smacof. If only because fixed weights correspond with a simpler and presumably more efficient algorithm.

## Residual Definition

In our examples and in our code we use the residuals $\delta_k-d_k(X)$ are
arguments of our loss functions. From the statistical point of view we have to remember, however, that most of these loss functions were designed for the
robust estimation of a location parameter or a linear regression function.
The error distributions were explicitly or implicitly assumed to be symmetric
around zero, and defined on the whole real line, which was reflected in the fact that loss functions were even and had infinite support.
In MDS, however, distances and dissimilarities are non-negative and reasonable
error functions are not symmetric. One could follow the example of @ramsay_77 and measure residuals as $\log\delta_{ij}-\log d_{ij}(X)$. This does not have
any effect on the majorization of the loss functions, but it means that in
the smacof step to find $X^{(k+1)}$ we have to minimize
$$
\sigma(X)=\sum \omega_k(X^{(k)})(\log\delta_{ij}-\log d_{ij}(X))^2,
$$
which is considerably more complicated (@deleeuw_groenen_mair_E_16a).

## Robust Nonmetric MDS

Our discussion and our software is all about metric MDS. It seems easy to
extend the discussion to non-linear and non-metric MDS by adding an
alternating least squares step optimally scaling the dissimilarities.
This would take place between two majorizations of the robust loss
function, so one or more transformation and smacof steps can be taken
between updating the weights. But this paradigm does not work for
robust smacof. 

Consider any hard redescender, such as Tukey or Hinich. At iteration $\nu$,
for current weights, first first improve the configuration, then compute the optimal transformation of the dissimilarities, and then compute new weights.
This is a recipe for disaster. At some point we minimize
$$
\sigma(\hat d)=\sum \omega_k(X^{(\nu)})(\hat d_k-d_k(X^{(\nu+1)}))^2
$${#eq:zero}
over the disparities $\hat d$, which must be monotone with the dissimilarities.
Because of the hard redescending some of the weights, for current
absolute residuals larger than $c$, will be zero. The monotone
regression is done for the observations with non-zero weights,
and the disparities corresponding with zero weights are
only determined by the order they are required to have. Thus
they can be freely chosen in an interval between two disparities
obtained from the monotone regression. That interval can be large, in
fact if one of the zero weights corresponds with the largest
dissimilarity it can be infinite. What we choose in the
interval will determine the new residual and thus the next set 
of weights. 

In the unfortunate situation that the current absolute 
residuals are all larger than $c$, even after choosing
the optimal $\hat d$, the next weights will all be zero
and the algorithm stops with zero stress.

## Practicalities

Recommending one particular loss function from the many we have discussed is
not easy. In some cases, for example for Cauchy loss, one can justify the
choice of a loss function by assuming a particular error distribution
and using the maximum likelihood principle. But in general perhaps the best way to proceed for a given MDS problem is
to take what we could call a *trajectory approach*. Choose one particular
parametric family, for example the Huber one, and compute the robust
smacof solution $X(c)$ for a number of increasing positive $c$ values. For small $c$ we start with a close approximation of the LAV solution, increasing
$c$ will eventually take us to the LS solution. The starting point for
computing each solution will be the solution for the previous $c$. 
We can plot the trajectory of the points in the configurations $X(c)$,
and even make an animation. It seems that the Huber family is a good
candidate for such a study, with the generalized Charbonnier a good
second. If the main concern is to suppress the influence of outliers
then trying some of the hard redescenders, such as the Tukey family,
makes sense. Studying trajectories for some of robust loss functions is clearly interesting, but it is not something we can or will explore in this paper.



\sectionbreak

# Code

The function smacofRobust has a parameter "engine", which can be equal
to smacofCharbonnier, smacofGeneralizedCharbonnier, smacofBarron, smacofHuber, smacofTukey, smacofHinnich, smacofCauchy, smacofFair, smacofAndrews, smacofLogistic, smacofWelsch, or smacofGaussian. These thirteen small modules compute the respective loss function values and
weights for the IRLS procedure. This makes it easy for interested parties to add additional robust loss functions.

```{r code, eval = FALSE}
{{< include smacofRobust.R >}}
```

\sectionbreak

# References
