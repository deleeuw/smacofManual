---
title: "Robust Least Squares Multidimensional Scaling"
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: We use an iteratively reweighted version of the smacof
  algorithm to minimize various robust multidimensional scaling 
  loss functions. Our results use a general theorem on sharp 
  quadratic majorization of @deleeuw_lange_A_09. We relate this
  theorem to earlier results in robust statistics, location
  theory, and sparse recovery. Code in R is included. 
---

\sectionbreak

\listoffigures

\sectionbreak


```{r gruijterdata, echo = FALSE}
gruijter <-
  structure(
    c(
      5.63,
      5.27,
      4.6,
      4.8,
      7.54,
      6.73,
      7.18,
      6.17,
      6.72,
      5.64,
      6.22,
      5.12,
      4.59,
      7.22,
      5.47,
      5.46,
      4.97,
      8.13,
      7.55,
      6.9,
      4.67,
      3.2,
      7.84,
      6.73,
      7.28,
      6.13,
      7.8,
      7.08,
      6.96,
      6.04,
      4.08,
      6.34,
      7.42,
      6.88,
      6.36,
      7.36
    ),
    names = c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66"),
    Size = 9L,
    call = quote(as.dist.default(m = polpar)),
    class = "dist",
    Diag = FALSE,
    Upper = FALSE
  )
delta <- as.matrix(gruijter)
names <- c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66")
row.names(delta) <- colnames(delta) <- names
index <- matrix(1:81, 9, 9)
jndex <- index[-c(6, 8), -c(6, 8)][outer(1:7, 1:7, "<")]
icpn <- c(46:50, 52:54)
ibp <- c(64:70, 72)
iall <- c(jndex, icpn, ibp)
data(morse2, package = "smacof")
morse2 <- 1 - morse2
morse2 <- ifelse(morse2 == 0, .01, morse2)
dm <- diag(morse2)
morse <- -log((morse2 * t(morse2)) / (outer(dm, dm)))
kndex <- matrix(1:1296, 36, 36)[outer(1:36, 1:36, "<")]
```

```{r loadcode, echo = FALSE}
source("smacofRobust.R")
```


# Introduction

The title of this paper is something paradoxical. Least squares estimation
is typically not robust, it is sensitive to outliers and pays a lot of attention to fitting the larger observations. What we mean by robust least squares MDS, however, is using the smacof machinery designed to minimize least squares loss functions of the form
\begin{equation}
\sigma_2(X):=\sum w_k(\delta_k-d_k(X))^2\label{eq:stressdef},
\end{equation}
to minimize robust loss functions. The prototypical robust loss function is 
least absolute value loss
\begin{equation}
\sigma_1(X):=\sum w_k|\delta_k-d_k(X)|\label{eq:stradddef},
\end{equation}
which we will call
*strife*, because stress, sstress, and strain are already taken.

Strife is not differentiable at configurations $X$ for which there is at least one $k$ for which either $d_k(X)=\delta_k$ or $d_k(X)=0$ (or both). This lack of differentiability complicates the minimization problem. Moreover experience
with one-dimensional and city block MDS suggests that having many points
where the loss function is not differentiable leads to (many) additional local minima.

In this paper we will discuss (and implement) various variations of 
$\sigma_1$ from \eqref{eq:stradddef}. They can be interpreted in two different
ways. On the one hand we use smoothers of the absolute value
function, and consequently of strife. We want to eliminate the
problems with differentiability, at least the ones caused by
$\delta_k=d_k(X)$. If this is our main goal, then we want to choose
the smoother in such a way that it is as close to the absolute value
function as possible. This is not unlike the distance
smoothing used by @pliner_96 and @groenen_heiser_meulman_99 in 
the global minimization of $\sigma_2$ from \eqref{eq:stressdef}.

On the other hand our modified loss function can be interpreted
as more robust versions of the least squares loss function, and
consequently of stress. Our goal here is to combines the robustness of the
absolute value function with the efficiency and computational ease 
of least squares. If that is our goal then there is no reason to
stay as close to the absolute value function as possible.

Our robust or smooth loss functions are all of the form
\begin{equation}
\sigma(X):=\sum w_k\ f_c(\delta_k-d_k(X))\label{eq:strifedef},
\end{equation}
for a suitable choice of the real valued function $f$. We will
define what we mean by "suitable" later on. The subscript $c$
of $f_c$ is meant to indicate that the loss function may depend
on one or more real-valued tuning parameters $c$ that regulate the degree
of smoothness and/or robustness. For now, note that loss
\eqref{eq:stressdef} is the special case with $f_c(x)=x^2$ and loss \eqref{eq:stradddef}is the special case with $f_c(x)=|x|$. There is no tuning in both these cases.

\sectionbreak

# Majorizing Strife {#sec-majorization}

The idea of minimizing a least absolute value (LAV) to obtain parameter estimates dates back to the work of Boskovitch in the middle of the
eighteenth century. Until recently it has been applied mainly to fit
linear models, so that we can actually use standard linear programming
algorithms to obtain optimal solutions.

The pioneering work in strife minimization in MDS using smacof is @heiser_88, building on earlier work in @heiser_87. It is based on a creative use of the Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of the absolute
value function. For the general theory of majorization algorithms (now more commonly known as MM algorithms) we refer to their original introduction in @deleeuw_C_94c and to the excellent book by @lange_16.

The AM/GM inequality says that for all non-negative $x$ and $y$ we have 
\begin{equation}
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),\label{eq:amgm}
\end{equation}
with equality if and only if $x^2=y^2$. If $y>0$ we can write \eqref{eq:amgm} as
\begin{equation}
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\label{eq:amgmmaj}
\end{equation}
and this provides a quadratic majorization of $|x|$ at $y$. There is no quadratic
majorization of $|x|$ at $y=0$, which is a nuisance we will have to deal with.

Using the majorization \eqref{eq:amgmmaj}, and assuming $\delta_k\not=d_k(Y)$ for all $k$, we define
\begin{equation}
\omega_1(X):=\frac12\sum w_k\frac{1}{|\delta_k-d_k(Y)|}((\delta_k-d_k(Y))^2+(\delta_k-d_k(X))^2).\label{eq:omegadef}
\end{equation}
Now $\sigma_1(X)\leq\omega_1(X)$ for all $X$ and $\sigma_1(Y)=\omega_1(Y)$, and thus
$\omega_1$ majorizes $\sigma_1$ at $Y$.

## Algorithm 

Define
\begin{equation}
w_k(Y):=w_k\frac{1}{|\delta_k-d_k(Y)|}.\label{eq:wk1def}
\end{equation}
Reweighted smacof to minimize strife computes $X^{(k+1)}$ by decreasing
\begin{equation}
\sum w_k(X^{(k)})(\delta_k-d_k(X^{(k)}))^2,\label{eq:sstrf}
\end{equation}
using a standard smacof step. It then computes the new weights $w_k(X^{(k+1)})$ from \eqref{eq:wk1def}
and uses them in the next smacof step to update $X^{(k+1)}$. And so on, until convergence.

A straightforward variation of the algorithm does a number of smacof steps before
upgrading the weights. This still leads to a monotone, and thus convergent, algorithm.
How many smacof steps we have to take in the inner iterations is something that needs further study. It is likely to depend on the fit of the data, on the shape of the function near the local minimum, and on how far the iterations are from the local minimum.

## Zero Residuals

It may happen that for some $k$ we have $d_k(X^{(k)})=\delta_k$ while iterating. There have been various proposals to deal with such an 
unfortunate event, and we will discuss some of them further on. Even more
importantly we will see that that the minimizer of the absolute value
loss usually satisfies $d_k(X)=\delta_k$ for quite a few elements,
which means that near convergence the algorithm will become unstable
because the weights from \eqref{eq:wk1def} become very large. 

A large number of somewhat ad-hoc solutions have been proposed to deal with the problem of zero residuals, both in location analysis and in the statistical literature. We tend to agree with the assessment of @aftab_hartley_15.

> .. attempts to analyze this difficulty [caused by infinite weights of IRLS for the $\ell_p$-loss] have a long history of proofs and counterexamples to incorrect claims.

@schlossmacher_73 is the first discussion of the majorization method in the statistical literature (for LAV linear regression). His proposal is to simply set a weight equal to zero if the corresponding residual is
less than some small positive value $\epsilon$. A similar approach, also used in location analysis, is to cap the weights at some large positive value.
In @heiser_88 all residuals smaller than this epsilon get a weight equal to the weighted average of all these small residuals. @phillips_02 assumes double-exponential
errors in LAV regression and then concludes that the EM algorithm gives the 
majorization method we have discussed. He uses \eqref{eq:wk1def} throughout if all residuals are larger than $\epsilon$. If one of more residuals are smaller than epsilon then the weight for those residuals is set equal to one, while for the remaining residuals the weight is set to epsilon divided by the absolute value of the
residual. Often we get the assurance in these papers that the problem is not really important in practice, because it is very rare, and by just wiggling we will get to the unique solution anyway. But both in location analysis and in 
LAV regression the loss function is convex, however, which guarantees a unique minimum. This is certainly not the case in robust MDS. In this paper we try to follow a more systematic approach that uses smooth parametric approximations to the absolute value function, where the parameter can be used to make the 
approximation as precise as necessary.

To illustrate the problems with differentiability we compute the directional derivatives of strife. 

Let $s_k(X):=w_k|d_k(X)-\delta_k|$.

1. If $\delta_k=0$ and $d_k(X)=0$ then $ds_k(X;Y)=w_kd_k(Y)$.
2. If $\delta_k>0$ and $d_k(X)=0$ then $ds(X;Y)=-w_kd_k(Y)$.
3. If $d_k(X)>0$ and $d_k(X)-\delta_k>0$ then 
$ds_k(X;Y)=w_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
4. If $d_k(X)>0$ and $d_k(X)-\delta_k<0$ then $ds_k(X;Y)=-w_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
5. If $d_k(X)>0$ and $d_k(X)-\delta_k=0$ then $ds_k(X;Y)=w_k\frac{1}{d_k(X)}|\text{tr}\ X'A_kY|$.

The directional derivative of $\sigma_1$ is consequently the sum of
five terms, corresponding with each of these five cases.

In the case of stress the directional derivatives could be used to prove
that if $w_k\delta_k>0$ for all $k$ then stress is differentiable at each local minimum (@deleeuw_A_84f). For strife to be differentiable we would have to prove that at a local minimum both $d_k(X)>0$ and $(d_k(X)-\delta_k)\not= 0$. 
for all $k$ with $w_k>0$.

But this is impossible by the following argument. In the one-dimensional
case we can partition $\mathbb{R}^n$ into $n!$ polyhedral convex cones
corresponding with the permutations of $x$. Within each cone the distances
are a linear function of $x$. Each cone can be partitioned by intersecting it
with the $2^\binom{n}{2}$ polyhedra defined by the inequalities $\delta_k-d_k(x)\geq 0$ or $\delta_k-d_k(x)\leq 0$. Some of these intersections can and will obviously be empty. Within each of these non-empty polyhedral regions strife is a linear function of $x$. Thus it attains its minimum at a vertex of the region, which is a solution for which some
distances are zero and some residuals are zero. There can be no  
minima, local or global, in the interior of one of the polyhedral regions.
We have shown that in one dimension strife is not differentiable at a local minimum, and that there is presumably a large number of them. Of course even for moderate $n$ the number of regions, which is maximally $n!2^\binom{n}{2}$, 
is too large to actually compute or draw. 

In the multidimensional case linearity goes out the window. The
set of configurations $d_k(X)=\delta_k$ is an ellipsoid and $d_k(X)=0$ is a hyperplane. Strife is not differentiable at all intersections of these
ellipsoids and hyperplanes. The partitioning of $\mathbb{R}^n$ by these
ellipsoids and hyperplanes is not simple to describe. It has convex
and non-convex cells, and within each cell strife is the difference
of two weighted sums of distances. Anything can happen.

## $\ell_0$ loss

A somewhat extreme special case of Equation \eqref{eq:strifedef} has
$$
f(x)=\begin{cases}
0&\text{ if }x = 0,\\
1&\text{ otherwise}.
\end{cases}
$$
This is $\ell_0$ loss. Minimizing $\ell_0$ loss means maximizing the number 
of cases with perfect fit, i.e. with $\delta_k=d_k(X)$. The reason we
mention it here is that the work of @donoho_elad_03 and @candes_tao_05   suggests that the minimizer of $\ell_1$ loss, i.e. absolute value loss, gives a good approximation to the minimizer of $\ell_0$ loss, at least in a number of
special cases. In MDS we do not have linearity or convexity, but nevertheless
the results are suggestive. By computing the directional derivatives we have
seen that at least in the one-dimensional MDS case a number of residuals
will indeed be zero at the optimum LAV solution.

There is an excellent review of the use of $\ell_1$ in various sparse recovery fields in @candes_wakin_boyd_08. In that paper they also propose an iteratively
reweighted LAV algorithm, which solves $\ell_1$ problems between weight updates. Maybe because of that they go so far as calling $\ell_1$ "the modern least squares". But let's not get carried away, in actual ease and frequency of use $\ell_1$ still has a long way to go if it wants to replace $\ell_2$.

\sectionbreak

# Generalizing Strife

We have seen that @heiser_88 applied majorization to minimize strife, using the AM/GM inequality. We now generalize this approach so that it can easily deal with other robust loss functions. A great number of loss functions will appear
below. The intention is not to confuse the reader by requiring them to choose
from this large number of alternatives with rather limited information. We show all these loss functions as examples of the general principle of algorithm construction and as examples of loss functions that have been used in
statistics, location analysis, image analysis and engineering over the years.
They are all implemented in the function smacofRobust(), written in R (@r_core_team_24).

## Majorization

First some definitions. 

::: {#def-majorize}
A function $g$ *majorizes* a function $f$ at $y$ if $g(x)\geq f(x)$ for all $x$ and $g(y)=f(y)$. Majorization is *strict* if $g(x)>f(x)$ for all $x\not= y$.
:::
 
::: {#def-sharp}
If $\mathfrak{H}$ is a family of functions that all majorize $f$ at $y$ then $h\in\mathfrak{H}$ is a *sharp majorization* in $\mathfrak{H}$ if $h(x)\leq g(x)$ for all $g\in\mathfrak{H}$. The sharp majorization is by definition unique.
:::

### Sharp Quadratic Majorization

The AM/GM inequality was used in @sec-majorization to construct a quadratic majorization of strife. In this section we are specifically interested in this paper in sharp quadratic majorization, in
which $\mathfrak{H}$ is the set of all convex quadratics that majorize $f$ at $y$.
This case has been studied in detail (in the case of real-valued functions on the line) by @deleeuw_lange_A_09. Their Theorems 4.5 and 4.6 on pages 2478-2479 say

::: {#thm-wght}
Suppose $f(x)$ is an even, differentiable function on $\mathbb{R}$ such that the ratio 
$f'(x)/x$ is non-increasing on $(0,\infty)$. Then the even quadratic
\begin{equation}
g(x)=\frac{f'(y)}{2y}(x^2-y^2)+f(y)\label{eq:sharp}
\end{equation}
is a sharp quadratic majorizer of $f$ at the point $y$.
:::

::: {#thm-sqrt} 
The ratio $f'(x)/x$ is decreasing on $(0,\infty)$ if and only 
$f(\sqrt{x})$ is concave. The set of functions satisfying this condition is closed under the formation of (a) positive multiples, (b) convex combinations, (c) limits, and (d) composition with a concave increasing function $g(x)$.
:::

Note that these theorems give a sufficient condition for quadratic
majorization (in fact, for sharp quadratic majorization) and not a necessary one. Quadratic majorization may still be possible if the conditions in the theorem are violated.

We can get a considerable more general version of theorem @thm-wght by using
the framework for real analysis in @bourbaki_76, @bourbaki_04 (cf also
@dieudonne_69, sections 7.6 and 8.7).

::: {#def-regulated}
A real-valued function on a closed interval $[a,b]$ is *regulated* if it has a limit from the right on $[a,b)$ and a limit from the left on $(a,b]$.
:::

Note that the end-points of the interval can be $\pm\infty$. Regulated functions have limits from the right and the left at every
point in the interior of $(a,b)$. They can only have discontinuities of the first kind (jump discontinuities). Step functions, monotone functions, and continuous functions are all regulated functions. An alternative definition is that regulated functions are limits of sequences of step functions, where the convergence is uniform on compact sets. 

::: {#def-primitive}
A real-valued function $f$ on $[a,b]$ is a *primitive* of a real-valued function $g$ on $[a,b]$ if $f$ is differentiable with $f'(x)=g(x)$, except possibly at a denumerable number of points.
:::

A regulated function has at least one primitive, and primitives are unique
up to addition of a constant function. The primitive $f$ of a continuous function $g$ is differentiable everywhere, with $f'(x)=g(x)$.

::: {#def-integral}
The *integral* of a regulated function $f$ on $[a,b]$, written as $\int_a^b f(x)dx$, is equal to $g(b)-g(a)$, where $g$ is one of the primitives of $f$.
:::

::: {#thm-dll}
Suppose $g$ is a regulated function on $[a,b]$ and $f$ is one of its primitives. Define $h(z):=g(z)/z$ if $z\not= 0$ and $h(0)=0$.
Assume $h(z)\geq A$ on $[a,b]$. Then for all $x,y$ in $[a,b]$ we have the quadratic majorization 
$$
f(x)\leq f(y)+\frac12 A(x^2-y^2)
$${#eq-amajor1}
of $f$ at $y$.
:::
::: {.proof}
$f(y)-f(x)=\int_x^y g(z)dz=\int_x^yh(z)zdz\leq \frac12A(y^2-x^2)$.
:::

::: {#cor-dcor}
Suppose $g$ is a regulated function on $[a,y]$ and $f$ is one of its primitives. Define $h(z):=g(z)/z$ if $z\not= 0$ and $h(0)=0$ and assume $h$ is non-increasing on $[a,y]$. Then
we have the quadratic majorization
$$f(x)\leq f(y)+\frac12h(y)(x^2-y^2).$${#eq-amajor}
of $f$ at $y$.
:::

Suppose $g$ is the sign function, which is obviously regulated, and 
$f$ is the absolute value function, a primitive of $g$. Then

We now apply @thm-wght to functions of the form
\begin{equation}
\sigma_f(X):=\sum w_k\ f(\delta_k-d_k(X)),\label{eq:fstressdef}
\end{equation}
where $f$ satisfies the conditions in the theorem. If
\begin{equation}
\omega_f(X):=\sum w_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),\label{eq:fstressmaj}
\end{equation}
then $\omega_f$ is a sharp quadratic majorization at $Y$.

### Two-point Quadratic Majorization

Sometimes sharp quadratic majorizations can be found by using the results 
by @vanruitenburg_05 on quadratic majorization with two (or more) support
points. We generalize his results by introducing the concept of a majorizing fan (@deleeuw_E_18i).

::: {#def-fan}
A majorizing fan $\mathfrak{G}$ of $f$ at $y$ is a set of functions that
majorize $f$ at $y$ and that have the property that for each pair
of functions in $\mathfrak{G}$ one of them strictly majorizes the
other. 
:::

Note that we do not assume differentiability of $f$, or even continuity.
Examples of fans are in @deleeuw_E_18i.

::: {#lem-first}
Suppose $g,h$ are in a majorizing fan of $f$ at $y$. 
Suppose, in addition, that $g$ majorizes $f$ at $z\not=y$
and $h$ majorizes $f$ at $v\not= y$. Then $g=h$. 
:::
::: {.proof}
For if $g\not= h$ there are two possibilities. Either $g$ strictly majorizes $h$ at $y$ or $h$ strictly majorizes $g$ at $y$. In the first case  $h(z)<g(z)=f(z)$ and thus $h$ does not majorize $f$. In the second case
$g(v)<h(v)=f(v)$ and thus $g$ does not majorize $f$. Thus $g=h$.
:::

::: {#thm-ruit}
Suppose $g\not= h$ are in a majorizing fan of $f$ at $y$. 
Suppose, in addition, that $g$ majorizes $f$ at $z\not=y$
Then $h$ strictly majorizes $g$ at $y$. 
:::
::: {.proof}
For if $g$ strictly majorizes $h$ then $h(z)<g(z)=f(z)$
and thus $h$ does not majorize $f$.
:::

Thus, by @thm-ruit, if we have a majorizer $g$ of $f$ in a
fan $\mathfrak{G}$ at $y$ that also majorizes $f$ and $z\not= y$, then
$g$ is a strict majorizer in $\mathfrak{G}$ of $f$ at $y$.

Now suppose $f$ is differentiable at $y$ and $\mathfrak{G}$ is the
set of all quadratic functions that majorize $f$ at $y$. Then $\mathfrak{G}$
is a majorizing fan of $f$ at $y$.



If $f(x)=|x|$ then
\begin{equation}
g(x)=\frac{1}{2|y|}(x^2-y^2)+|y|=\frac{1}{2|y|}(x^2+y^2),\label{eq:abssharp}
\end{equation}
which is the same as \eqref{eq:amgmmaj}. Thus the AM/GM method gives the sharp
quadratic majorization.

In iteration $k$ the robust smacof algorithm does a smacof step towards minimization of
 $\omega_f$ over $X$. We can ignore the parts of \eqref{eq:fstressmaj} that only depend on $Y$, and 
minimize
\begin{equation}
\sum w_k(X^{(k)})(\delta_k-d_k(X))^2,\label{eq:fstressaux}
\end{equation}
with 
\begin{equation}
w_k(X^{(k)}):=w_k\frac{f'(\delta_k-d_k(X^{(k)}))}{2(\delta_k-d_k(Y))}.\label{eq:wkdef}
\end{equation}
It then recomputes the weights $w_k(X^{(k+1)})$ and goes to the smacof step again. This can be thought of as iteratively reweighted least squares (IRLS), and also as nested majorization, with the smacof majorization based on the Cauchy-Schwartz inequality within the sharp quadratic majorization of the loss function based on the AM/GM inequality.

## Literature

The literature on results like @thm-wght and @thm-sqrt is an absolute 
shambles. There are various reasons for that. Relevant results have
been published in robust statistics, comptational statistics, optimization, location theory, image restortation, sparse recovery. There are
not many references between fields, almost everything is within.
Moreover much of it is hidden in the usual caves of engineering
conference proceedings. Also, in most cases, the authors have specific 
applications in mind, which they then embed in a likelihood, Bayesian, linear regression, facility location, or EM framework and language.

@deleeuw_lange_A_09 give some references to previous work on results like  @thm-wght, notably @groenen_giaquinto_kiers_03, @jaakkola_jordan_00, and @hunter_li_05. In these earlier papers we do not find @thm-wght in its full
generality. In @groenen_giaquinto_kiers_03 majorization of the log logistic 
function is considered. Besides requiring equality of the function and the majorizing quadratic at the support point $y$ they also require equality
at $-y$ and then check that the resulting quadratic is indeed a majorizer.
In @jaakkola_jordan_00 also consider a symmetrized version of the log
logistic function. They note that the resulting function is a convex
funcion of $x^2$, and use a linear majorizer at $x^2$ to obtain a
quadratic majorization. @hunter_li_05 come closest to @thm-wght.
In their proposition 3.1 they approximate the general penalty function used in varable selection at $y$ by a quadratic with coefficient $f'(y)/2y$, and
then show that it provides a quadratic majorization. In neither of the
three papers there is a notion of sharp quadratic majorization. 
 

@vanruitenburg_05

In robust statistics it has been known for a long time that iterative reweighted least squares with weights $f'(x)/x$ gives a quadratic majorization algorithm.

\sectionbreak

# Power Smoothers

We first discuss a class of smoothers of the absolute value function that
maintain most of its structure. They have a shift parameter $c$ that takes
care of the non-differentiability. And some of them also have a power 
parameter $q$ that determines the shape of the loss function bowl.

## Charbonnier

The first, and perhaps most obvious, choice for smoothing the absolute
value function is
$$
f_c(x)=\sqrt{x^2 + c^2}.
$${#eq-charf}
This smoother was previously used by @deleeuw_E_18f in LAV regression and in @deleeuw_E_20b in what was called least squares absolute value regression.

In @fig-charfig we show the loss function for $c=1$ (black), $c=0.1$ (red),
$c=0.01$ (blue), and $c=0.001$ (green).

```{r}
#| label: fig-charfig
#| fig.align: center
#| echo: FALSE 
#| fig.cap: Charbonnier Loss
#| fig.asp: 1
#| fig.dim: c(8, 8)
par(pty = "s")
curve(sqrt(x^2 + 1), from = -3, to = 3, ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.1), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.01), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.001), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 4))
```

For $c>0$ we have $f_c(x)>|x|$.
If $c\rightarrow 0$ then  $f_c(x)$ decreases monotonically to $|x|$. Also $\max_x|f_c(x)-|x||=c$ attained at $x=0$, which implies
uniform convergence of $f_c$ to $|x|$.

In the engineering literature @eq-charf is known as Charbonnier loss, after @charbonnier_blanc-feraud_aubert_barlaud_94, who were possibly
the first researchers to use it in image restauration. 
@ramirez_sanchez_kreinovich_argaez_14 count the number of elementary 
computer operations and argue \eqref{eq:charf} is also the "most computationally efficient smooth approximation to $|x|$". 

By l'Hôpital
\begin{subequations}
\begin{equation}
\lim_{x\rightarrow 0}\frac{\sqrt{x^2+c^2}-c}{\frac12x^2}=1.
\end{equation}
Of course also
\begin{equation}
\lim_{x\rightarrow\infty}\frac{\sqrt{x^2+c^2}}{|x|}=1
\end{equation}
and
\begin{equation}
\lim_{x\rightarrow\pm\infty}\sqrt{x^2+c^2}-|x|=0
\end{equation}
\end{subequations}
Thus if $x$ is much smaller than $c$ then loss is approximately a quadratic in $x$, 
and if $x$ is much larger than $c$ then loss is approximately the
absolute value. 

Loss function \eqref{eq:charf} is infinitely many times differentiable. Its first derivative is
\begin{equation}
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,\label{eq:charg}
\end{equation}
which converges, again in the sup-norm and uniformly, to the sign function if $c\rightarrow 0$. The IRLS weights are
\begin{equation}
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}\label{eq:charw}
\end{equation}
which is clearly a decreasing function of $x$ on $\mathbb{R}^+$.

## Generalized Charbonnier

The loss function $(x^2+c^2)^\frac12$ smoothes $|x|$. In the same way generalized 
Charbonnier loss smoothes $\ell_p$ loss $|x|^q$.
\begin{equation}
f_{c,q}(x):=(x^2+c^2)^{\frac12 q}\label{eq:gcharf}
\end{equation}
\begin{equation}
w_{c,q}(x)=q(x^2+c^2)^{\frac12 q-1}\label{eq:gcharw}
\end{equation}
which is non-increasing for $q\leq 2$. Note that we do not assume that
$q>0$, and consequently \eqref{eq:gcharf} provides us with much more flexibility than Charbonnier loss \eqref{eq:charf}.

In figure @fig-gcharfig we have plotted \eqref{eq:gcharf} for
$c=1$ and $\alpha$  equal to $-5$ (black), $-1$ (red), $-.5$ (blue),
and $-.1$ (green). 

```{r}
#| label: fig-gcharfig
#| fig.align: center
#| echo: FALSE 
#| fig.cap: Generalized Charbonnier Loss
#| fig.asp: 1
#| fig.dim: c(8, 8)
par(pty = "s")
curve((x^2 + 1) ^ (-5), from = -3, to = 3, ylab = "f", ylim = c(0, 1))
curve((x^2 + 1) ^ (-1), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 1))
curve((x^2 + 1) ^ (-.5), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 1))
curve((x^2 + 1) ^ (-.1), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 1))
```

We see that for $\alpha\rightarrow-\infty$ generalized Charbonnier loss aproximates $\ell_0$ loss.

## Barron

There are a large number of generalizations
of the power smoother types of loss functions in the engineering community, and in their maze of conference publications. We will discuss one nice generalization in @barron_19. 

\begin{equation}
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).\label{eq:barron}
\end{equation}

To quote Barron

>Here $\alpha\in\mathbb{R}$ is a shape parameter that controls the robust-ness of the loss and $c>0$ is a scale parameter that controls the size of the loss’s quadratic bowl near $x=0$.

There are a number of interesting special cases of \eqref{eq:barron} by selecting various values of the $\alpha$ parameters. For $\alpha=1$ it becomes Charbonnier loss, and for $\alpha=-2$ it is Geman-McClure loss.
There are also some limiting cases. For $\alpha\rightarrow 2$ Barron loss becomes squared error loss, for $\alpha\rightarrow 0$ it becomes Cauchy loss, and for $\alpha\rightarrow-\infty$ it becomes Welsch loss.

Thus
\begin{equation}
f_{\alpha,c}(x)=\begin{cases}
\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).
\end{cases}
\end{equation}
\sectionbreak

# Convolution Smoothers

Suppose $\pi$ is a probability density, symmetric around zero, with finite or infinite support, expectation zero, and variance one. Define the convolution
$$
f_c(x):=\frac{1}{c}\int_{-\infty}^{+\infty}|x-y|\ \pi(\frac{y}{c})dy.
$$
Now $c^{-1}\pi(y/c)$ is still a symmetric probability density integrating to one, with
expectation zero, but it now has variance $c^2$. Thus if $c\rightarrow 0$ it becomes more and more like the Dirac delta function and $f_c(x)$ converges to 
the absolute value function.

It is  clear that we can use any scale family of probability densities to define convolution smoothers. There is an infinite number of possible choices, with finite or infinite support, smooth or nonsmooth, using splines or wavelets, and so on. We give two quite different examples.

## Huber

Take
$$
\pi(x)=\begin{cases}\frac12 &\text{ if }|x|\leq 1,\\0&\text{ otherwise.}\end{cases}
$$
Then
$$
f_c(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&\text{ if }|x|\leq c,\\
|x|&\text{ otherwise}.
\end{cases}
$${#eq-hubera}

The Huber function (@huber_64) is traditionally written as
\begin{equation}
f_c(x)=\begin{cases}
\frac12x^2&\text{ if }|x|<c,\\
c|x|-\frac12 c^2&\text{ otherwise}.
\end{cases}
\end{equation}
which gives the same weight function as @eq-hubera (up to a multiplicative 
constant). Because Charbonnier loss behaves the same way as Huber loss, as absolute value loss for large $x$ and as squared loss for small $x$, it is also known as Pseudo-Huber loss.

The Huber function is differentiable, although not twice diffentiable. Its derivative is
$$
f'(x)=\begin{cases}
c&\text{ if }x\geq c,\\
x&\text{ if }|x|\leq c,\\
-c&\text{ if }x\leq -c.
\end{cases}
$$
$$
w(x)=
\begin{cases}
\hfill\frac{c}{x}&\text{ if }x\geq c,\\
\hfill1&\text{ if }|x|\leq c,\\
-\frac{c}{x}&\text{ if }x\leq -c.
\end{cases}
$$
The Huber function is even and differentiable. Moreover
$f'(x)/x$ decreases from. Thus @{thm-wght} applies.

The MDS majorization algorithm for the Huber loss is to update $Y$ by 
minimizing (or by performing one smacof step to decrease)
$$
\sum w_k(Y)(\delta_k-d_k(X))^2
$$
where
$$
w_k(Y)=\begin{cases}
w_k&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac{cw_k}{|\delta_k-d_k(Y)|}&\text{ otherwise}.
\end{cases}
$$

## Gaussian

In @deleeuw_E_18f we also discussed the
convolution smoother proposed by @voronin_ozkaya_yoshida_15. The idea is
to use the convolution of the absolute value
function 
and a Gaussian pdf.
$$
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
$$

Carrying out the integration gives

$$
f_c(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
$$
The derivative is
$$
f'_c(x)=2\Phi(x/c)-1
$$
It may not be immediately obvious in this case that the weight function $f'(x)/x$ is non-increasing on $\mathbb{R}^+$. We prove that its
derivative is negative on $(0,+\infty)$. 
The derivative of $f'(x)/x$ has the sign of $xf''(x)-f'(x)$, which is
$z\phi(z)-\Phi(z)+1/2$, with $z=x/c$. It remains to show that $\Phi(z)-z\phi(z)\geq\frac12$,
or equivalently that $\int_0^z\phi(x)dx-z\phi(z)\geq 0$.
Now if $0\leq x\leq z$ then $\phi(x)\geq\phi(z)$ and thus $\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)$, which completes the proof.


$$
w_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
$$




\sectionbreak

# A Bouquet of Loss Functions

In the early seventies, after the pioneering mostly theoretical work in robust statistics of Huber, Hampel, and Tukey, the mainframe computer allowed statisticians to make large-scale comparisons of many robust loss functions. The most impressive of such comparisons was the Princeton Robustness Study
(@andrews_bickel_hampel_huber_rogers_tukey_72).

In @holland_welsch_77 the computer package ROSEPACK was introduced that 
made it relatively easy to compute robust estimators using several different loss functions. Eight different weight functions were implemented as
options. Somewhat later @coleman_holland_kaden_klema_peters_80 made an
more modern computer implementation available, using the same eight weight
functions, which was not limited to mainframes.

We have implemented the same eight weight functions in smacofRobust. Below we give formulas for the loss function, the influence function, and the weight function. One of the eight is Huber loss, which we already discussed in the convolution section. We graph the remaining seven loss functions for selected values of the "tuning constants" $c$.

 @holland_welsch_77, following @andrews_bickel_hampel_huber_rogers_tukey_72,
 distnguish between "hard redescenders" that have an influence function $f'$
 equal to zero if $x$ is large enough (Andrews, Tukey, and Hinich loss),
 "soft redescenders" with influence functions asymptotic to zero for large $x$
 (Cauchy, Welsch loss), and loss functions with a monotone influence 
 function (Huber, Logistic, Fair loss)
 

## Andrews

\begin{align}
f(x)&=\begin{cases}
c^2(1-\cos(x/c))&\text{ if }|x|\leq\pi c,\\
2c^2&\text{ otherwise.}
\end{cases}\\
f'(x)&=\begin{cases}
c\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}\\
w(x)&=\begin{cases}
(x/c)^{-1}\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}
\end{align}

```{r}
#| label: andrewsfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Andrews Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
fandrews <- function(x, c) {ifelse(abs(x) < pi * c, 
                 (c ^ 2) * (1 - cos(x / c)), 
                 2 * (c ^ 2))}
curve(fandrews(x, 3), from = -3, to = 3, ylab = "f", ylim = c(0, .6))
curve(fandrews(x, 2), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 6))
curve(fandrews(x, 1), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 6))
curve(fandrews(x, .5), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 6))
```


## Tukey 

\begin{align}
f(x)&=\begin{cases}
\frac{c^2}{6}\left(1-\left(1-(x/c)^2\right)^3\right)&\text{ if } |x|\leq c,\\
\frac{c^2}{6}&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
x\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
w(x)&=\begin{cases}
\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

```{r}
#| label: tukeyfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Andrews Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
fandrews <- function(x, c) {ifelse(abs(x) < pi * c, 
                 (c ^ 2) * (1 - cos(x / c)), 
                 2 * (c ^ 2))}
curve(fandrews(x, 3), from = -1, to = 1, ylab = "f", ylim = c(0, .6))
curve(fandrews(x, 2), from = -1, to = 1, add = TRUE, col = "RED", ylab = "f", ylim = c(0, .6))
curve(fandrews(x, 1), from = -1, to = 1, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, .6))
curve(fandrews(x, .5), from = -1, to = 1, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, .6))
```


## Hinich

\begin{align}
f(x)&=\begin{cases}
\frac12 x^2&\text{ if } |x|\leq c,\\
\frac12 c^2&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
1&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
w(x)&=\begin{cases}
1/x&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

```{r}
#| label: hinichfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Hinich Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
fhinich <- function(x, c) {ifelse(abs(x) < c, (x ^ 2) / 2, (c ^ 2) / 2)}
curve(fhinich(x, 3), from = -3, to = 3, ylab = "f", ylim = c(0, 1))
curve(fhinich(x, 2), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 1))
curve(fhinich(x, 1), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 1))
curve(fhinich(x, .5), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 1))
```



## Cauchy 

\begin{align}
f(x)&=\frac12c^2\log(1+\{\frac{x}{c}\}^2),\\
f'(x)&=x\frac{1}{\{1+\frac{x}{c}\}^2},\\
w(x)&=\frac{1}{\{1+\frac{x}{c}\}^2}
\end{align}

```{r}
#| label: cauchyfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Cauchy Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
fcauchy <- function(x, c) {(c ^ 2) / 2 * log(1 + (x /c) ^ 2)}
curve(fcauchy(x, 3), from = -3, to = 3, ylab = "f", ylim = c(0, .6))
curve(fcauchy(x, 2), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 6))
curve(fcauchy(x, 1), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 6))
curve(fcauchy(x, .5), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 6))
```



## Welsch

\begin{align}
f(x)&=\frac12c^2[1-\exp(-\{\frac{x}{c}\}^2)],\\
f'(x)&=x\exp(-\{\frac{x}{c}\}^2,\\
w(x)&=\exp(-\{\frac{x}{c}\}^2),
\end{align}

```{r}
#| label: welschfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Welsch Loss
#| fig.dim: c(8, )
#| fig.asp: 1
fwelsch <- function(x, c) 1 - exp(-.5 * (x / c) ^ 2)
curve(fwelsch(x, 2), from = -3, to = 3, ylab = "f", ylim = c(0, 1))
curve(fwelsch(x, 1), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 1))
curve(fwelsch(x, .5), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 1))
curve(fwelsch(x, .1), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 1))
```


## Logistic

\begin{align}
f(x)&=c^2[\log(\cosh(x/c))],\\
f'(x)&=c\tanh(x/c),\\
w(x)&=(x/c)^{-1}\tanh(x/c).
\end{align}

```{r}
#| label: logisticfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Logistic Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
flogistic <- function(x, c) (c ^ 2) * log(cosh(x / c))
curve(flogistic(x, .6), from = -3, to = 3, ylab = "f", ylim = c(0, .6))
curve(flogistic(x, .4), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 6))
curve(flogistic(x, .2), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 6))
curve(flogistic(x, .1), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 6))
```

## Fair

\begin{align}
f(x)&=c^2\{|x|/c-\log(1+|x|/c)\},\\
f'(x)&=x(1+(|x|/c))^{-1},\\
w(x)&=(1+(|x|/c))^{-1}.
\end{align}

```{r}
#| label: fairfig
#| fig.align: center
#| echo: FALSE
#| fig.cap: Fair Loss
#| fig.dim: c(8, 8)
#| fig.asp: 1
ffair <- function(x, c) log((x / c)^2 + 1)
curve(ffair(x, 3), from = -3, to = 3, ylab = "f", ylim = c(0, .6))
curve(ffair(x, 2), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 6))
curve(ffair(x, 1), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 6))
curve(ffair(x, .5), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 6))
```


\sectionbreak

# Examples

## Gruijter

The example we use are dissimilarities between nine Dutch political parties,
collected by @degruijter_67. They are averages over a politically heterogenous 
group of 100 introductory psychology students, and consequently they
regress to the mean. Any reasonable MDS analysis of these data would at
least allow for an additive constant.

Some background on Dutch politics around that time may be useful.

* CPN - Communists.
* PSP - Pacifists, left-wing.
* PvdA - Labour, Democratic Socialists.
* D'66 - Pragmatists, nether left-wing fish nor right-wing flesh, brand new in 1967.
* KVP - Christian Democrats, catholic.
* ARP - Christian Democrats, protestant.
* CHU - Christian Democrats, protestants, different flavor.
* VVD - Liberals, European flavour, right-wing.
* BP - Farmers, protest party, right-wing.

The dissimilarities are in the table below.

```{r gruijter, echo = FALSE}
delta
```

The reason we have chosen this example is partly because CPN and BP are
outliers, and we can expect the robust loss functions to handle outlying
dissimilarities differently from the bulk of the data.

Unless otherwise indicated we run smacofRobust() with a maximum of
10,000 iterations, and we decide that we have comvergence if the
difference between consecutive stress values is less than `r 1e-15`.
We perform a single smacof iteration between the updates of
the weights. For each analysis we show the configuration plot and the Shepard plot. In the Shepard plot points corresponding to the eight CPN-dissimilarities arelabeled "C", while BP-dissimilarities are "B".

### Least Squares


```{r}
#| label: ls
#| echo: FALSE
hls <- smacofRobust(delta, engine = smacofHuber, cons = 10, verbose = FALSE, itmax = 10000)
```
We start with a least squares analysis, actually with Huber loss with
$c=10$, which for these data is equivalent to least squares. The process converges in `r formatC(hls$itel, format = "d")` iterations.

```{r pxls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Squares"}
par(pty = "s")
plot(hls$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hls$x, names, col = "RED")
```

```{r pdls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Squares"}
par(pty = "s")
plot(delta[jndex], hls$d[jndex], xlab = "delta", ylab = "d", col = "RED", xlim = c(0,10), ylim = c(0,10))
abline(0, 1)
points(delta[icpn], hls$d[icpn], col = "BLUE", pch = "C")
points(delta[ibp], hls$d[ibp], col = "BLUE", pch = "B")
```

The Shepard plot clearly shows why an additive constant would be very beneficial in this case.
```{r phlsh, fig.align = "center", fig.cap = "Histogram Least Squares Residuals", echo = FALSE}
hist(abs(delta[iall] - hls$d[iall]), main = "", xlab = "absolute residual")
```

### Least Absolute Value

```{r av, echo = FALSE}
hav <- smacofRobust(delta, engine = smacofAV, cons = .001, verbose = FALSE, itmax = 100000, eps = 1e-15)
```
For our LAV smacof we use engine smacofAV with $c=.001$.
We have convergence in `r formatC(hav$itel, format = "d")` iterations.

```{r pxav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Absolute Value"}
par(pty = "s")
plot(hav$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hav$x, names, col = "RED")
```

```{r pdav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Absolute Value"}
par(pty = "s")
plot(delta[jndex], hav$d[jndex], xlab = "delta", ylab = "d", col = "RED", xlim = c(0,10), ylim = c(0,10))
abline(0, 1)
points(delta[icpn], hav$d[icpn], col = "BLUE", pch = "C")
points(delta[ibp], hav$d[ibp], col = "BLUE", pch = "B")
```

In the Shepard plot we see that there are a number of dissimilarities which
are fitted exactly. If we count them there are about 15. Note that 
configurations have $(n-1)+(n-2)=2n-3$ degrees of freedom, which is
15 in this case. Thus if we take the 15 dissimilarities which are 
fitted exactly, give them weight one, give all other 21 dissimilarities
weight zero, and do a smacof analysis using these weights, then we
will have perfect fit in two dimensions, and the solution will be
the LAV solution. All this is easier said than done,
because it presumes Charbonnier loss with $c=0$ and the ability to
decide what exact equality is. It also shows the possibility of
a huge number of local minima in the LAV case,
because there are so many ways to pick 15 out of 36 dissimilarities.

```{r phavh, fig.align = "center", fig.cap = "Histogram Least Absolute Value  Residuals", echo = FALSE}
hist(abs(delta[iall] - hav$d[iall]), main = "", xlab = "absolute residual")
```

### Huber

```{r hme, echo = FALSE}
hme <- smacofRobust(delta, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```
smacofHuber with $c=1$ converges in `r formatC(hme$itel, format = "d")` iterations.

```{r pxhme, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Huber c = 1"}
par(pty = "s")
plot(hme$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hme$x, names, col = "RED")
```

```{r pdhme, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Huber c = 1"}
par(pty = "s")
plot(delta[jndex], hme$d[jndex], xlab = "delta", ylab = "d", col = "RED", xlim = c(0,10), ylim = c(0,10))
abline(0, 1)
points(delta[icpn], hme$d[icpn], col = "BLUE", pch = "C")
points(delta[ibp], hme$d[ibp], col = "BLUE", pch = "B")
```

```{r phmeh, fig.align = "center", fig.cap = "Histogram Huber Residuals", echo = FALSE}
hist(abs(delta[iall] - hme$d[iall]), main = "", xlab = "absolute residual")
```

### Tukey

```{r htu, echo = FALSE}
htu <- smacofRobust(delta, engine = smacofTukey, cons = 2, verbose = FALSE, itmax = 10000)
```
Converges in `r formatC(htu$itel, format = "d")` iterations.

```{r pxtmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Tukey c = 2"}
par(pty = "s")
plot(htu$x, type = "n", xlab = "dim1", ylab = "dim2")
text(htu$x, names, col = "RED")
```

```{r pdtmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Tukey c = 2"}
par(pty = "s")
plot(delta[jndex], htu$d[jndex], xlab = "delta", ylab = "d", col = "RED", xlim = c(0,10), ylim = c(0,10))
abline(0, 1)
points(delta[icpn], htu$d[icpn], col = "BLUE", pch = "C")
points(delta[ibp], htu$d[ibp], col = "BLUE", pch = "B")
```

```{r phtuh, fig.align = "center", fig.cap = "Histogram Tukey Residuals"}
hist(abs(delta[iall] - htu$d[iall]), main = "", xlab = "absolute residual")
```

## Rothkopf

Our seond example are the Rothkopf Morse data (@rothkopf_57), which are better
behaved as the Gruijter data. We used the asymetric confusion matrix and defined
a dissimilarity by the Shepard-Luce like formula
$$
\delta_{ij}=-\log\frac{p_{ij}p_{ji}}{p_{ii}p_{jj}}.
$$
The fivenum for these data is `r fivenum(morse[kndex])`.

### Least Squares


```{r mls, echo = FALSE}
mhls <- smacofRobust(morse, engine = smacofHuber, cons = 25, verbose = FALSE, itmax = 10000)
```
For least squares we use the smacofHuber engine with $c=25$, well outside the range of the residuals. We have convergence in `r mhls$itel` iterations.

```{r mpxls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Squares"}
par(pty = "s")
plot(mhls$x, type = "n", xlab = "dim1", ylab = "dim2")
text(mhls$x, row.names(morse))
```

```{r mkpdls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Squares"}
par(pty = "s")
plot(morse, mhls$d, xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r mphlsh, fig.align = "center", fig.cap = "Histogram Least Squares Residuals", echo = FALSE}
hist(abs(morse[kndex] - mhls$d[kndex]), main = "", xlab = "absolute residual")
```

### Least Absolute Value

Chardonnier with $c=.001$, de facto least absolute value loss.

```{r mav, echo = FALSE}
mhav <- smacofRobust(morse, engine = smacofAV, cons = .001, verbose = FALSE, itmax = 10000)
```
For least absolute value we use Chardonnier loss with $c=.001$. We have convergence in `r mhav$itel` iterations.

```{r mpxav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Absolute Value"}
par(pty = "s")
plot(mhav$x, type = "n", xlab = "dim1", ylab = "dim2")
text(mhav$x, row.names(morse))
```

```{r mpdav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Absolute Value"}
par(pty = "s")
plot(morse[kndex], mhav$d[kndex], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```
```{r mphlavh, fig.align = "center", fig.cap = "Histogram Least Absolute Value Residuals", echo = FALSE}
hist(abs(morse[kndex] - mhav$d[kndex]), main = "", xlab = "absolute residual")
```
### Huber

Huber with $c=1$

```{r mhme, echo = FALSE}
mhme <- smacofRobust(morse, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```

```{r mpxhme, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Huber"}
par(pty = "s")
plot(mhme$x, type = "n", xlab = "dim1", ylab = "dim2")
text(mhme$x, row.names(morse))
```

```{r mpdhme, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Huber"}
par(pty = "s")
plot(morse[kndex], mhme$d[kndex], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r mphmeh, fig.align = "center", fig.cap = "Histogram Huber Residuals", echo = FALSE}
hist(abs(morse[kndex] - mhme$d[kndex]), main = "", xlab = "absolute residual")
```

### Tukey

Tukey with $c=1$.

```{r mhtu, echo = FALSE}
mhtu <- smacofRobust(morse, engine = smacofTukey, cons = 1, verbose = FALSE, itmax = 10000)
```

```{r mpxtu, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Median Tukey"}
par(pty = "s")
plot(mhtu$x, type = "n", xlab = "dim1", ylab = "dim2")
text(mhtu$x, row.names(morse))
```

```{r pdttu, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Median Tukey"}
par(pty = "s")
plot(morse[kndex], mhtu$d[kndex], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r mphtuh, fig.align = "center", fig.cap = "Histogram Tukey Residuals", echo = FALSE}
hist(abs(morse[kndex] - mhtu$d[kndex]), main = "", xlab = "absolute residual")
```
\sectionbreak

# Discussion

## Fixed weights

## Bounding the Second Derivative

Minimize
$$
\sum w_k\ f(\delta_k-d_k(X))
$$
if $f''(x)\leq K$.

$$
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
$$
Minimize
$$
\sum w_k\left[d_k(X)-\{d_k(X^{(k)})-K^{-1}f'(\delta_k-d_k(X^{(k)}))\}\right]^2
$$
$$
f_c''(x)=(x^2 + c^2)^{-\frac12}-x^2(x^2 + c^2)^{-\frac32}\leq(x^2 + c^2)^{-\frac12}\leq c^{-1}.
$$
Thus
$$
f(x)\leq f(y)+f'(y)(x-y)+\frac{1}{2c}(x-y)^2=f(y)+\frac{1}{2c}(x-(y-cf'(y)))^2
$$

## Residual Choice

In our examples and in our code we use the residuals $\delta_k-d_k(X)$ are
arguments of our loss functions. From the statistical point of view we have to remember, however, that most of these loss functions were designed for the
robust estimation of a location parameter or a linear regression function.
The error distributions were explicitly or implicitly assumed to be symmetric
around zero, and defined on the whole real line, which was reflected in the fact that loss functions were even and had infinite support.
In MDS, however, distances and dissimilarities are non-negative and reasonable
error functions are not symmetric. One could follow the example of @ramsay_77 and measure residuals as $\log\delta_{ij}-\log d_{ij}(X)$. This does not have
any effect on the majorization of the loss functions, but it means that in
the smacof step to find $X^{(k+1)}$ we have to minimize
$$
\sigma(X)=\sum w_k(X^{(k)})(\log\delta_{ij}-\log d_{ij}(X))^2,
$$
which is considerably more complicated (@deleeuw_groenen_mair_E_16a).

## Robust Nonmetric MDS



\sectionbreak

# Code

The function smacofRobust has a parameter "engine", which can be equal
to smacofAV, smacofHuber, smacofTukey, or smacofConvolution. These
four small modules compute the respective loss function values and
weights for the IRLS procedure. This makes it easy to add additional robust
loss functions.

```{r code, eval = FALSE}
{{< include smacofRobust.R >}}
```


# References
