---
title: "Robust Least Squares Multidimensional Scaling"
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: Combining different loss functions with linear models and minimizing   loss with iteratively reweighted least squares (IRLS) has a long history in
  robust statistics. In this paper we use an IRLS version of the smacof
  algorithm to minimize various robust multidimensional scaling loss functions.
  Our results use a general theorem on sharp quadratic majorization of
  @deleeuw_lange_A_09. We relate this theorem to earlier results in robust
  statistics, location theory, and sparse recovery. Code in R is included. 
---

\sectionbreak

\listoffigures

\sectionbreak


```{r gruijterdata, echo = FALSE}
gruijter <-
  structure(
    c(
      5.63,
      5.27,
      4.6,
      4.8,
      7.54,
      6.73,
      7.18,
      6.17,
      6.72,
      5.64,
      6.22,
      5.12,
      4.59,
      7.22,
      5.47,
      5.46,
      4.97,
      8.13,
      7.55,
      6.9,
      4.67,
      3.2,
      7.84,
      6.73,
      7.28,
      6.13,
      7.8,
      7.08,
      6.96,
      6.04,
      4.08,
      6.34,
      7.42,
      6.88,
      6.36,
      7.36
    ),
    names = c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66"),
    Size = 9L,
    call = quote(as.dist.default(m = polpar)),
    class = "dist",
    Diag = FALSE,
    Upper = FALSE
  )
delta <- as.matrix(gruijter)
names <- c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66")
row.names(delta) <- colnames(delta) <- names
index <- matrix(1:81, 9, 9)
jndex <- index[-c(6, 8), -c(6, 8)][outer(1:7, 1:7, "<")]
icpn <- c(46:50, 52:54)
ibp <- c(64:70, 72)
iall <- index[outer(1:9, 1:9, "<")]
data(morse2, package = "smacof")
morse2 <- 1 - morse2
morse2 <- ifelse(morse2 == 0, .01, morse2)
dm <- diag(morse2)
morse <- -log((morse2 * t(morse2)) / (outer(dm, dm)))
kndex <- matrix(1:1296, 36, 36)[outer(1:36, 1:36, "<")]
```

```{r loadcode, echo = FALSE}
library(ggplot2)
source("smacofRobust.R")
```


# Introduction {#sec-intro}

The title of this paper is somewhat surprising. Least squares estimation
is typically not robust, it is sensitive to outliers and pays too much  attention to minimizing the largest residuals. What we mean by robust least squares multidimensional scaling (MDS), however, is to use the smacof machinery designed to minimize least squares loss functions of the form
$$
\sigma(X):=\sum\omega_k(\delta_k-d_k(X))^2,
$${#eq-stressdef}
to minimize robust loss functions. In @eq-stressdef the $\omega_k$ are
positive *weights*, the $\delta_k$ are positive *dissimilarities*, and the
$d_k(X)$ are Euclidean *distances* between two of the rows of the $n\times p$
*configuration* matrix $X$.

Some terminological conventions we use throughout the paper. By *positive* we mean smaller than or equal to zero. Smaller than zero is *strictly positive*. Some for *negative* and *strictly negative*, for *increasing* and *strictly increasing*, and for *decreasing* and *strictly decreasing*.

The prototypical robust loss function is least absolute value loss
$$
\sigma(X):=\sum \omega_k|\delta_k-d_k(X)|,
$${#eq-stradddef}
which we will call
*strife*, since the names *stress*, *sstress*, and *strain* are already taken. Note we are overloading the symbol $\sigma$, because we will use
it for all of the loss functions in this paper.

Strife is not differentiable at configurations $X$ for which there is at least one $k$ for which either $d_k(X)=\delta_k$ or $d_k(X)=0$ (or both). This lack of differentiability complicates the minimization problem. Moreover experience
with one-dimensional and city block MDS suggests that having areas
where the loss function is not differentiable leads to (many) additional local minima.

In this paper we will discuss (and implement) various variations of 
strife from @eq-stradddef. They can be interpreted in two different
ways. On the one hand they are smoothers of the absolute value
function, and consequently of strife. We want to eliminate the
problems with differentiability, at least the ones caused by
$\delta_k=d_k(X)$. If this is our main goal, then we want to choose
the smoother in such a way that it is close to the absolute value
function. This is similar to the distance
smoothing used by @pliner_96 and @groenen_heiser_meulman_99 in 
the global minimization of stress from @eq-stressdef, except that
we do not smooth the distance function but the strife residual.

On the other hand our modified loss functions can be interpreted
as more robust versions of the least squares loss function, and
consequently of stress. Our goal then is to combine the robustness of the
absolute value function with the efficiency and computational ease 
of least squares. If robustness is our main goal then there is no reason to
stay close to the absolute value function.

Our robust or smooth loss functions are all of the form
$$
\sigma(X):=\sum\omega_k\ f(r_k(X)),
$${#eq-strifedef}
where $r_k(X)$ is the *residual*, i.e.
$$
r_k(X):=\delta_k-d_k(X).
$${#eq-resdef}
The function $f$ is assumed to be even (i.e. symmetric around zero) and attains its minimum, which is equal to zero, at zero. For now, note that loss in @eq-stressdef is the special case with $f(x)=x^2$ and loss in @eq-stradddef is the special case with $f(x)=|x|$. 

\sectionbreak

# Majorization {#sec-major}

Our loss functions will be minimized by using majorization algorithms (these days more commonly known as MM algorithms).
This paper discusses a general way to construct majorization algorithms
for robust loss functions. 

For completeness we give a short
introduction to majorization, without aiming for maximum generality. 
For the general theory of majorization algorithms  we refer to their introduction in @deleeuw_C_94c and to the excellent and comprehensive book by @lange_16. As mentioned in @sec-intro the robust loss functions in @eq-strifedef are real-valued functions of a single real variable,
defined on the whole real line, they are even, and they attain a minimum equal to zero at zero. Thus they are all symmetric bowls anchored at zero.

::: {#def-majorize}
A function $g$ *majorizes* a function $f$ at a point $y$ if $g(x)\geq f(x)$ for all $x$ and $g(y)=f(y)$. The point $y$ is a *support point* of the majorization. Majorization of $f$ at $y$ is *strict* if $g(x)>f(x)$ for all $x\not= y$.
:::
 
Majorizers have at least one support point, but they can have many. The
function $g(x)=x^2+\sin^2(x)$ majorizes $f(x)=x^2$ at every integer multiple of $\pi$, and strictly majorizes $f$ at none of its support points. Also
any function $f$ majorizes itself at all points of the real line.

::: {#def-sharp}
If $\mathfrak{H}$ is a family of functions that all majorize $f$ at $y$ then $h\in\mathfrak{H}$ is a *sharp majorization* in $\mathfrak{H}$ if $h(x)\leq g(x)$ for all $g\in\mathfrak{H}$. The sharp majorization, if it exists, is by definition unique.
:::


::: {#thm-diff}
Suppose $g$ majorizes $f$ at $y$.

* If $f$ and $g$ are differentiable at $y$ then $f'(y)=g'(y)$.
* If $f$ and $g$ are twice-differentiable at $y$ then $f''(y)\leq g''(y)$.
* If $f''(y)<g''(y)$ then $g$ strictly majorizes $f$ in a neighborhood of $y$.
:::
::: {.proof}
$h=g-f$ is negative and has a minimum equal to zero at $y$. Thus the derivative of $h$ vanishes at $y$ and the second derivative is negative at $y$. If the second derivative is strictly negative then we use the sufficient condition for a local minimum.
:::

A *majorization algorithm* to minimize $f$ is iterative. We update $x^{(\nu)}$, the approximation of the minimizer in iteration $\nu$,  by
$$
x^{(\nu+1)}\in\mathop{\text{argmin}}_x\ g_\nu(x),
$${#eq-majmaj}
where $g_\nu$ majorizes $f$ at $x^{(\nu)}$. If $x^{(\nu)}\in\mathop{\text{argmin}}_x\ g_\nu(x)$ we stop. Otherwise
we find a new majorizer $g_{\nu+1}$, which majorizes $f$ at $x^{(\nu+1)}$,
and start a new iteration.

Convergence of majorization algorithms follows from the
*sandwich inequality* 
$$
f(x^{(\nu+1)})\leq g_\nu(x^{(\nu+1)})\leq g_\nu(x^{(\nu)})=f(x^{(\nu)}).
$${#eq-sandwich}
This we the algorithm produces a decreasing sequence of loss function values, which converges if loss is bounded below. In @eq-sandwich the first 
inquality (from the left) follows from majorization. If the
majorization is strict, then so is the inequality. The second inequality
follows from minimization of $g$. It is strict if $g$ has a
unique minimizer, for example if $g$ is strictly convex. The final
equality in @eq-sandwich comes from majorization at $x^{(\nu)}$.

If $x^{(\nu)}$ minimizes $g_\nu$ we stop and we have finite convergence.
If the algorithm generates an infinite sequence, which is the usual case,
the second inequality in @eq-sandwich is always strict, and the algorithm generates a strictly decreasing sequence of loss function values. Note
that for the validity of the sandwich inequality it suffices to decrease
$g_\nu$ in every iteration, and not necessarily to minimize it. Different ways to decrease $g_\nu$ correspond with different step-size procedures in gradient methods.

Of course convergence of loss function values does not guarantee convergence
of the $x^{(k)}$. For the additional continuity, campactness, and identification conditions that are needed we refer to the majoriztaion and MM literature.

# Majorizing Strife {#sec-majorization}

The idea of minimizing a least absolute value (LAV) to obtain parameter estimates dates back to the work of Boskovitch in the middle of the
eighteenth century. Until fairly recently it has been applied mainly to fit
location parameters and more general linear models.

The pioneering work in strife minimization using smacof is @heiser_88, which builds on earlier work of @heiser_87. It is based on a creative use of the Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of the absolute value function. 

The AM/GM inequality says that for all non-negative $x$ and $y$ we have 
$$
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),
$${#eq-amgm}
with equality if and only if $|x|=|y|$. If $|y|>0$ we can write @eq-amgm as
$$
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\
$${#eq-amgmmaj}
and this provides a quadratic majorization of $|x|$ at $y$. There is no quadratic majorization of $|x|$ at $y=0$, which is a problem we will have to deal with at some point.

Using the majorization @eq-amgmmaj, and assuming $\delta_k\not=d_k(Y)$ for all $k$, we define
$$
\omega_k(X):=\omega_k\frac{1}{r_k(X)},
$${#eq-wk1def}
and, for a fixed $Y$,
$$
\eta(X):=\frac12\sum \omega_k(Y)(r_k^2(X)+r_k^2(Y)).
$${#eq-strifemaj}
Now $\sigma(X)\leq\eta(X)$ for all $X$ and $\sigma(Y)=\eta(Y)$, and thus $\eta$ majorizes $\sigma$ at $Y$.

## Algorithm {#sec-amgm}


Reweighted smacof to minimize strife computes $X^{(\nu+1)}$ by minimizing or decreasing
$$
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X^{(\nu)}))^2,
$${#eq-sstrf}
using a standard smacof step. It then computes the new weights $\omega_k(X^{(\nu+1)})$ from @eq-wk1def
and uses them in the next smacof step to update $X^{(\nu+1)}$. And so on, until convergence.

A straightforward variation of the algorithm does a number of smacof steps between
upgrading the weights. This still leads to a monotone, and thus convergent, algorithm.
How many smacof steps we have to take in these inner iterations is something that needs further study. It is likely to depend on the fit of the data, on the shape of the function near the local minimum, and on how far the currfent iteration is from the local minimum.

## Zero Residuals {#sec-zero}

It may happen that for some $k$ we have $d_k(X^{(\nu)})=\delta_k$ while iterating. There have been various proposals to deal with such an 
unfortunate event, and we will discuss some of them below. Even more
importantly we will see that that the minimizer of the absolute value
loss usually satisfies $d_k(X)=\delta_k$ for quite a few elements,
which means that near convergence the algorithm may become unstable
because the weights from @eq-wk1def become very large. 

A large number of somewhat ad-hoc solutions have been proposed to deal with the problem of zero residuals, both in location analysis and in the statistical literature. We tend to agree with the assessment of @aftab_hartley_15.

> .. attempts to analyze this difficulty [caused by infinite weights of IRLS for the $\ell_p$-loss] have a long history of proofs and counterexamples to incorrect claims.

@schlossmacher_73 is the first discussion of the majorization method in the statistical literature (for LAV linear regression). His proposal is to simply set a weight equal to zero if the corresponding residual is
less than some small positive value $\epsilon$. A similar approach, also used in location analysis, is to cap the weights at some large positive value.
In @heiser_88 all residuals smaller than this epsilon get a weight equal to the weighted average of all these small residuals. @phillips_02 assumes double-exponential
errors in LAV regression and then concludes that the EM algorithm gives the 
majorization method we have discussed. He uses \eqref{eq:wk1def} throughout if all residuals are larger than $\epsilon$. If one of more residuals are smaller than epsilon then the weight for those residuals is set equal to one, while for the remaining residuals the weight is set to epsilon divided by the absolute value of the
residual. Often we get the assurance in these papers that the problem is not really important in practice, because it is very rare, and by just wiggling we will get to the unique solution anyway. But both in location analysis and in 
LAV regression the loss function is convex, however, which guarantees a unique minimum. This is certainly not the case in robust MDS. In this paper we try to follow a more systematic approach that uses smooth parametric approximations to the absolute value function, where the parameter can be used to make the 
approximation as precise as necessary.

In the case of stress the directional derivatives can be used to prove
that if $\omega_k\delta_k>0$ for all $k$ then stress is differentiable at each local minimum (@deleeuw_A_84f). For strife to be differentiable we would have to prove that at a local minimum both $d_k(X)>0$ and $(d_k(X)-\delta_k)\not= 0$ for all $k$ with $\omega_k>0$. But this is impossible by the following argument. 

In the one-dimensional
case we can partition $\mathbb{R}^n$ into $n!$ polyhedral convex cones
corresponding with the permutations of $x$. Within each cone the distances
are a linear function of $x$. Each cone can be partitioned by intersecting it
with the polyhedra defined by the linear inequalities $\delta_k-d_k(x)\geq 0$ or $\delta_k-d_k(x)\leq 0$. Some of these intersections can and will obviously be empty. Within each of these non-empty polyhedral regions strife is a linear function of $x$. Thus it attains its minimum for the region at a vertex, which is a solution for which some
distances are zero and/or some residuals are zero. There can be no  
minima, local or global, in the interior of one of these polyhedral regions.
We have thus shown that in one dimension strife is not differentiable at a local minimum, and that there is presumably a large number of them. Even for moderate $n$ the number of regions is of course too large to actually compute or draw. 

In the multidimensional case linearity goes out the window. The
set of configurations $d_k(X)=\delta_k$ is an ellipsoid and $d_k(X)=0$ defines a hyperplane. Strife is not differentiable at all intersections of these ellipsoids and hyperplanes. The partitioning of $\mathbb{R}^n$ by these
ellipsoids and hyperplanes is not simple to describe. It has convex
and non-convex cells, and within each cell strife is the difference
of two weighted sums of distances. Anything can happen. 

## $\ell_0$ loss

A somewhat extreme special case of @eq-strifedef has
$$
f(x)=\begin{cases}
0&\text{ if }x = 0,\\
1&\text{ otherwise}.
\end{cases}
$${#eq-elnul}
This is $\ell_0$ loss. Minimizing $\ell_0$ loss means maximizing the number 
of cases with perfect fit, i.e. with $\delta_k=d_k(X)$. The reason we
mention it here is that the work of @donoho_elad_03 and @candes_tao_05   suggests that the minimizer of $\ell_1$ loss, i.e. absolute value loss, gives a good approximation to the minimizer of $\ell_0$ loss, at least in a number of special cases. In MDS we do not have linearity or convexity, but nevertheless the theoretical results in simpler cases are suggestive. We have seen that at least in the one-dimensional MDS case a number of residuals will indeed be zero at the optimum LAV solution.

There is an excellent review of the use of $\ell_1$ in various sparse recovery fields in @candes_wakin_boyd_08. In that paper they also propose an iteratively
reweighted LAV algorithm, which solves $\ell_1$ problems between weight updates. Maybe because of that they go so far as calling $\ell_1$ "the modern least squares". But let's not get carried away, in actual ease and frequency of use $\ell_1$ still has a long way to go if it wants to replace $\ell_2$.

\sectionbreak

# Generalizing Strife

We have seen that @heiser_88 applied majorization to minimize strife, using the AM/GM inequality. We now generalize this approach so that it can  deal with other robust loss functions. A great number of different loss functions will be discussed. My intention is certainly not to confuse the reader and potential user by presenting a large number of alternatives with rather limited information. We show all these loss functions as examples of a general principle of algorithm construction and as examples of loss functions that have been used in
statistics, location analysis, image analysis, and engineering over the years.
They are all implemented in the function smacofRobust(), written in R (@r_core_team_24), and listed in the appendix of this paper.

## Sharp Quadratic Majorization

The AM/GM inequality was used in @sec-majorization to construct a quadratic majorization of strife. In this paper we are specifically interested in sharp quadratic majorization, in which $\mathfrak{H}$ is the set of all (not necessarily convex) quadratics that majorize $f$ at $y$. This case has been studied in detail (in the case of real-valued functions on the line) in @deleeuw_lange_A_09, and much of this section is taken from their paper. We
added some minor extensions and reformulations.

For the loss functions we study in this paper there are two problems that have to be solved. First, we want a general procedure to construct quadratic majorizers. Second, we what to show that some of our majorizers are sharp.

If $f$ is differentiable at $y$, then all quadratics $g$ that majorize $f$ at $y$ are of the form
$$
g_a(x):=f(y)+f'(y)(x-y)+\frac12a(x-y)^2
$${#eq-qmaj}
for some $a=g_a''(x)$, not necessarily positive.
If $f$ is twice differentiable at $y$ then $g_a''(y)\geq f''(y)$ by @thm-diff, and thus we have the necessary condition $a\geq f''(y)$. Note that not all functions have quadratic majorizations. If $f$ is a non-trivial cubic and $g$ is quadratic, then $h=g-f$ is a non-trivial cubic, and consequently we cannot have $h\geq 0$ on the whole real line.

We now look more closely at $a$ in @eq-qmaj, initially concentrating on
necessary conditions for majorization. For $x\not = y$ define
$$
\alpha(x):=\frac{f(x)-f(y)-f'(y)(x-y)}{\frac12(x-y)^2}.
$${#eq-alpdef}
Of course $\alpha$ is a different function of $x$ for each $y$, but since we are dealing with majorization at one fixed $y$ we suppress this dependence. 

If $f$ is two times differentiable at $y$ then, by the definion of the second derivative,
$$
\lim_{x\rightarrow y}\alpha(x)=f''(y),
$${\#eq-alphop}
and thus can we define $\alpha(y)=f''(y)$ to make $\alpha$ continuous at $y$.

If $f$ is convex, then $\alpha$ is the ratio of two positive convex functions of $x$, and is thus positive. If $f$ is concave then $\alpha$ is negative.
If $f$ is two times differentiable then
there is a $z$ between $x$ and $y$ such that $\alpha(x)=f''(z)$.
Thus if $f''(x)\leq K$ for all $x$, then $\alpha(x)\leq K$ as well.

Quadratic majorization of $f$ by $g_a$ from @eq-qmaj at $y$ is equivalent to $\alpha(x)\leq a$ for all $x$. Thus $g_a$ majorizes $f$ at $y$ if and only if $\alpha$ is bounded above by $a$. If $\alpha$ is unbounded above there is no
quadratic majorizer at $y$. We can also define
$$ 
a_+:=\sup_x\alpha(x),
$${#eq-aplus}
and say that sharp quadratic majorization at $y$ is possible if $a_+<+\infty$. We have majorization at $y$ by $g_a$ if $a\geq a_+$, we have sharp majorization if $a=a_+$. It follows that sharp quadratic majorizations
exist if $f''$ is bounded above.

We illustrate these concepts by applying them to low-degree polynomials. First a cubic. Expand the cubic around $y$ as 
$$
f(x)=f(y)+f'(y)(x-y)+\frac12f''(y)(x-y)^2+\frac16f'''(y)(x-y)^3.
$${#eq-cubexp}
Thus
$$
\alpha(x)=f''(y)+\frac13f'''(y)(x-y).
$${#eq-cubalp}
Since $\alpha$ is always unbounded above, no quadratic majorizer exists 
at any $y$. 

Now apply the same reasoning to a non-trivial quartic. We find
$$
\alpha(x)=f''(y)+\frac13f'''(y)(x-y)+\frac{1}{12}f^{iv}(y)(x-y)^2,
$${#eq-quaalp}
a quadratic in $x$. Of course for a quartic $f^{iv}(y)$ is the same for every $y$ and we may as well write $f^{iv}$. If $f^{iv}$ is strictly positive the quadratic in @eq-quaalp is unbounded above, and no quadratic majorization exists at any $y$. If $f^{iv}$ is strictly negative then $\alpha$ has a maximum at $x=y-2f'''(y)/f^{iv}$, and a sharp quadratic majorization exists at any 
$y$. 




We can get some more information about $a_+$ by differentiating $\alpha$.

::: {#thm-locmax}
Suppose there is an $x$ where $\alpha$ is two times differentiable and $a_+=\alpha(x)$. Then 
\begin{subequations}
\begin{align}
\frac{f(x)-f(y)}{x-y}&=\frac12(f'(x)+f'(y)),\label{eq-loc1}\\
f''(x)&\leq\frac{f'(x)-f'(y)}{x-y}.\label{eq-loc2}
\end{align}
Moreover
\begin{equation}
a_+=\frac{f'(x)-f'(y)}{x-y}.\label{eq-loc3}
\end{equation}
\end{subequations}
If \eqref{eq-loc1} and the inequality in \eqref{eq-loc2} is strict then $\alpha$ has a local maximum at $x$.
:::
::: {.proof}
After some manipulation \eqref{eq-loc1} and \eqref{eq-loc2} are the necessary conditions $\alpha'(x)=0$ and 
$\alpha''(x)\leq 0$ for a local maximum. If $\alpha'(x)=0$ and 
$\alpha''(x)>0$ the conditions are sufficient. If $x$ satisfies \eqref{eq-loc1} then substitution in @eq-alpdef gives \eqref{eq-loc3}.
:::

Note that we have not shown that $\alpha$ always attains its maximum. 
@deleeuw_lange_A_09 give the example of the differentiable function
\begin{equation}
f(x)=\begin{cases}
x^2&\text{ if }x\leq 1,\\
2x-1&\text{ otherwise},
\end{cases}\label{eq:dllexam}
\end{equation}
which has $\alpha(x)=0$ for $x>1$ and $\alpha(x)<2$ for $x\leq 1$, so that
$a_+=\sup_{x\leq 1}\alpha(x)=2$ and the maximum does not exist. 

Also, the conditions of @thm-locmax cannot possibly show
that $\alpha$ has a *global* maximum at $x$, and that consequently $g_a$ of @eq-qmaj with $a$ given by \eqref{eq-loc3} is a sharp quadratic majorizer.

Differentiation of $\alpha$ gives the following result.

::: {#cor-incdec}
Suppose $\alpha$ is differentiable. Then it is strictly increasing if and only if for all $x$
$$
\frac{f(x)-f(y)}{x-y}<\frac12(f'(x)+f'(y)),
$${#eq-strinc}
and strictly decreasing if and only if for all $x$
$$
\frac{f(x)-f(y)}{x-y}<\frac12(f'(x)+f'(y)).
$${#eq-strdec}
:::
::: {.proof}
These are the conditions $\alpha'(x)>0$ and $\alpha'(x)<0$ for all $x$.
:::

If @eq-strinc or @eq-strdec is true then $\alpha$ does not have a maximum,
and the supremum (possibly infinite) is the limit of $\alpha$ to $-\infty$ or $+\infty$.

Because of our robust loss functions we are especially interested in the case
that $f$ is even. Setting $x=-y$ makes both sides of \eqref{eq-loc1} equal to zero. Thus $\alpha$ has a stationary point at $x=-y$. If in addition
$$
f''(y)<\frac{f'(y)}{y}
$${#eq-miny}
then $\alpha$ has a local maximum at $x=-y$. 

::: {#thm-evena}
Suppose $f$ is even and twice-differentiable. If $f'(x)/x$ is decreasing
on the positive real line then $\alpha$ has a local maximum at $x=-y$.
:::
::: {.proof}
If $f'(x)/x$ is strictly decreasing its derivative is strictly negative. Thus
$xf''(x)-f'(x)<0$ for $x>0$. For an even function
$f'(x)/x$ is strictly decreasing on the positive real line if and only if it is strictly increasing on the negative real line. Thus $xf''(x)-f'(x)>0$ for $x<0$. In both cases @eq-miny follows.
:::

## Two Support Points {#sec-twosupport}

We can say more if it is known that the quadratic majorization has more than one support point.

::: {#thm-twop}
Suppose the quadratic $g_a$ of @eq-qmaj majorizes $f$ at $y$ and at $z\not= y$. Then
$$
a=\frac{f'(z)-f'(y)}{z-y}
$${#eq-atwop}
:::
::: {.proof}
From @eq-qmaj we have $g_a'(x)=f'(y)+a(x-y)$. But because of @thm-diff
we must also have $g_a'(z)=f'(z)$. Thus $g_a'(z)=f'(y)+a(z-y)=f'(z)$.
:::

Again, we have not shown that $g_a$ with $a$ from @eq-atwop majorizes
$f$ at $y$ and $z$. Only the reverse implication, which is that if $g_a$
majorizes $f$ at $y$ and $z$ then $g_a$ is uniquely determined
by @eq-atwop. In practice, even if one knows the support points $y$ and $z$, 
one still has to prove majorization. This is precisely how @heiser_88, @verboon_heiser_94, and  @groenen_giaquinto_kiers_03 establish their majorizations. 

In his master's thesis @vanruitenburg_05 takes us one step further down the necessary conditions road. To present his main result we need two lemmas.

::: {#lem-fan}
If different quadratics $g$ and $h$ majorize $f$ at $y$ then either
$g$ strictly majorizes $h$ or $h$ strictly majorizes $g$.
:::
::: {.proof}
We have
\begin{subequations}
\begin{align}
g(x)&=f(y)+f'(y)(x-y)+\frac12a_1(x-y)^2,\label{eq-gfunc}\\
h(x)&=f(y)+f'(y)(x-y)+\frac12a_2(x-y)^2.\label{eq-hfunc}.
\end{align}
\end{subequations}
Thus $g(x)-h(x)=\frac12(a_1-a_2)(x-y)^2$, which is either
strictly positive or strictly negative for $x\not= y$.
:::

::: {#lem-ruit}
Suppose quadratics $g$ and $h\not= g$ majorize $f$ at $y$. 
Suppose, in addition, that $g$ majorizes $f$ at $z\not=y$.
Then $h$ strictly majorizes $g$ at $y$. 
:::
::: {.proof}
By @lem-fan one of $g$ and $h$ has to strictly majorize the other. If $g$ strictly majorizes $h$ then $h(z)<g(z)=f(z)$ and thus $h$ does not majorize $f$, contrary to assumption. It follows that $h$ strictly majorizes $g$ at $y$.
:::

::: {#thm-ruit} 
If the quadratic $g$ majorizes $f$ at $y$ and at $z\not= y$, then $g$ is a
sharp majorizer of $f$ at $y$.
:::
::: {.proof}
Directly from @lem-ruit.
:::


Again our results simplify if the function $f$ is even.

::: {#thm-evqu}
If $f$ is even and the quadratic $g$ majorizes $f$ at $y$ and $-y$, where $y\not= 0$, then $g$ is the even quadratic given by
$$
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
$${#eq-geven}
Moreover $g$ is the sharp quadratic majorization of $f$ at $y$ and $-y$.
:::
::: {.proof}
If $f$ is even then $f'$ is odd. Consequently @eq-atwop becomes
$$
a=\frac{f'(y)}{y},
$${#eq-aeven}
which is even. Moreover because $g$ majorizes $f$ at $y$
\begin{subequations}
\begin{equation}
g(x)=f(y)+f'(y)(x-y)+\frac12\frac{f'(y)}{y}(x-y)^2,\label{eq-gatplusy}
\end{equation}
and because $g$ majorizes $f$ at $-y$
\begin{equation}
g(x)=f(y)-f'(y)(x+y)+\frac12\frac{f'(y)}{y}(x+y)^2.\label{eq-gatminy}
\end{equation}
\end{subequations}
Averaging the two equations \eqref{eq-gatplusy} and \eqref{eq-gatminy} for $g$, and simplifying, gives the required result. That the majorization is sharp follows from @thm-ruit.
:::

## Sufficient Conditions 

@deleeuw_lange_A_09 gives a way to construct quadratic
majorizers of a differentiable function on the real line. We give a slightly more general version of their basic theorem that provides a convenient way to deal with loss functions that are not differentiable everywhere. Our proofs
are different.

::: {#thm-wght}
Suppose there is a function $h$, concave on $\mathbb{R}^+$, such that
$f(x)=h(x^2)$. Then
$$
g(x)=f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
$${#eq-given}
is a sharp quadratic majorizer of $f$ at $y$
:::
::: {.proof}
By concavity for all $u\in\mathbb{R}^+$ and for all $v\in\mathbb{R}^+$ for which $h'(v)$ exists
$$
h(u)\leq h(v)+h'(v)(u-v).
$${#eq-concave}
Substituting $u=x^2$ and $v=y^2$ gives the quadratic majorization
$$
f(x)\leq f(y)+h'(y^2)(x^2-y^2),
$${#eq-conc2}
which is true for all real $x$ and $y$, provided $h$ is differentiable at
$y^2$. If it is then using $f'(y)=2yh'(y^2)$ in @eq-conc2 gives
$$
f(x)\leq f(y)+\frac12\frac{f'(y)}{y}(x^2-y^2).
$${#eq-concmaj}
:::

Note that $f(x)=h(x^2)$ implies that $f$ is even. If we want to get rid of the differentiability assumption altogether we can use the majorization
$$
f(x)\leq f(y)+a(y)(x^2-y^2)
$$
where $a(y)$ is any element of $\partial h(y^2)$, the superdifferential
of $h$ at $y^2$. The superdifferential, which is the analog of the subdifferential for concave functions (@border_18), is non-empty on $\mathbb{R}^+$, except possibly at zero.


::: {#thm-sqrt}
$f(x)=h(x^2)$ for concave $h$ on $\mathbb{R}^+$ if and only if $f(\sqrt{x})$ is concave in $x$, and, for differentiable $f$, if and only if $f'(x)/x$ is decreasing on $\mathbb{R}^+$.
:::
::: {.proof}
The first part of the theorem merely states the obvious. 
:::

Some quick examples of the use of @thm-wght before we go to the
robust loss functions.

::: {#exm-sqrt}
Suppose $h$ is the square root, so that $f(x)=\sqrt(x^2)=|x|$
and for $y\not=0$ we have $f'(y)/y=1/|y|$.
From @thm-wght we have the quadratic majorization
$$
|x|\leq|y|+\frac12\frac{1}{|y|}(x^2-y^2)=\frac12\frac{1}{|y|}(x^2+y^2),
$$
which is the AM/GM inequality. So AM/GM majorization of the absolute
value is sharp.
:::

::: {#exm-quartic}
Suppose $h(x)=\frac12x(1-\frac12x)$. Thus $f$ is the even quartic
$$
f(x)=\frac12x^2(1-\frac12x^2).
$$
We have $f'(y)/y=1-y^2,$ and thus the majorizer
$$
g(x)=f(y)+\frac12(1-y^2)(x^2-y^2).
$$
For all $y^2<1$ $g$ has its minimum at zero. If $y^2>1$ the majorizer is a concave quadratic, which has no
minimum and is unbounded below. If $y^2=1$ the majorizer is the horizontal line $x=\frac14$ and the majorization method stops at a local maximum.
:::


```{r}
#| label: fig-quartic
#| fig.align: center
#| echo: FALSE
#| fig.cap: A Quartic
fq <- function(x) {
  z <- (x ^ 2) / 2
  return(z * (1 - z))
}
gq <- function(x, y) {
  return(fq(y) + (1 - (y ^ 2)) * ((x ^ 2) - (y ^ 2)) / 2.0)
}
base <- ggplot() + xlim(-1.3, 1.3)
base <- base +
  geom_function(aes(), fun = fq, colour = "red") 
base + geom_function(aes(), fun = gq, args = list(y = 0.5), colour = "blue")
```



We now apply @thm-wght to functions of the form
$$
\sigma_f(X):=\sum \omega_k\ f(\delta_k-d_k(X)),
$${#eq-fstressdef}
where $f$ satisfies the conditions in the theorem. If
$$
\eta(X):=\sum \omega_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),
$${#eq-fstressmaj}
then $\eta$ is a sharp quadratic majorization at $Y$.

In iteration $k$ the robust smacof algorithm does a smacof step towards minimization of
 $\eta$ over $X$. We can ignore the parts of @eq-fstressmaj that only depend on $Y$, and 
minimize
$$
\sum \omega_k(X^{(\nu)})(\delta_k-d_k(X))^2,
$${#eq-fstressaux}
with 
$$
\omega_k(X^{(\nu)}):=\omega_k\frac{f'(\delta_k-d_k(X^{(\nu)}))}{2(\delta_k-d_k(Y))}.
$${#eq-wkdef}
We then recomputes the weights $\omega_k(X^{(\nu+1)})$ and go to the smacof step again. This can be thought of as iteratively reweighted least squares (IRLS), and also as nested majorization, with the smacof majorization based on the Cauchy-Schwartz inequality within the sharp quadratic majorization of the loss function based on @thm-wght.

\sectionbreak

# Power Smoothers

We first discuss a class of smoothers of the absolute value function that
maintain most of its structure. They have a shift parameter $c$ that takes
care of the non-differentiability. Although different smoothers have different scales and interpretations for $c$, we will use the same symbol throughout. Also some smoothers have a power 
parameter $q$ that determines the shape of the loss function bowl. 

## Charbonnier {#sec-charb}

The first, and perhaps most obvious, choice for smoothing the absolute
value function is
$$
f(x)=\sqrt{x^2 + c^2}.
$${#eq-charf}
In the engineering literature @eq-charf is known as Charbonnier loss, after @charbonnier_blanc-feraud_aubert_barlaud_94, who were possibly
the first researchers to use it in image restauration. 
@ramirez_sanchez_kreinovich_argaez_14 count the number of
computer operations and conclude that @eq-charf is also the "most computationally efficient smooth approximation to $|x|$". 

For $c>0$ we have $f_c(x)>|x|$.
If $c\rightarrow 0$ then  $f_c(x)$ decreases monotonically to $|x|$. Also $\max_x|f_c(x)-|x||=c$ attained at $x=0$, which implies
uniform convergence of $f_c$ to $|x|$.



By l'HÃ´pital
\begin{subequations}
\begin{equation}
\lim_{x\rightarrow 0}\frac{\sqrt{x^2+c^2}-c}{\frac12x^2}=1.
\end{equation}
Of course also
\begin{equation}
\lim_{x\rightarrow\infty}\frac{\sqrt{x^2+c^2}}{|x|}=1
\end{equation}
and
\begin{equation}
\lim_{x\rightarrow\pm\infty}\sqrt{x^2+c^2}-|x|=0
\end{equation}
\end{subequations}
Thus if $x$ is much smaller than $c$ then loss is approximately a quadratic in $x$, 
and if $x$ is much larger than $c$ then loss is approximately the
absolute value. 

Loss function @eq-charf is infinitely many times differentiable. Its first derivative is
\begin{equation}
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,\label{eq:charg}
\end{equation}
which converges, again in the sup-norm and uniformly, to the sign function if $c\rightarrow 0$. The IRLS weights are
\begin{equation}
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}\label{eq:charw}
\end{equation}
which is clearly a decreasing function of $x$ on $\mathbb{R}^+$.

```{r}
#| label: fig-char
#| fig.align: center
#| echo: FALSE
#| fig.cap: "Charbonnier Loss"
fcharbonnier <- function(x, c) {sqrt(x ^ 2 + c ^ 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1"), fun = fcharbonnier, args = list(c = 1))
base <- base + labs(x = "variable", y = "value")
base <- base + geom_function(aes(colour = "c = .1"), fun = fcharbonnier, args = list(c = .1)) 
base <- base + geom_function(aes(colour = "c = .01"), fun = fcharbonnier, args = list(c = .01)) 
base + geom_function(aes(colour = "c = .001"), fun = fcharbonnier, args = list(c = .001)) 
```




## Generalized Charbonnier

The loss function $(x^2+c^2)^\frac12$ smoothes $|x|$. In the same way generalized Charbonnier loss smoothes $\ell_p$ loss $|x|^q$. We have
a two-parameter family of loss functions in this case.
$$
f_{c,q}(x):=(x^2+c^2)^{\frac12 q}
$${#eq-gcharf}
\begin{equation}
w_{c,q}(x)=q(x^2+c^2)^{\frac12 q-1}\label{eq:gcharw}
\end{equation}
which is non-increasing for $q\leq 2$. Note that we do not assume that
$q>0$, and consequently generalized Charbonnier loss provides us with more flexibility than Charbonnier loss from @eq-charf. Of course if
$q<0$ "loss" becomes "gain", with a maximum at zero instead of a
minimum. To get a proper loss function, take the negative. 
@fig-gchar plots generalized Charbonnier loss for some negative 
values of $q$. We see that for $\alpha\rightarrow-\infty$ generalized Charbonnier loss aproximates $\ell_0$ loss.

```{r}
#| label: fig-gchar
#| fig.align: center
#| echo: FALSE 
#| fig.cap: Generalized Charbonnier Loss
gfcharbonnier <- function(x, c, q) {
  s <- sqrt(x ^ 2 + c ^ 2) ^ q
  if (q < 0) return(1 - s)
  return(s)
}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1, q = -5"), fun = gfcharbonnier, args = list(c = 1, q = -5)) 
base <- base + geom_function(aes(colour = "c = 1, q = -1"), fun = gfcharbonnier, args = list(c = 1, q = -1)) 
base <- base + geom_function(aes(colour = "c = 1, q = -.5"), fun = gfcharbonnier, args = list(c = 1, q = -.5)) 
base + geom_function(aes(colour = "c = 1, q = -.1"), fun = gfcharbonnier, args = list(c = 1, q = -.1)) 
```



## Barron

There are a fair number of generalizations
of the power smoother loss functions in the engineering literature. We will discuss one nice generalization from @barron_19. 

$$
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right).\
$${#eq-barron}

To quote Barron

>Here $\alpha\in\mathbb{R}$ is a shape parameter that controls the robustness of the loss and $c>0$ is a scale parameter that controls the size of the loss's quadratic bowl near $x=0$.

A number of interesting special cases of @eq-barron are obtained by selecting various values of the $\alpha$ parameters. For $\alpha=1$ it becomes Charbonnier loss, and for $\alpha=-2$ it is Geman-McClure loss.
There are also some limiting cases. For $\alpha\rightarrow 2$ Barron loss becomes squared error loss, for $\alpha\rightarrow 0$ it becomes Cauchy loss, and for $\alpha\rightarrow-\infty$ it becomes Welsch loss. Accordingly
$$
f'_{\alpha,c}(x)=\begin{cases}
\frac{x}{c^2}&\text{ if }\alpha=2,\\
\frac{2x}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{x}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{x}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
$${#eq-barronf}
and thus
$$
h_{\alpha,c}(x)=\begin{cases}
\frac{1}{c^2}&\text{ if }\alpha=2,\\
\frac{2}{x^2+2c^2}&\text{ if }\alpha=0,\\
\frac{1}{c^2}\exp\left(-\frac12(x/c)^2\right)&\text{ if }\alpha\rightarrow-\infty,\\
\frac{1}{c^2}\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{(\frac12\alpha-1)}&\text{ otherwise}.
\end{cases}
$${#eq-barronh}

```{r}
#| label: fig-barron
#| fig.align: center
#| echo: FALSE
#| fig.cap: "Barron Loss"
fbarron <- function(x, c, alpha) {
  f1 <- abs(alpha - 2) / alpha
  f2 <- (((x / c) ^ 2) / abs(alpha - 2) + 1) 
  return(f1 * (f2 ^ (alpha / 2) - 1))
}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 1, alpha = 2"), fun = fbarron, args = list(c = 1, alpha = 1.999))
base <- base + labs(x = "variable", y = "value")
base <- base + geom_function(aes(colour = "c = 1, alpha = 1"), fun = fbarron, args = list(c = 1, alpha = 1))
base <- base + geom_function(aes(colour = "c = 1, alpha = 0"), fun = fbarron, args = list(c = .1, alpha = .001)) 
base <- base + geom_function(aes(colour = "c = 1, alpha = -2"), fun = fbarron, args = list(c = 1, alpha = -2)) 
base + geom_function(aes(colour = "c = 1, alpha = -10"), fun = fbarron, args = list(c = 1, alpha = -10))
```

\sectionbreak

# Convolution Smoothers

Suppose $\pi$ is a probability density, symmetric around zero, with finite or infinite support, expectation zero, and variance one. Define the convolution
$$
f_c(x):=\frac{1}{c}\int_{-\infty}^{+\infty}|x-y|\ \pi(\frac{y}{c})dy.
$$
Now $c^{-1}\pi(y/c)$ is still a symmetric probability density integrating to one, with expectation zero, but it now has variance $c^2$. Thus if $c\rightarrow 0$ it becomes more and more like the Dirac delta function and $f_c(x)$ converges to  the absolute value function.

It is  clear that we can use any scale family of probability densities to define convolution smoothers. There is an infinite number of possible choices, with finite or infinite support, smooth or nonsmooth, using splines or wavelets, and so on. We give two quite different examples.

## Huber

Take
$$
\pi(x)=\begin{cases}\frac12 &\text{ if }|x|\leq 1,\\0&\text{ otherwise.}\end{cases}
$$
Then
$$
f_c(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&\text{ if }|x|\leq c,\\
|x|&\text{ otherwise}.
\end{cases}
$${#eq-hubera}

The Huber function (@huber_64) is traditionally transformed linearly so that it is zero for $x=0$. This gives
$$
f_c(x)=\begin{cases}
\frac12x^2&\text{ if }|x|<c,\\
c|x|-\frac12 c^2&\text{ otherwise}.
\end{cases}
$${#eq-huberb}
For robust estimation and IRLS it does not matter if we use @eq-hubera or
@eq-huberb. Our discussion in the introduction suggests that if we just want
a smoother of the absolute value function, then @eq-hubera is the natural choice, if we want a robust loss function that combines the advantages of 
least squares and least absolute value then that leads us to @eq-huberb.

Because Charbonnier loss behaves the same way as Huber loss, as absolute value loss for large $x$ and as squared loss for small $x$, it is also known as Pseudo-Huber loss.

The Huber function is differentiable, although not twice diffentiable. Its derivative is
$$
f'(x)=\begin{cases}
c&\text{ if }x\geq c,\\
x&\text{ if }|x|\leq c,\\
-c&\text{ if }x\leq -c.
\end{cases}
$$
$$
\omega(x)=
\begin{cases}
\hfill\frac{c}{x}&\text{ if }x\geq c,\\
\hfill1&\text{ if }|x|\leq c,\\
-\frac{c}{x}&\text{ if }x\leq -c.
\end{cases}
$$
The Huber function is even and differentiable. Moreover
$f'(x)/x$ decreases from. Thus @{thm-wght} applies.

The MDS majorization algorithm for the Huber loss is to update $Y$ by 
minimizing (or by performing one smacof step to decrease)
$$
\sum \omega_k(Y)(\delta_k-d_k(X))^2
$$
where
$$
\omega_k(Y)=\begin{cases}
\omega_k&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac{c\omega_k}{|\delta_k-d_k(Y)|}&\text{ otherwise}.
\end{cases}
$$

```{r}
#| label: fig-huber
#| fig.align: center
#| echo: FALSE
#| fig.cap: Huber Loss
fhuber <- function(x, c) {ifelse(abs(x) < c, (x ^ 2) / 2, c * abs(x) - ((c ^ 2) / 2))}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fhuber, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fhuber, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fhuber, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fhuber, args = list(c = .5))
```

## Gaussian

In @deleeuw_E_18f we also discussed the
convolution smoother proposed by @voronin_ozkaya_yoshida_15. The idea is
to use the convolution of the absolute value
function and a Gaussian pdf.
$$
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
$$

Carrying out the integration gives

$$
f_c(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
$$
The derivative is
$$
f'_c(x)=2\Phi(x/c)-1
$$
It may not be immediately obvious in this case that the weight function $f'(x)/x$ is non-increasing on $\mathbb{R}^+$. We prove that its
derivative is negative on $(0,+\infty)$. 
The derivative of $f'(x)/x$ has the sign of $xf''(x)-f'(x)$, which is
$z\phi(z)-\Phi(z)+1/2$, with $z=x/c$. It remains to show that $\Phi(z)-z\phi(z)\geq\frac12$,
or equivalently that $\int_0^z\phi(x)dx-z\phi(z)\geq 0$.
Now if $0\leq x\leq z$ then $\phi(x)\geq\phi(z)$ and thus $\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)$, which completes the proof.


$$
\omega_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
$$

```{r}
#| label: fig-gaussian
#| fig.align: center
#| echo: FALSE
#| fig.cap: Gaussian Convolution Loss
fgauss <- function(x, c) {x * (2 * pnorm(x / c) - 1) + 2 * c * dnorm(x / c)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fgauss, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fgauss, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fgauss, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fgauss, args = list(c = .5))
```



\sectionbreak

# A Bouquet of Loss Functions

In the early seventies, after the pioneering mostly theoretical work in robust statistics of Huber, Hampel, and Tukey, the mainframe computer allowed statisticians to make large-scale comparisons of many robust loss functions. The most impressive of such comparisons was the Princeton Robustness Study
(@andrews_bickel_hampel_huber_rogers_tukey_72).

In @holland_welsch_77 the computer package ROSEPACK was introduced that 
made it relatively easy to compute robust estimators using several different loss functions. Eight different weight functions were implemented as
options. Somewhat later @coleman_holland_kaden_klema_peters_80 made an
more modern computer implementation available, using the same eight weight
functions, which was not limited to mainframes.

We have implemented the same eight weight functions in smacofRobust. Below we give formulas for the loss function, the influence function, and the weight function. One of the eight is Huber loss, which we already discussed in the convolution section. We graph the remaining seven loss functions for selected values of the "tuning constants" $c$.

 @holland_welsch_77, following @andrews_bickel_hampel_huber_rogers_tukey_72,
 distnguish between "hard redescenders" that have an influence function $f'$
 equal to zero if $x$ is large enough (Andrews, Tukey, and Hinich loss),
 "soft redescenders" with influence functions asymptotic to zero for large $x$
 (Cauchy, Welsch loss), and loss functions with a monotone influence 
 function (Huber, Logistic, Fair loss)
 

## Andrews

The first loss function in this section is taken from 
@andrews_bickel_hampel_huber_rogers_tukey_72.

\begin{align}
f(x)&=\begin{cases}
c^2(1-\cos(x/c))&\text{ if }|x|\leq\pi c,\\
2c^2&\text{ otherwise.}
\end{cases}\\
f'(x)&=\begin{cases}
c\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}\\
\omega(x)&=\begin{cases}
(x/c)^{-1}\sin(x/c)&\text{ if }|x|\leq\pi c,\\
0&\text{ otherwise.}
\end{cases}
\end{align}

Because $\cos$ is even and $\sin(x)/x$ decreases on $[0,\pi]$ @thm-wght  applies.

```{r}
#| label: fig-andrews
#| fig.align: center
#| echo: FALSE
#| fig.cap: Andrews Loss
fandrews <- function(x, c) {ifelse(abs(x) < pi * c, 
                 (c ^ 2) * (1 - cos(x / c)), 
                 2 * (c ^ 2))}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fandrews, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fandrews, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fandrews, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fandrews, args = list(c = .5)) 
```


## Tukey 

The usual reference for Tukey loss is @beaton_tukey_74, although closely related hard redescenders are also in @andrews_bickel_hampel_huber_rogers_tukey_72.

\begin{align}
f(x)&=\begin{cases}
\frac{c^2}{6}\left(1-\left(1-(x/c)^2\right)^3\right)&\text{ if } |x|\leq c,\\
\frac{c^2}{6}&\text{ otherwise}.
\end{cases}\\
f'(x)&=\begin{cases}
x\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
\omega(x)&=\begin{cases}
\left(1-\left(1-(x/c)^2\right)^2\right)&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

The conditions of @thm-wght are clearly satisfied.

```{r}
#| label: fig-tukey
#| fig.align: center
#| echo: FALSE
#| fig.cap: Tukey Loss
ftukey <- function(x, c) {ifelse(
    abs(x) < c, ((c ^ 2) / 6) * ( 1 - (1 - (x / c) ^ 2) ^ 3), 
                 (c ^ 2) / 6)}
base <- ggplot() + xlim(-2, 2)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = ftukey, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = ftukey, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = ftukey, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = ftukey, args = list(c = .5))
```


## Hinich

Hinich loss, from @hinich_talwar_75, is somewhat special because it is not differentiable at $c$. For $x\not= c$ and $x>0$ the function $f'(x)/x$
is discontinuous, but non-increasing on $[0,+\infty)$.

\begin{align}
f(x)&=\begin{cases}
\frac12 x^2&\text{ if } |x|\leq c,\\
\frac12 c^2&\text{ otherwise}.
\end{cases}\\
g(x)&=\begin{cases}
x&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}\\
h(x)&=\begin{cases}
1&\text{ if } |x|\leq c,\\
0&\text{ otherwise}.
\end{cases}
\end{align}

```{r}
#| label: fig-hinich
#| fig.align: center
#| echo: FALSE
#| fig.cap: Hinich Loss
fhinich <- function(x, c) {ifelse(abs(x) < c, (x ^ 2) / 2, (c ^ 2) / 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fhinich, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fhinich, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fhinich, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fhinich, args = list(c = .5))
```



## Cauchy 

Cauchy loss seems to have many names. @black_anandan_96 call it Lorentzian loss, and @holland_welsch_77 call it t-likelihood loss. It  is related to 
the Cauchy distribution, which is Student's t distribution with one degree of freedom.

@mlotshwa_vandeventer_bosman_23

\begin{align}
f(x)&=\frac12c^2\log(1+\{\frac{x}{c}\}^2),\\
f'(x)&=x\frac{1}{\{1+\frac{x}{c}\}^2},\\
\omega(x)&=\frac{1}{\{1+\frac{x}{c}\}^2}
\end{align}


```{r}
#| label: fig-cauchy
#| fig.align: center
#| echo: FALSE
#| fig.cap: Cauchy Loss
fcauchy <- function(x, c) {(c ^ 2) / 2 * log(1 + (x / c) ^ 2)}
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fcauchy, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fcauchy, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fcauchy, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fcauchy, args = list(c = .5))
```



## Welsch

@dennis_welsch_78

Leclerc loss

\begin{align}
f(x)&=\frac12c^2[1-\exp(-\{\frac{x}{c}\}^2)],\\
f'(x)&=x\exp(-\{\frac{x}{c}\}^2,\\
\omega(x)&=\exp(-\{\frac{x}{c}\}^2),
\end{align}

```{r}
#| label: fig-welsch
#| fig.align: center
#| echo: FALSE
#| fig.cap: Welsch Loss
fwelsch <- function(x, c) 1 - exp(-.5 * (x / c) ^ 2)
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = fwelsch, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = fwelsch, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = fwelsch, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = fwelsch, args = list(c = .5))
```


## Logistic

\begin{align}
f(x)&=c^2[\log(\cosh(x/c))],\\
f'(x)&=c\tanh(x/c),\\
\omega(x)&=(x/c)^{-1}\tanh(x/c).
\end{align}

```{r}
#| label: fig-logistic
#| fig.align: center
#| echo: FALSE
#| fig.cap: Logistic Loss
flogistic <- function(x, c) (c ^ 2) * log(cosh(x / c))
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = flogistic, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = flogistic, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = flogistic, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = flogistic, args = list(c = .5))
```

## Fair

\begin{align}
f(x)&=c^2\{|x|/c-\log(1+|x|/c)\},\\
f'(x)&=x(1+(|x|/c))^{-1},\\
\omega(x)&=(1+(|x|/c))^{-1}.
\end{align}

```{r}
#| label: fig-fair
#| fig.align: center
#| echo: FALSE
#| fig.cap: Fair Loss
ffair <- function(x, c) log((x / c) ^ 2 + 1)
base <- ggplot() + xlim(-3, 3)
base <- base +
  geom_function(aes(colour = "c = 3"), fun = ffair, args = list(c = 3)) 
base <- base + geom_function(aes(colour = "c = 2"), fun = ffair, args = list(c = 2)) 
base <- base + geom_function(aes(colour = "c = 1"), fun = ffair, args = list(c = 1)) 
base + geom_function(aes(colour = "c = .5"), fun = ffair, args = list(c = .5))
```

\sectionbreak

# Examples

## Gruijter

The example we use are dissimilarities between nine Dutch political parties,
collected by @degruijter_67. They are averages over a politically heterogenous 
group of 100 introductory psychology students, and consequently they
regress to the mean. Any reasonable MDS analysis of these data would at
least allow for an additive constant.

Some background on Dutch politics around that time may be useful.

* CPN - Communists.
* PSP - Pacifists, left-wing.
* PvdA - Labour, Democratic Socialists.
* D'66 - Pragmatists, nether left-wing nor right-wing, brand new in 1967.
* KVP - Christian Democrats, catholic.
* ARP - Christian Democrats, protestant.
* CHU - Christian Democrats, protestant.
* VVD - Liberals, European flavour, conservative.
* BP - Farmers, protest party, right-wing.

The dissimilarities are in the table below.

```{r gruijter, echo = FALSE}
knitr::kable(delta)
```

The reason we have chosen this example is partly because CPN and BP are
outliers, and we can expect the robust loss functions to handle outlying
dissimilarities differently from the bulk of the data.

Unless otherwise indicated we run smacofRobust() with a maximum of
10,000 iterations, and we decide that we have convergence if the
difference between consecutive stress values is less than `r 1e-15`.
We perform one single smacof iteration between the updates of
the weights. For each analysis we show the configuration plot, the Shepard plot, and a histogram of the absolute values of the residuals. In the Shepard plot points corresponding to the eight CPN-dissimilarities are labeled "C", while BP-dissimilarities are "B".

### Least Squares


```{r}
#| label: ls
#| echo: FALSE
hls <- smacofRobust(delta, engine = smacofHuber, cons = 10, verbose = FALSE, itmax = 10000)
```
We start with a least squares analysis, actually with Huber loss with
$c=10$, which for these data is equivalent to least squares. The process converges in `r formatC(hls$itel, format = "d")` iterations.

```{r pxls, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Least Squares"}
df <- data.frame(dim1 = hls$x[, 1],
                 dim2 = hls$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```


```{r pdls, echo = FALSE, fig.align = "center", fig.cap = "Grujter Shepard Plot Least Squares"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hls$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hls$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hls$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

The Shepard plot clearly shows why an additive constant would be very beneficial in this case.

```{r phlsh, fig.align = "center", fig.cap = "Gruijter Histogram Least Squares Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hls$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Least Absolute Value

```{r av, echo = FALSE}
hav <- smacofRobust(delta, engine = smacofCharbonnier, cons = .001, verbose = FALSE, itmax = 100000, eps = 1e-15)
```
For our LAV smacof we use engine smacofCharbonnier with $c=.001$.
We have convergence in `r formatC(hav$itel, format = "d")` iterations.

```{r pxav, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Least Absolute Value"}
df <- data.frame(dim1 = hav$x[, 1],
                 dim2 = hav$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdav, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Least Absolute Value"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hav$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hav$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hav$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

In the Shepard plot we see that there are a number of dissimilarities which
are fitted exactly. If we count them there are about 15-20. Note that 
configurations in two dimensions have $(n-1)+(n-2)=2n-3$ degrees of freedom, which is
15 in this case. Thus if we take the 15 dissimilarities which are 
fitted exactly, give them weight one, give all other 21 dissimilarities
weight zero, and do a regular non-robust smacof analysis using these weights, then we will have perfect fit in two dimensions, and the solution will be
the LAV solution. All this is easier said than done,
because it presumes that we use Charbonnier loss with $c=0$ and that we are able to decide which residuals are exactly equal to zero. The LAV analysis also suggests the possibility of
a huge number of local minima,
because there are so many ways to pick 15 out of 36 dissimilarities.

```{r phavh, fig.align = "center", fig.cap = "Gruijter Histogram Least Absolute Value Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hav$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Huber

```{r hme, echo = FALSE}
hme <- smacofRobust(delta, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```
smacofHuber with $c=1$ converges in `r formatC(hme$itel, format = "d")` iterations.

```{r pxhme, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Huber c = 1"}
df <- data.frame(dim1 = hme$x[, 1],
                 dim2 = hme$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdhme, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Huber c = 1"}
df1 <- data.frame(
      delta = delta[jndex],
      d = hme$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = hme$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = hme$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 12) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

```{r phmeh, fig.align = "center", fig.cap = "Gruijter Histogram Huber Residuals", echo = FALSE}
residuals <- abs(delta[iall] - hme$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Tukey

```{r htu, echo = FALSE}
htu <- smacofRobust(delta, engine = smacofTukey, cons = 2, verbose = FALSE, itmax = 10000)
```
smacofTukey with $c=2$ converges in `r formatC(htu$itel, format = "d")` iterations.

```{r pxtmed, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Configuration Tukey c = 2"}
df <- data.frame(dim1 = htu$x[, 1],
                 dim2 = htu$x[, 2],
                 names = names)
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdtmed, echo = FALSE, fig.align = "center", fig.cap = "Gruijter Shepard Plot Tukey c = 2"}
df1 <- data.frame(
      delta = delta[jndex],
      d = htu$d[jndex]
)
df2 <- data.frame(
          delta = delta[icpn],
          d = htu$d[icpn],
          names = rep("C", length(icpn))
)
df3 <- data.frame(
          delta = delta[ibp],
          d = htu$d[ibp],
          names = rep("B", length(icpn))
)
ggplot(df1, aes(delta, d)) + xlim(0, 10) + ylim(0, 10) +
  geom_point(data = df1, colour = "red") + 
  geom_text(aes(label = names), data = df2, colour = "blue") +
  geom_text(aes(label = names), data = df3, colour = "blue") +
  geom_abline(slope = 1, intercept = 0)
```

```{r phtuh, fig.align = "center", fig.cap = "Gruijter Histogram Tukey Residuals", echo = FALSE}
residuals <- abs(delta[iall] - htu$d[iall])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

## Rothkopf

Our second example are the Rothkopf Morse data (@rothkopf_57), which have a better fit and have fewer outliers than the Gruijter data. We used the asymetric confusion matrix from the smacof package (@deleeuw_mair_A_09c) and defined dissimilarities by the Shepard-Luce formula
$$
\delta_{ij}=-\log\frac{p_{ij}p_{ji}}{p_{ii}p_{jj}}.
$$

### Least Squares


```{r mls, echo = FALSE}
jndex <- outer(1:36, 1:36, ">")
mhls <- smacofRobust(morse, engine = smacofHuber, cons = 25, verbose = FALSE, itmax = 10000)
```
For least squares we use the smacofHuber engine with $c=25$, well outside the range of the residuals. We have convergence in `r mhls$itel` iterations.


```{r mpxls, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Least Squares"}
df <- data.frame(dim1 = mhls$x[, 1],
                 dim2 = mhls$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mkpdls, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Least Squares"}
df <- data.frame(
      delta = morse[jndex],
      d = mhls$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphlsh, fig.align = "center", fig.cap = "Rothkopf Histogram Least Squares Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhls$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Least Absolute Value

```{r mav, echo = FALSE}
mhav <- smacofRobust(morse, engine = smacofCharbonnier, cons = .001, verbose = FALSE, itmax = 10000)
```
For least absolute value we use Chardonnier loss with $c=.001$. We have convergence in `r mhav$itel` iterations.

```{r mpxav, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Least Absolute Value"}
df <- data.frame(dim1 = mhav$x[, 1],
                 dim2 = mhav$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mpdav, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Least Absolute Value"}
df <- data.frame(
      delta = morse[jndex],
      d = mhav$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphlavh, fig.align = "center", fig.cap = "Rothkopf Histogram Least Absolute Value Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhav$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Huber

```{r mhme, echo = FALSE}
mhme <- smacofRobust(morse, engine = smacofHuber, cons = 1, verbose = FALSE, itmax = 10000)
```

smacofHuber with $c=1$ converges in `r mhme$itel` iterations.

```{r mpxhme, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Huber c = 1"}
df <- data.frame(dim1 = mhme$x[, 1],
                 dim2 = mhme$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r mpdhme, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Huber c = 1"}
df <- data.frame(
      delta = morse[jndex],
      d = mhme$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphmeh, fig.align = "center", fig.cap = "Rothkopf Histogram Huber Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhme$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```

### Tukey


```{r mhtu, echo = FALSE}
mhtu <- smacofRobust(morse, engine = smacofTukey, cons = 1, verbose = FALSE, itmax = 10000)
```

Tukey with $c=1$ converges in `r mhtu$itel` iterations.


```{r mpxtu, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Configuration Tukey c = 1"}
df <- data.frame(dim1 = mhtu$x[, 1],
                 dim2 = mhtu$x[, 2],
                 names = row.names(morse))
ggplot(df, aes(dim1, dim2)) + geom_text(aes(label = names), colour = "red")
```

```{r pdttu, echo = FALSE, fig.align = "center", fig.cap = "Rothkopf Shepard Plot Tukey c = 1"}
df <- data.frame(
      delta = morse[jndex],
      d = mhtu$d[jndex]
)
ggplot(df, aes(delta, d)) + xlim(0, 9) + ylim(0, 9) +
  geom_point(colour = "red") +
  geom_abline(slope = 1, intercept = 0)
```

```{r mphtuh, fig.align = "center", fig.cap = "Rothkopf Histogram Tukey Residuals", echo = FALSE}
residuals <- abs(morse[jndex] - mhtu$d[jndex])
ggplot(as.data.frame(residuals), aes(residuals)) + geom_histogram(binwidth = .5)
```
\sectionbreak

# Literature {#sec-literature}

The literature on results like @thm-wght and @thm-sqrt is difficult to review. There are various reasons for that. Relevant results have
been published in robust statistics, computational statistics, optimization, location analysis, image restoration, sparse recovery. As is often the case, there are not many references between fields, almost everything is within.
Even the names of the loss functions differ between fields.
Much of it is hard to find in conference proceedings. Also, in most cases, the authors have specific  applications in mind, which they then embed in a likelihood, Bayesian, linear regression, logistic regression, facility location, or EM framework and language.

@deleeuw_lange_A_09 give some references to previous work on results like  @thm-wght, notably @groenen_giaquinto_kiers_03, @jaakkola_jordan_00, and @hunter_li_05. In these earlier papers we do not find @thm-wght in its full
generality. In @groenen_giaquinto_kiers_03 majorization of the log logistic 
function is considered. Besides requiring equality of the function and the majorizing quadratic at the support point $y$ they also require equality
at $-y$ and then check that the resulting quadratic is indeed a majorizer.
In @jaakkola_jordan_00 also consider a symmetrized version of the log
logistic function. They note that the resulting function is a convex
funcion of $x^2$, and use a linear majorizer at $x^2$ to obtain a
quadratic majorization. @hunter_li_05 come closest to @thm-wght.
In their proposition 3.1 they approximate the general penalty function they use for varable selection at $y$ by a quadratic with coefficient $f'(y)/2y$, and
then show that it provides a quadratic majorization. In neither of the
three papers there is a notion of sharp quadratic majorization. 

I will discuss some of the literature under the headings "robust statistics", "location analysis", and "sparse recovery". Since I most definitely am not
an expert in either of these three fields the literature reviews will be biased and incomplete. A final section, where I am somewhat more sure-footed, is
"multivariate analysis".

## Robust Statistics
 
In robust statistics it has been known for a long time that iterative reweighted least squares (IRLS) with weights $f'(x)/x$ gives a quadratic majorization algorithm. This result, and the corresponding IRLS algorithm, is often attributed to @beaton_tukey_74.

## Location Analysis

In location analysis the first majorization/IRLS method is generally 
attributed to a 16-year old Hungarian mathematics prodigy (@weiszfeld_37). His 
algorithm avant-la-lettre was intended to find the minimum of a function of
the form
$$
\sigma(x)=\sum_k w_kd_k(x),
$${#eq-web}
where $d_k(x)=\|x-y_k\|$, over $x$ in $\mathbb{R}^n$. The $y_k$ are known locations, called *anchors* in the literature, and the norm is Euclidean. Actually @weiszfeld_37 did not use weights $w_k$ and worked in
three-dimensional space. There is an English translation of Weiszfeld's paper, with bibliography and comments, in @weiszfeld_plastria_09. 

The problem of minimizing @eq-web
is known under various names, usually consisting of
one of the seven different non-empty selections from the triple (Torricelli, Fermat, Weber). The history of the problem is discussed, for example, in @plastria_11.

Weiszfeld first acknowledges that @sturm_84 has already established the
existence and uniqueness of the minimum point. He then proceeds to give three new proofs. We are interested in the first one, described in his first theorem. It defines the iterative sequence
$$
x^{(\nu+1)}=\frac{\sum\omega_k(x^{(\nu)})y_k}{\sum\omega_k(x^{(\nu)})}
$${#eq-wup}
with weights
$$
\omega_\nu(x)=\frac{1}{d_k(x^{(\nu)})}.
$${#eq-ww}
The proof then consists of showing that the sequence converges to the
unique minimum point of @eq-web. It would have been nice if we were
told where @eq-wup came from, but it is simply taken as the starting
point. 

We can guess what suggested this particular sequence. Observe first that the correponding problem with $d_k(x)$ replaced by $d_k^2(x)$ is easy to solve. The solution is simply the weighted mean of the $y_k$. This suggests the rewrite
$$
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k(x)}d_k^2(x),
$${#eq-rew}
which in turn suggests @eq-wup. Alternatively, differentiate
the loss in @eq-web and set the partials equal to zero. This
gives
$$
\sum_k w_k\frac{1}{d_k(x)}(x-y_k)=0,
$${#eq-wstatio1}
or
$$
x=\frac{\sum\omega_k(x)y_k}{\sum\omega_k(x)}.
$${#eq-wstatio2}
@sturm_84 mentions that he derived his existence and uniqueness theorem
without using differentiation, but he mentions a paper by Lorenz 
LindelÃ¶f from 1866 which derives his necessary conditions using
differentiation (I have not been able to find a copy of that paper).
The discussion is a clear example of the nineteenth century tension between using synthetic (geometric) methods or analytic (calculus) methods. 

Even more synthetic were the methods in a paper by LamÃ© and Clapeyron
of 1829. (I have not been able to find a copy of that paper either).
There is a partial translation in @franksen_grattan-guinness_89.
LamÃ© and Clapeyron suggest solving their "moindre distances" problems,
which generalize the single facility location problem in various ways,
by ingeneous systems of pulleys. There is a translation of the
general principles section of the LamÃ© and Clapeyron
paper in @franksen_grattan-guinness_89. From that translation we read in 
General Principle 9 their defense of the mechanical method they propose.

> But if one considers that the proposed problem is totally insoluble, in its entire generality, by the current means of analysis and geometry; that it is only in very special and very simple cases that one may obtain a complete graphical solution; that finally in the applications the data themselves are only approximate; one will be forced to adrmt that in the state of imperfection in which algebraic analysis is still found today, the manner of solution in question here is the only one which obtains for the proposed problem.

In General Principle 21 they suggest an iterative method of "trial and error"
to solve the weighted location problem, where the weights in each iteration
are adjusted by multiplying them with the inverse of the distances in the previous iteration. Thus they propose the Weiszfeld algorithm, albeit in a version using pulleys.



Over the years the location problem has been
generalized in numerous directions, to multiple locations, to using different norms, to unknown anchors, to nonlinear manifolds, and to obnoxious anchors you want to be far from. A good recent overview is @beck_sabach_15. A paper close in spirit to our paper is @aftab_hartley_trumpf_15, which has generalizations to $\ell_q$ norms and to Riemannian manifolds of non-negative curvature.

There is a huge literature on the convergence of the Weiszfeld algorithm.
As in our @sec-amgm we can simply use AM/GM inequality.
Thus
$$
\|x-y_\nu\|\leq\frac12\frac{1}{\|x^{(\nu)}-y_\nu\|}(\|x-y_\nu\|^2+\|x^{(\nu)}-y_\nu\|^2),
$${#eq-webamgm}
which immediately gives @eq-wup. Convergence follows from the general
majorization of MM theory.

Unlike robust smacof the Toricelli-Fermat-Weber problem is convex, 
and consequently has no problems with non-global local minima.
A most elegant proof of convergence using the tools of modern convex analysis is in @mordukhovich_nam_19. Older proofs sometimes have difficulty dealing with cases in which the iterates coincide with one of the anchors or in which the solution is actually one of the anchors. This creates problems similar to the problems in our @sec-zero, but in this simple case the problem is can be completely resolved using convexity and has no serious algorithmic consequences.

In a straightforward generalization of the Toricelli-Fermat-Weber problem , which is particularly relevant for the developments in our paper, @katz_69 proposes to minimize
$$
\sigma(x)=\sum_{k=1}^mw_kd_k^q(x),
$${#eq-katzp}
and even 
$$
\sigma(x)=\sum_{k=1}^mw_kf_k(d_k(x))
$${#eq-katzf}
with Euclidean distances and $f_k$ functions defined on the non-negative reals. Note that if $f$ is convex ands increasing then $\sigma$ of
@eq-katzf is convex. If $q\geq 1$ then $\sigma$ of @eq-katzp is convex.

The algorithm Katz suggests for minimizinf $\sigma$ of @eq-katzp generalizes the decompostion in @eq-rew to
$$
\sigma(x)=\sum_k w_kd_k(x)=\sum_kw_k \frac{1}{d_k^{2-q}(x)}d_k^2(x),
$${#eq-katzdec}
which leads to
$$
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{1}{d_k^{2-q}(x^{(\nu)}}}.
$${#eq-katzupdp}
For @eq-katzf, analogous  with @eq-wstatio1,
we set the derivative of @eq-katzf equal to zero. Thus we solve
$$
\sum_{k=1}^mw_k\frac{f_k'(d_k(x))}{d_k(x)}(x-y_k)=0,
$${\#eq-katzder}
and the iteration becomes
$$
x^{(\nu+1)}=\frac{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}y_k}{\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)}))}{d_k(x^{(\nu)})}}.
$${#eq-katzupdf}
The conditions in @katz_69 on $f$ needed for the convergence proof, and also the proof itself, are rather complicated. We can simply use the conditions of @thm-wght that $f'(x)/x$ is non-increasing on the non-negatives reals to construct a quadratic majorization algorithm in which we minimize 
$$
\sum_{k=1}^m\sum_{k=1}^mw_k\frac{f'(d_k(x^{(\nu)})}{d_k(x^{(\nu)})}d_k^2(x)
$${#eq-katzmajor}
to find $x^{k+1}$. This gives directly the update @eq-katzupdp.

## Sparse Recovery

This is a field which is difficult to delineate. A somewhat ad-hoc definition is recovering complete information from incomplete information, often in the
context of specific engineering problems. There is overlap with signal detection, image analysis, matrix completion, ... But "sparse recovery" scientific activities "sparse recovery" could be extended far beyond these
boundaries. Since classical statistics infers properties of the population from those of a sample it is a form of sparse recovery. Since science infers properties of the real world from outcomes of experiments it is sparse recovery too.

## Multivariate Analysis

The smacof majorization method for multidimensional scaling was first presented
at the *US-Japan Seminar on Theory, Methods and Applications of Multidimensional Scaling and Related Techniques* at UCSD in La Jolla, August 1975. Shortly after that I read the basic EM paper by @dempster_laird_rubin_77,
and shortly after that I realized that smacof and EM were both special
cases of a general minimization strategy, which I called majorization
at the time. In June 1978 both Nan Laird and I attended the *Fifth International Symposium on Multivariate Analysis* at the University of Pittsburgh. I remember mentioning majorization, excitedly, to Nan on the conference bus. 

The smacof majorization method was fully discussed in @deleeuw_C_77, @deleeuw_heiser_C_77, and @deleeuw_heiser_C_80. The familiar picture illustrating two steps of the general majorization algorithm first appears in @deleeuw_A_88b. But unlike EM, which took off as a rocket in 1977, the general idea of majorization remained unpublished, until @deleeuw_C_94c and @heiser_95. 
Majorization was used regularly in the
Gifi project. The book @gifi_B_90, which is a version of 1981 lecture
notes, mentions majorization only once, but since then a stream of
papers and dissertations from the Data Theory department in Leiden
using majorization appeared. @heiser_95 mentions most of them. In @deleeuw_C_88b another large majorization
subfield, the *aspect approach* to multivariate analysis, was developed.
In section 7 of that paper the general majorization/minorization approach to optimization is outlined, possibly for the first time in print.

Robust versions of low rank matrix approximation, a.k.a. principal component
analysis, were first considered by @gabriel_odoroff_84. They start by
discussing the alternating least squares algorithm for least squares weighted matrix approximation of @gabriel_zamir_79. The alternating is to compute new row scores for currently fixed column scores by linear regression, and then computing new column scores corresponding with the new row scores, again by
linear regression. @gabriel_odoroff_84 suggest to replace the linear least
squares weighted averages in each of the two stages by medians or trimmed
means to get a robust PCA. There is no sign of a convergence proof, but there
is the suggestion to use alternating least absolute value methods to minimize
the sum of absolute residuals of the matrix approximation. This suggestion
was taken up by @verboon_heiser_94 using the majorization approach and the
Huber and Tukey robust loss functions. Their robust PCA method is very similar 
to our robust MDS method, but the presentation of their method has
some magical elements. The Huber and Tukey majorization functions are
presented without any discussion where they came from, and it is then verified
that they are indeed majorizations. There is clearly nothing wrong with this, but using our @thm-wght gives a more general and more direct approach.

@heiser_86 was the first to connect the Weiszfeld problem with correspondence analysis and multidimensional scaling, emphasizing the majorization aspects.
As we have seen in @heiser_87 and @heiser_88 he constructed majorization
algorithms for multidimensional scaling and correspondence analysis.

The IRLS approach to robustifying multivariate matrix approximation techniques could easily lead to a large and varied number of publications. There are some excellent examples making their way through the usual publication channels. I will just give two recent examples, with good bibliographies. They are Huber Principal Component Analysis (@he_li_liu_zhou_23) and Cauchy Factor Analysis (@li_24).

\sectionbreak

# Discussion

## Bounding the Second Derivative

In some cases our basic theorems may not apply, but there may be an alternative
way to majorize loss. In fact, this is classic quadratic bounding as in @vosz_eckhardt_80 or @boehning_lindsay_88. As before, we want to minimize
$\sum \omega_k\ f(\delta_k-d_k(X))$,
but now we suppose that there is a $K>0$ such that $f''(x)\leq K$. We then have
the majorization

$$
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
$${#eq-qbound}
and in iteration $k$ we minimize, or at least decrease, 
$$
\sum \omega_k\left[d_k(X)-\{d_k(X^{(\nu)})-K^{-1}f'(\delta_k-d_k(X^{(\nu)}))\}\right]^2
$${#eq-qiter}
Note that in this algorithm the weights do not change.  Instead
of fitting a fixed target with moving weights, we fit a
moving target with fixed weights.

We can apply bounding the second derivative, for example, to Charbonnier loss, using the inequality
$$
f_c''(x)=(x^2 + c^2)^{-\frac12}-x^2(x^2 + c^2)^{-\frac32}\leq(x^2 + c^2)^{-\frac12}\leq c^{-1},
$${#eq-chark}
Of course this method requires that the second derivative exists at $x$.
Although I have not done any comparisons it will probably require more
iterations and take longer than the method in @sec-charb.

The paper by @vosz_eckhardt_80 deserves some special mention here.

$$
\mathcal{D}^2\sigma(x)=\sum\frac{1}{d_i(x)}\left\{I-\frac{(x-y_i)(x-y_i)'}{d_i^2(x)}\right\}
$$

## Fixed Weights

One could also consider using the fixed weights in regular non-robust smacof
to achieve some form of robustness. Redefine stress as
$$
\sigma(X):=\sum_k \omega_kf(\delta_k)(\delta_k-d_k(X))^2
$${#eq-wstress}
For example, we can choose a negative power for $f$, so that it downweights the large dissimilarities. If the dissimilarities is large, then it should have less influence on the fit, and thus on the solution $X$. This type of fixed power-weighting is used in various places (@deleeuw_heiser_C_80, @groenen_vandevelden_16) to approximate loss functions such the one with logarithmic residuals in @ramsay_77.

But we have to keep in mind that downweighting large dissimilarities
is not the same thing as downweighting large residuals. The residuals
depend on $X$, and it is perfectly possible that some small dissimilarities
have large residuals. On the other hand emphasizing small dissimilarities
in the loss function means that we want small dissimilarities to be
fitted relatively well, which means that on average we want small dissimilarities to have small residuals. The Shepard plot will tend to 
fan out at the high end.

Despite these reservations, it will be useful to study if and how fixed weights can be used to improve robustness of smacof. If only because fixed weights correspond with a simpler and presumably more efficient algorithm.

## Residual Definition

In our examples and in our code we use the residuals $\delta_k-d_k(X)$ are
arguments of our loss functions. From the statistical point of view we have to remember, however, that most of these loss functions were designed for the
robust estimation of a location parameter or a linear regression function.
The error distributions were explicitly or implicitly assumed to be symmetric
around zero, and defined on the whole real line, which was reflected in the fact that loss functions were even and had infinite support.
In MDS, however, distances and dissimilarities are non-negative and reasonable
error functions are not symmetric. One could follow the example of @ramsay_77 and measure residuals as $\log\delta_{ij}-\log d_{ij}(X)$. This does not have
any effect on the majorization of the loss functions, but it means that in
the smacof step to find $X^{(\nu+1)}$ we have to minimize
$$
\sigma(X)=\sum \omega_k(X^{(\nu)})(\log\delta_{ij}-\log d_{ij}(X))^2,
$$
which is considerably more complicated (@deleeuw_groenen_mair_E_16a).

## Robust Nonmetric MDS

Our discussion and our software is all about metric MDS. It seems easy to
extend the discussion to non-linear and non-metric MDS by adding an
alternating least squares step optimally scaling the dissimilarities.
This would take place between two majorizations of the robust loss
function, so one or more transformation and smacof steps can be taken
between updating the weights. But this paradigm does not work for
robust smacof. 

Consider any hard redescender, such as Tukey or Hinich. At iteration $\nu$,
for current weights, first first improve the configuration, then compute the optimal transformation of the dissimilarities, and then compute new weights.
This is a recipe for disaster. At some point we minimize
$$
\sigma(\hat d)=\sum \omega_k(X^{(\nu)})(\hat d_k-d_k(X^{(\nu+1)}))^2
$${#eq:zero}
over the disparities $\hat d$, which must be monotone with the dissimilarities.
Because of the hard redescending some of the weights, for current
absolute residuals larger than $c$, will be zero. The monotone
regression is done for the observations with non-zero weights,
and the disparities corresponding with zero weights are
only determined by the order they are required to have. Thus
they can be freely chosen in an interval between two disparities
obtained from the monotone regression. That interval can be large, in
fact if one of the zero weights corresponds with the largest
dissimilarity it can be infinite. What we choose in the
interval will determine the new residual and thus the next set 
of weights. 

In the unfortunate situation that the current absolute 
residuals are all larger than $c$, even after choosing
the optimal $\hat d$, the next weights will all be zero
and the algorithm stops with zero stress.

## Practicalities

Recommending one particular loss function from the many we have discussed is
not easy. In some cases, for example for Cauchy loss, one can justify the
choice of a loss function by assuming a particular error distribution
and using the maximum likelihood principle. But in general perhaps the best way to proceed for a given MDS problem is
to take what we could call a *trajectory approach*. Choose one particular
parametric family, for example the Huber one, and compute the robust
smacof solution $X(c)$ for a number of increasing positive $c$ values. For small $c$ we start with a close approximation of the LAV solution, increasing
$c$ will eventually take us to the LS solution. The starting point for
computing each solution will be the solution for the previous $c$. 
We can plot the trajectory of the points in the configurations $X(c)$,
and even make an animation. It seems that the Huber family is a good
candidate for such a study, with the generalized Charbonnier a good
second. If the main concern is to suppress the influence of outliers
then trying some of the hard redescenders, such as the Tukey family,
makes sense. Studying trajectories for some of robust loss functions is clearly interesting, but it is not something we can or will explore in this paper.



\sectionbreak

# Code

The function smacofRobust has a parameter "engine", which can be equal
to smacofCharbonnier, smacofGeneralizedCharbonnier, smacofBarron, smacofHuber, smacofTukey, smacofHinnich, smacofCauchy, smacofFair, smacofAndrews, smacofLogistic, smacofWelsch, or smacofGaussian. These thirteen small modules compute the respective loss function values and
weights for the IRLS procedure. This makes it easy for interested parties to add additional robust loss functions.

```{r code, eval = FALSE}
{{< include smacofRobust.R >}}
```

\sectionbreak

# References
