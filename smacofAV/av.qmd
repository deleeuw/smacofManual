---
title: "Robust Least Squares Multidimensional Scaling"
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: We use an iteratively reweighted version of the smacof
  algorithm to minimize various robust multidimensional scaling 
  loss functions. Our results use a general theorem on sharp 
  quadratic majorization of @deleeuw_lange_A_09. We relate this
  theorem to earlier results in robust statistics, localization
  theory, and sparse recovery. Code in R is included. 
---

\sectionbreak

```{r gruijterdata, echo = FALSE}
gruijter <-
  structure(
    c(
      5.63,
      5.27,
      4.6,
      4.8,
      7.54,
      6.73,
      7.18,
      6.17,
      6.72,
      5.64,
      6.22,
      5.12,
      4.59,
      7.22,
      5.47,
      5.46,
      4.97,
      8.13,
      7.55,
      6.9,
      4.67,
      3.2,
      7.84,
      6.73,
      7.28,
      6.13,
      7.8,
      7.08,
      6.96,
      6.04,
      4.08,
      6.34,
      7.42,
      6.88,
      6.36,
      7.36
    ),
    names = c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66"),
    Size = 9L,
    call = quote(as.dist.default(m = polpar)),
    class = "dist",
    Diag = FALSE,
    Upper = FALSE
  )
delta <- as.matrix(gruijter)
names <- c("KVP", "PvdA", "VVD",
               "ARP", "CHU", "CPN", "PSP", "BP", "D66")
row.names(delta) <- colnames(delta) <- names
index <- outer(1:9, 1:9, ">")
```

```{r loadcode}
source("smacofRobust.R")
```


# Introduction

The title of this chapter seems something paradoxical. Least squares estimation
is typically not robust, it is sensitive to outliers and pays a lot of attention to fitting the larger observations. What we mean by robust least squares MDS, however, is using the smacof machinery designed to minimize loss of the form
\begin{equation}
\sigma_2(X):=\sum w_k(\delta_k-d_k(X))^2\label{eq:stressdef},
\end{equation}
to minimize robust loss functions. The prototypical robust loss function is 
\begin{equation}
\sigma_1(X):=\sum w_k|\delta_k-d_k(X)|\label{eq:stradddef},
\end{equation}
which we will call
*strife*, because stress, sstress, and strain are already taken.

Strife is not differentiable at configurations $X$ for which there is at least one $k$ for which either $d_k(X)=\delta_k$ or $d_k(X)=0$ (or both). This lack of differentiability complicates the minimization problem. Moreover experience
with one-dimensional and city block MDS suggests that having many points
where the loss function is not differentiable leads to (many) additional local minima.

In this chapter we will discuss (and implement) various variations of 
$\sigma_1$ from \eqref{eq:stradddef}. They can be interpreted in two different
ways. On the one hand we use smoothers of the absolute value
function, and consequently of strife. We want to eliminate the
problems with differentiability, at least the ones caused by
$\delta_k=d_k(X)$. If this is our main goal, then we want to choose
the smoother in such a way that it is as close to the absolute value
function as possible. This is not unlike the distance
smoothing used by @pliner_96 and @groenen_heiser_meulman_99 in 
the global minimization of $\sigma_2$ from \eqref{eq:stressdef}.

On the other hand our modified loss function can be interpreted
as more robust versions of the least squares loss function, and
consequently of stress. Our goal here is to combines the robustness of the
absolute value function with the efficiency and computational ease 
of least squares. If that is our goal then there is no reason to
stay as close to the absolute value function as possible.

Our robust or smooth loss functions are all of the form
\begin{equation}
\sigma(X):=\sum w_k\ f(\delta_k-d_k(X))\label{eq:strifedef},
\end{equation}
for a suitable choice of the real valued function $f$. We will
define what we mean by "suitable" later on. For now, note that loss
\eqref{eq:stressdef} is the special case with $f(x)=x^2$ and loss \eqref{eq:stradddef} 
is the special case with $f(x)=|x|$. 

\sectionbreak

# Majorizing Strife

The pioneering work in strife minimization using smacof is @heiser_88, building on earlier work in @heiser_87. It is based on a creative use of the Arithmetic Mean-Geometric Mean (AM/GM) inequality to find a majorizer of the absolute
value function. For the general theory of majorization algorithms (now more commonly known as MM algorithms) we refer to their original introduction in @deleeuw_C_94c and to the excellent book by @lange_16.

The AM/GM inequality says that for all non-negative $x$ and $y$ we have 
\begin{equation}
|x||y|=\sqrt{x^2y^2}\leq\frac12(x^2+y^2),\label{eq:amgm}
\end{equation}
with equality if and only if $x=y$. If $y>0$ we can write \eqref{eq:amgm} as
\begin{equation}
|x|\leq\frac12\frac{1}{|y|}(x^2+y^2),\label{eq:amgmmaj}
\end{equation}
and this provides a quadratic majorization of $|x|$ at $y$. There is no quadratic
majorization of $|x|$ at $y=0$, which is a nuisance we must deal with.

Using the majorization \eqref{eq:amgmmaj}, and assuming $\delta_k\not= d_k(Y)$ for all $k$, we define
\begin{equation}
\omega_1(X):=\frac12\sum w_k\frac{1}{|\delta_k-d_k(Y)|}((\delta_k-d_k(Y))^2+(\delta_k-d_k(X))^2).\label{eq:omegadef}
\end{equation}
Now $\sigma_1(X)\leq\omega_1(X)$ for all $X$ and $\sigma_1(Y)=\omega_1(Y)$. Thus
$\omega_1$ majorizes $\sigma_1$ at $Y$.

## Algorithm 

Define
\begin{equation}
w_k(Y):=w_k\frac{1}{|\delta_k-d_k(Y)|}.\label{eq:wk1def}
\end{equation}
Reweighted smacof to minimize strife computes $X^{(k+1)}$ by decreasing
\begin{equation}
\sum w_k(X^{(k)})(\delta_k-d_k(X^{(k)}))^2,\label{eq:sstrf}
\end{equation}
using a standard smacof step. It then computes the new weights $w_k(X^{(k+1)})$ from \eqref{eq:wk1def}
and uses them in the next smacof step to update $X^{(k+1)}$. And so on, until convergence.

A straightforward variation of the algorithm does a number of smacof steps before
upgrading the weights. This still leads to a monotone, and thus convergent, algorithm.
How many smacof steps we have to take in the inner iterations is something that needs further study. It is likely to depend on the fit of the data, on the shape of the function near the local minimum, and on how far the iterations are from the local minimum.

## Zero Residuals

It may happen that for some $k$ we have $d_k(X^{(k)})=\delta_k$ while iterating. There have been various proposals to deal with such an 
unfortunate event, and we will discuss some of them further on. Even more
importantly we will see that that the minimizer of the absolute value
loss usually satisfies $d_k(X)=\delta_k$ for quite a few elements,
which means that near convergence the algorithm will become unstable
because the weights from \eqref{eq:wk1def} become very large. 

To illustrate the problems with differentiability we compute the directional derivatives of strife. 

Let $s_k(X):=w_k|d_k(X)-\delta_k|$.

1. If $\delta_k=0$ and $d_k(X)=0$ then $ds_k(X;Y)=w_kd_k(Y)$.
2. If $\delta_k>0$ and $d_k(X)=0$ then $ds(X;Y)=-w_kd_k(Y)$.
3. If $d_k(X)>0$ and $d_k(X)-\delta_k>0$ then 
$ds_k(X;Y)=w_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
4. If $d_k(X)>0$ and $d_k(X)-\delta_k<0$ then $ds_k(X;Y)=-w_k\frac{1}{d_k(X)}\text{tr}\ X'A_kY$.
5. If $d_k(X)>0$ and $d_k(X)-\delta_k=0$ then $ds_k(X;Y)=w_k\frac{1}{d_k(X)}|\text{tr}\ X'A_kY|$.

The directional derivative of $\sigma_1$ is consequently the sum of
five terms, corresponding with each of these five cases.

In the case of stress the directional derivatives could be used to prove
that if $w_k\delta_k>0$ for all $k$ then stress is differentiable at each local minimum (@deleeuw_A_84f). For strife to be differentiable we would have to prove that at a local minimum both $d_k(X)>0$ and $(d_k(X)-\delta_k)\not= 0$. 
for all $k$ with $w_k>0$.

But this is impossible by the following argument. In the one-dimensional
case we can partition $\mathbb{R}^n$ into $n!$ polyhedral convex cones
corresponding with the permutations of $x$. Within each cone the distances
are a linear function of $x$. Each cone can be partitioned by intersecting it
with the $2^\binom{n}{2}$ polyhedra defined by the inequalities $\delta_k-d_k(x)\geq 0$ or $\delta_k-d_k(x)\leq 0$. Some of these intersections can and will obviously be empty. Within each of these non-empty polyhedral regions strife is a linear function of $x$. Thus it attains its minimum at a vertex of the region, which is a solution for which some
distances are zero and some residuals are zero. There can be no  
minima, local or global, in the interior of one of the polyhedral regions.
We have shown that in one dimension strife is not differentiable at a local minimum, and that there is presumably a large number of them. Of course even for moderate $n$ the number of regions, which is maximally $n!2^\binom{n}{2}$, 
is too large to actually compute or draw. 

In the multidimensional case linearity goes out the window. The
set of configurations $d_k(X)=\delta_k$ is an ellipsoid and $d_k(X)=0$ is a hyperplane. Strife is not differentiable at all intersections of these
ellipsoids and hyperplanes. The partitioning of $\mathbb{R}^n$ by these
ellipsoids and hyperplanes is not simple to describe. It has convex
and non-convex cells, and within each cell strife is the difference
of two weighted sums of distances. Anything can happen.

\sectionbreak

# Generalizing Strife

A function $g$
*majorizes* a function $f$ at $y$ if $g(x)\geq f(x)$ for all $x$ and $g(y)=f(y)$.
Majorization is *strict* if $g(x)>f(x)$ for all $x\not= y$. If $\mathfrak{H}$ is a family of functions that all majorize $f$ at $y$ then $h\in\mathfrak{H}$ is a *sharp majorization* if $h(x)\leq g(x)$ for all $g\in\mathfrak{H}$. 
The AM/GM inequality was used in the previous section to construct a quadratic majorization of strife. 

We are specifically interested in this chapter in sharp quadratic majorization, in
which $\mathfrak{H}$ is the set of all convex quadratics that majorize $f$ at $y$.
This case has been studied in detail (in the case of real-valued functions on the line)
by @deleeuw_lange_A_09. Their Theorem 4.5 on page 2478 says

>Theorem 4.5: Suppose $f(x)$ is an even, differentiable function on $\mathbb{R}$ such that the ratio 
$f'(x)/x$ is non-increasing on $(0,\infty)$. Then the even quadratic
\begin{equation}
g(x)=\frac{f'(y)}{2y}(x^2-y^2)+f(y)\label{eq:sharp}
\end{equation}
is a sharp quadratic majorizer of $f$ at the point $y$.

>Theorem 4.6. The ratio $f'(x)/x$ is decreasing on $(0,\infty)$ if and only 
$f(\sqrt(x))$ is concave. The set of functions satisfying this condition is closed under the formation of (a) positive multiples, (b) convex combinations, (c) limits, and (d) composition with a concave increasing function $g(x)$.

Note that these theorems only give a sufficient condition for quadratic
majorization (in fact, for sharp quadratic majorization). 

We now apply this theorem to functions of the form
\begin{equation}
\sigma_f(X):=\sum w_k\ f(\delta_k-d_k(X)),\label{eq:fstressdef}
\end{equation}
where $f$ satisfies the conditions in the theorem. If
\begin{equation}
\omega_f(X):=\sum w_k\frac{f'(\delta_k-d_k(Y))}{2(\delta_k-d_k(Y))}\{(\delta_k-d_k(X))^2-(\delta_k-d_k(Y))^2\}+f(\delta_k-d_k(Y)),\label{eq:fstressmaj}
\end{equation}
then $\omega_f$ is a sharp quadratic majorization at $Y$.

Although the absolute value is not differentiable at the origin the theorem can still be applied. It just does not give a majorizer at $y=0$. If $f(x)=|x|$ then
\begin{equation}
g(x)=\frac{1}{2|y|}(x^2-y^2)+|y|=\frac{1}{2|y|}(x^2+y^2),\label{eq:abssharp}
\end{equation}
which is the same as \eqref{eq:amgmmaj}. Thus the AM/GM method gives a sharp
quadratic majorization.

In iteration $k$ the robust smacof algorithm does a smacof step towards minimization of
 $\omega_f$ over $X$. We can ignore the parts of \eqref{eq:fstressmaj} that only depend on $Y$, and 
minimize
\begin{equation}
\sum w_k(X^{(k)})(\delta_k-d_k(X))^2,\label{eq:fstressaux}
\end{equation}
with 
\begin{equation}
w_k(X^{(k)}):=w_k\frac{f'(\delta_k-d_k(X^{(k)}))}{2(\delta_k-d_k(Y))}.\label{eq:wkdef}
\end{equation}
It then recomputes the weights $w_k(X^{(k+1)})$ and goes to the smacof step again. This can be thought of as iteratively reweighted least squares (IRLS), and also as nested majorization, with the smacof majorization within the sharp quadratic majorization of the loss function.

\sectionbreak

# On IRLS

The history of iterative reweighted least squares (IRLS), as applied to
fitting functions to data, is rather complicated. It is mostly used in
fitting linear models, and in minimizing $\ell_p$ power loss. 

Weiszfeld

\sectionbreak

# Charbonnier loss


The first, and perhaps most obvious, choice for smoothing the absolute
value function is
\begin{equation}
f_c(x)=\sqrt{x^2 + c^2}.\label{eq:chardonnier}
\end{equation}
This smoother was previously used by @deleeuw_E_18f in least absolute value regression and in @deleeuw_E_20b in what was called least squares absolute value regression.

In the figure below we show the loss function for $c=1$ (black), $c=0.1$ (red),
$c=0.01$ (blue), and $c=0.001$ (green).

```{r charfig, fig.align = "center", echo = FALSE, fig.cap = "Chardonnier Loss", fig.dim = c(6, 6)}
par(pty = "s")
curve(sqrt(x^2 + 1), from = -3, to = 3, ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.1), from = -3, to = 3, add = TRUE, col = "RED", ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.01), from = -3, to = 3, add = TRUE, col = "BLUE", ylab = "f", ylim = c(0, 4))
curve(sqrt(x^2 + 0.001), from = -3, to = 3, add = TRUE, col = "GREEN", ylab = "f", ylim = c(0, 4))
```

For $c>0$ we have $f_c(x)>|x|$.
If $c\rightarrow 0$ then  $f_c(x)$ decreases monotonically to $|x|$. Also $\min_x|f_c(x)-|x||=c$ attained at $x=0$, which implies
uniform convergence of $f_c$ to $|x|$.



In the engineering literature \eqref{eq:chardonnier} is known as Charbonnier loss, after @charbonnier_blanc-feraud_aubert_barlaud_94, who were possibly
the first engineers to use it. x
@ramirez_sanchez_kreinovich_argaez_14 argue \eqref{eq:chardonnier} is also the "most computationally efficient smooth approximation to $|x|$".

By l'Hôpital
$$
\lim_{x\rightarrow 0}\frac{\sqrt{x^2+c^2}-c}{\frac12x^2}=1.
$$
Of course also
$$
\lim_{x\rightarrow\infty}\frac{\sqrt{x^2+c^2}}{|x|}=1
$$
and
$$
\lim_{x\rightarrow\pm\infty}\sqrt{x^2+c^2}-|x|=0
$$
Thus if $x$ is much smaller than $c$ loss is approximately a quadratic in $x$, 
and if $x$ is much larger than $c$ then loss is approximately the
absolute value. 

Loss function \eqref{eq:chardonnier} is infinitely many times differentiable. Its first
derivative is
$$
f'_c(x)=\frac{1}{\sqrt{x^2+c^2}}x,
$$
which converges, again in the sup-norm and uniformly, to the sign function if $c\rightarrow 0$. The IRLS weights are

$$
w_c(x)=\frac{1}{\sqrt{x^2+c^2}}
$$
which is clearly a decreasing function of $x$ on $\mathbb{R}^+$.


$$
\sigma_c(X):=\sum w_k\sqrt{(\delta_k-d_k(X))^2+c^2}
$$
Now majorization using
$$
\omega(X,X^{(k)}):=\sum w_kw_c(\delta_k-d_k(X^{(k)})(\delta_k-d_k(X))^2
$$


\sectionbreak

# Generalized Charbonnier Loss



$$
f(x):=(x^2+c^2)^q
$$
$$
f'(x)=2qx(x^2+c^2)^{q-1}
$$
$$
w(x)=2q(x^2+c^2)^{q-1}
$$
which is non-increasing for $q\leq 1$.


Not surprisingly there are a large number of generalizations
of Huber-like losses in the engineering community, and in their
maze of conference publications. Without having any confidence
of selecting a representative sample from the literature, we mention
and discuss @barron_17 and @barron_19. These papers also give a number of  useful references. 

$$
f_{\alpha,c}(x)=\frac{|\alpha-2|}{\alpha}\left(\left(\frac{(x/c)^2}{|\alpha-2|}+1\right)^{\alpha/2}-1\right)
$$

\sectionbreak

# Huber Loss



\sectionbreak


# Convolution Smoothers

General

## Huber Loss

The Huber function (@huber_64) is 
$$
f(x)=\begin{cases}
\frac12x^2&\text{ if }|x|<c,\\
c|x|-\frac12 c^2&\text{ otherwise}.
\end{cases}
$$

Because ... Chardonnier loss is also known
as Pseudo-Huber loss.

The Huber function is differentiable, although not twice diffentiable. Its derivative is
$$
f'(x)=\begin{cases}
c&\text{ if }x\geq c,\\
x&\text{ if }|x|\leq c,\\
-c&\text{ if }x\leq -c.
\end{cases}
$$
$$
w(x)=
\begin{cases}
\frac{c}{x}&\text{ if }x\geq c,\\
1&\text{ if }|x|\leq c,\\
-\frac{c}{x}&\text{ if }x\leq -c.
\end{cases}
$$
The Huber function is even and differentiable. Moreover
$f'(x)/x$ decreases from. Thus the theorem applies and
the sharp quadratic majorizer at $y$ is
$$
g(x)=\begin{cases}
\end{cases}
$$

$$
\sigma_k(X)=\begin{cases}
\frac12(\delta_k-d_k(X))^2&\text{ if }|\delta_k-d_k(X)|<c,\\
c|\delta_k-d_k(X)|-\frac12 c^2&\text{ if }|\delta_k-d_k(X)|\geq c.
\end{cases}
$$

$$
\omega_k(x,y)=\begin{cases}
\frac12\frac{c}{|y|}(x^2-y^2)-cy-\frac12c^2&\text{ if }y\leq -c,\\
\frac12x^2&\text{ if }|y|<c,\\
\frac12\frac{c}{|y|}(x^2-y^2)+cy-\frac12c^2&\text{ if }y\geq +c.
\end{cases}
$$
Now $x=\delta_k-d_k(X)$ and $y=\delta_k-d_k(Y)$

$$
\omega_k(X;Y)=\begin{cases}
\frac12\frac{c}{|\delta_k-d_k(Y)|}\{(\delta_k-d_k(X))^2+(d_k(Y)-\delta_k)^2\}-c(\delta_k-d_k(Y))-\frac12c^2&\text{ if }\delta_k-d_k(Y)\leq -c,\\
\frac12(\delta_k-d_k(X))^2&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac12\frac{c}{|\delta_k-d_k(Y)|}\{(\delta_k-d_k(X))^2+(d_k(Y)-\delta_k)^2\}+c(\delta_k-d_k(Y))-\frac12c^2&\text{ if }\delta_k-d_k(Y)\geq +c.
\end{cases}
$$
Thus the MDS majorization algorithm for the Huber loss is to update $Y$ by 
minimizing (or by performing one smacof step to decrease)
$$
\sum w_k(Y)(\delta_k-d_k(X))^2
$$
where
$$
w_k(Y)=\begin{cases}
w_k&\text{ if }|\delta_k-d_k(Y)|<c,\\
\frac{cw_k}{|\delta_k-d_k(Y)|}&\text{ otherwise}.
\end{cases}
$$

## Gaussian Convolution

In @deleeuw_E_18f we also used the
convolution smoother proposed by @voronin_ozkaya_yoshida_15. The idea is
to use the convolution of the absolute value
function and a *mollifier* as the smoothed
function. 

> A smooth function $\psi:\mathbb{R}\rightarrow\mathbb{R}$ is said to be a pdf if it is non-negative, and has area 
$\int\psi(x)dx=1$. For any pdf $\psi$ and any $c>0$, deﬁne the parametric function $\psi_c:\mathbb{R}\rightarrow\mathbb{R}$ by: $\psi_c(x):= \frac{1}{c}\psi (\frac{1}{c})$, for all $x\in\mathbb{R}$. Then $\{\psi_c:c>0\}$ is a family of pdf's, whose support decreases as $c\rightarrow 0$, but the volume under the graph always remains equal to one.

choose a Gaussian pdf.
$$
f(x)=\frac{1}{c\sqrt{2\pi}}\int_{-\infty}^{+\infty}|x-y|\exp\left\{-\frac12(\frac{y}{c})^2\right\}dy
$$

Carrying out the integration gives

$$
f(x)=x\{2\Phi(x/c)-1\}+2c\phi(x/c).
$$
The derivative is
$$
f'(x)=2\Phi(x/c)-1
$$
It may not be immediately obvious in this case that $f'(x)/x$ is decreasing. We prove that its
derivative is negative on $(0,+\infty)$. 
The derivative of $f'(x)/x$ has the sign of $xf''(x)-f'(x)$, which is
$z\phi(z)-\Phi(z)+1/2$, with $z=x/c$. It remains to show that $\Phi(z)-z\phi(z)\geq\frac12$,
or equivalently that $\int_0^z\phi(x)dx-z\phi(z)\geq 0$.
Now if $0\leq x\leq z$ then $\phi(x)\geq\phi(z)$ and thus $\int_0^z\phi(x)dx\geq\phi(z)\int_0^zdx=z\phi(z)$, which completes the proof.


$$
w_k(Y)=
\frac{\Phi((\delta_k-d_k(Y))/c)-\frac12}{\delta_k-d_k(Y)}\\
$$

Convolution with rectangular between c and -c gives the Huber function.
$$
f(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy
$$
$$
f(x)=\frac{1}{2c}\int_{-c}^{+c}|x-y|dy=\begin{cases}
\frac{1}{2c}(x^2+c^2)&\text{ if }|x|\leq c,\\
|x|&\text{ otherwise}.
\end{cases}
$$
$$
f'(x)=\begin{cases}
\frac{1}{c}x&\text{ if }|x|\leq c,\\
\text{sign}(x)&\text{ otherwise}.
\end{cases}
$$
$$
w(x)=\begin{cases}
\frac{1}{c}&\text{ if }|x|\leq c,\\
\frac{1}{|x|}&\text{ otherwise}.
\end{cases}
$$

It is also clear that we can use any scale family of
probability densities to define convolution smoothers. There is an infinite number of possible choices, with finite or infinite support, smooth or nonsmooth, using splines or wavelets, and so on.

\sectionbreak

## Downweighters


## Tukey Loss

## Welsch Loss

$$
f(x)=1-\exp(-\{\frac{x}{c}\}^2)
$$

$$
f'(x)=\frac{2}{c^2}\exp(-\{\frac{x}{c}\}^2)x
$$

## Cauchy Loss

$$
f(x)=\log(\{\frac{x}{c}\}^2+1)
$$

$$
f'(x)=\frac{1}{c^2}x\frac{1}{\{\frac{x}{c}\}^2+1}
$$
$$
w(x)=\frac{1}{c^2}\frac{1}{\{\frac{x}{c}\}^2+1}
$$
which is non-increasing on $\mathbb{R}^+$.

# Example

The example we use are dissimilarities between nine Dutch political parties,
collected by @degruijter_67.

```{r gruijter}
delta
```

```{r ls, echo = FALSE}
hls <- smacofRobust(delta, engine = smacofHuber, cons = 10, verbose = FALSE, itmax = 10000)
```
```{r pxls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Squares"}
par(pty = "s")
plot(hls$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hls$x, names)
```

```{r pdls, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Squares"}
par(pty = "s")
plot(delta[index], hls$d[index], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r av, echo = FALSE}
hav <- smacofRobust(delta, engine = smacofHuber, cons = .01, verbose = FALSE, itmax = 10000)
```

```{r pxav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Least Absolute Value"}
par(pty = "s")
plot(hav$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hav$x, names)
```

```{r pdav, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Least Absolute Value"}
par(pty = "s")
plot(delta[index], hav$d[index], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r hmed, echo = FALSE}
hmed <- smacofRobust(delta, engine = smacofHuber, cons = 6, verbose = FALSE, itmax = 10000)
```

```{r pxhmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Median Huber"}
par(pty = "s")
plot(hmed$x, type = "n", xlab = "dim1", ylab = "dim2")
text(hmed$x, names)
```

```{r pdhmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Median Huber"}
par(pty = "s")
plot(delta[index], hmed$d[index], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

```{r htu, echo = FALSE}
htu <- smacofRobust(delta, engine = smacofTukey, cons = 6, verbose = FALSE, itmax = 10000)
```

```{r pxtmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Configuration Median Tukey"}
par(pty = "s")
plot(htu$x, type = "n", xlab = "dim1", ylab = "dim2")
text(htu$x, names)
```

```{r pdtmed, echo = FALSE, fig.align = "center", fig.asp = 2, fig.cap = "Shepard Plot Median Tukey"}
par(pty = "s")
plot(delta[index], htu$d[index], xlab = "delta", ylab = "d", col = "RED")
abline(0, 1, col = "BLUE")
```

# Discussion

Fixed weights

Tukey loss

Minimize
$$
\sum w_k\ f(\delta_k-d_k(X))
$$
if $f''(x)\leq K$.

$$
f(\delta_k-d_k(X))\leq f(\delta_k-d_k(Y))+f'(\delta_k-d_k(Y))(d_k(Y)-d_k(X))+\frac12K(d_k(Y)-d_k(X))^2
$$
Minimize
$$
\left[d_k(X)-\{d_k(Y))-K^{-1}f'(\delta_k-d_k(Y))\}\right]^2
$$

# Code

The function smacofRobust has a parameter "engine", which can be equal
to smacofAV, smacofHuber, smacofTukey, or smacofConvolution. These
four small modules compute the respective loss function values and
weights for the IRLS procedure. This makes it easy to add additional robust
loss functions.

```{r code, eval = FALSE}
smacofRobust <- function(delta,
                         weights = 1 - diag(nrow(delta)),
                         engine = smacofAV,
                         ndim = 2,
                         cons = 0,
                         itmax = 1000,
                         eps = 1e-15,
                         verbose = TRUE) {
  nobj <- nrow(delta)
  wmax <- max(weights)
  xold <- smacofTorgerson(delta, ndim)
  dold <- as.matrix(dist(xold))
  h <- engine(nobj, weights, delta, dold, cons)
  rold <- h$resi
  sold <- sum(weights * rold)
  wold <- h$wght
  itel <- 1
  repeat {
    vmat <- -wold
    diag(vmat) <- -rowSums(vmat)
    vinv <- solve(vmat + (1 / nobj)) - (1 / nobj)
    bmat <- -wold * delta / (dold + diag(nobj))
    diag(bmat) <- -rowSums(bmat)
    xnew <- vinv %*% (bmat %*% xold)
    dnew <- as.matrix(dist(xnew))
    h <- engine(nobj, weights, delta, dnew, cons)
    rnew <- h$resi
    wnew <- h$wght
    snew <- sum(weights * rnew)
    if (verbose) {
      cat(
        "itel ",
        formatC(itel, width = 4, format = "d"),
        "sold ",
        formatC(sold, digits = 10, format = "f"),
        "snew ",
        formatC(snew, digits = 10, format = "f"),
        "\n"
      )
    }
    if ((itel == itmax) || ((sold - snew) < eps)) {
      break
    }
    xold <- xnew
    dold <- dnew
    sold <- snew
    wold <- wnew
    rold <- rnew
    itel <- itel + 1
  }
  return(list(
    x = xnew,
    s = snew,
    d = dnew,
    r = rnew,
    itel = itel
  ))
}

smacofTorgerson <- function(delta, ndim) {
  dd <- delta ^ 2
  rd <- apply(dd, 1, mean)
  md <- mean(dd)
  sd <- -.5 * (dd - outer(rd, rd, "+") + md)
  ed <- eigen(sd)
  return(ed$vectors[, 1:ndim] %*% diag(sqrt(ed$values[1:ndim])))
}

smacofAV <- function(nobj, wmat, delta, dmat, cons) {
  resi <- sqrt((delta - dmat) ^ 2 + cons)
  wght <- wmat / (resi + diag(nobj))
  return(list(resi = resi, wght = wght))
}

smacofConvolution <- function(nobj, wmat, delta, dmat, cons) {
  difi <- delta - dmat
  resi <- difi * (2 * pnorm(difi / cons) - 1) + 2 * cons * dnorm(difi / cons)
  wght <- wmat * (pnorm(difi / cons) - 0.5) / (difi + diag(nobj))
  return(list(resi = resi, wght = wght))
}

smacofTukey <- function(nobj, wmat, delta, dmat, cons) {
  difi <- delta - dmat
  resi <- ((cons ^ 2) / 6) * ifelse(abs(difi) < cons, (1 - (1 - (difi / cons) ^ 2) ^ 3), 1)
  wght <- ifelse(abs(difi) < cons, wmat * (1 - (difi / cons) ^ 2) ^ 2, 0) / 2
  return(list(resi = resi, wght = wght))
}

smacofHuber <- function(nobj, wmat, delta, dmat, cons) {
  difi <- delta - dmat
  resi <- ifelse(abs(difi) < cons, (difi ^ 2) / 2, cons * abs(difi) - ((cons ^ 2) / 2))
  wght <- ifelse(abs(difi) < cons, wmat,
                 wmat * sign(difi - cons) * cons / (difi + diag(nobj)))
  return(list(resi = resi, wght = wght))
}
```

# References
