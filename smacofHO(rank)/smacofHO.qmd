---
title: |
    | Smacof at 50: A Manual
    | Indicator Matrices
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---
```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(smacofHO, quietly = TRUE))
suppressPackageStartupMessages(library(smacofHC, quietly = TRUE))
suppressPackageStartupMessages(library(smacofHS, quietly = TRUE))
suppressPackageStartupMessages(library(smacofPC, quietly = TRUE))
```

```{r load code, echo = FALSE}
#dyn.load("foo.so")
#source("janUtil.R")
```


**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw> in the repositories smacofCode, smacofManual,
and smacofExamples.

# Introduction: Categorical Data{#cat}

In this chapter we shall analyze categorical data structures, with the following components.

* There are $m$ *variables*. 
* Variable $j$ has $k_j>1$ *categories*.
* There are $n$ *objects*.
* Each object defines a *partial order* over the categories of each variable. Make this the simple tree.

Introduce indicators

Discuss: separation and compactness. categories as points or regions.
circles, stars, voronoi regions

Start with MSA and @deleeuw_R_69d,
@deleeuw_R_04e, @deleeuw_U_03d

Thus the actual data we collect are the $n\times m$ partial orders $\lesssim_{ij}$.

We study minimization of the stress *loss function*
\begin{equation}
\sigma(X,Y_1,\cdots,Y_m):=\sum_{j=1}^m\sum_{i=1}^n\min_{\hat d_i^j\in\Delta_i^j}\sum_{l=1}^{k_j}w_{il}^j(\hat d_{il}^j-d_{il}(X,Y_j))^2
(\#eq:snmu)
\end{equation}
over the $n\times p$ matrix of *object scores* $X$, the $k_j\times p$
matrices of *category scores* $Y_j$, and the $n\times k_j$  *transformations* (or *optimal scalings*) $\Delta_j$. We write
$d_{il}(X,Y_j)$ for the distance between object $i$ and category
$l$ of variable $j$. Note that for each variable $j$ there are different matrices of category scores $Y_j$, but there is only a single matrix of object scores $X$. 

The $w_{il}^j$ in definition \@ref(eq:snmu)
are non-negative *weights*. Formulas and derivations simplify if the data are *row-weighted*, by which we mean that $w_{il}^j=w_i^j$. They simplify 
even more if weights are *constant*, i.e. if all non-zero weights
are equal to one.

The transformations in \@ref(eq:snmu)
are *row-conditional*, in the sense that for each $i$ a vector 
$\hat d_i^j$ of length $k_j$ is selected by the technique
from a cone of admissible transformations $\Delta_i^j$. Each row has its own cone. 

We need a few words to discuss the meaning of the word "model" in this
context, since it is used frequently in data analysis. The
model corresponding with a loss function $\sigma$ is the set of parameter
values for which $\sigma$ attains its global minimum (usually zero).
Thus a model is a system of equations and/or inequalities.
In the case of loss function \@ref(eq:snmu) the model is 
that the $d_{il}(X,Y_j)$ with $w_{il}^j>0$ satisfy the partial order $\lesssim_{ij}$. Define
\begin{equation}
\epsilon^j_{il\nu}:=
\begin{cases}
1&\text{ if we require }\hat d^j_{il}\lesssim\hat d^j_{i\nu},\\
0&\text{ otherwise}
\end{cases}
\end{equation}
Then the model is the system of inequalities
\begin{equation}
w_{il}^jw_{i\nu}^j\epsilon^j_{il\nu}(d_{il}(X,Y_j)-d_{i\nu}(X,Y_j))\geq 0
(\#eq:model)
\end{equation}

If the cones $\Delta_i^j$ contain the zero vector, then the global minimum of loss function \@ref(eq:snmu) is clearly equal to zero. Collapsing all $x_i$ and all $y_l^j$ into a single point makes all distances zero, and thus makes stress zero. There is also zero stress if we collapse all $x_i$ into one point and all $y_l^j$ into another point. These solutions are *trivial* in the sense that, although they satisfy the model, they are
independent of the data and consequently not informative. There are also more subtle
trivial solutions.
Suppose the cones $\Delta_i^j$ contain the set of all non-negative constant vectors. Collapse all $x_i$ into a single point, and place all $y_l^j$ for variable $j$ on a sphere around the point $x_i$. There can be different radii for different variables. This makes all $d(x_i,y_l^j)$ equal to the radius of the sphere and thus makes stress zero. 



In the context of non-metric unfolding there has been much work
on avoiding trivial and degenerate solutions. This started as soon as
Kruskal-Guttman-type iterative MDS techniques using data transformation became available. Early contributions were @roskam_68 and @kruskal_carroll_69. For valuable summaries of more recent work, mostly by Willem Heiser and his students, we refer to the dissertations of @vandeun_05 and @busing_10.

It follows from the existence of these trivial solutions that we cannot define the purpose of our algorithms as finding the minimum of \@ref(eq:snmu) over all $\hat D_j$, $X$ and $Y_j$. Some constraints on the optimization problems are needed to prevent these trivial or degenerate solutions. 

# Homogeneity Analysis{#hom}

The *Gifi System* (@gifi_B_90, @michailidis_deleeuw_A_98, @deleeuw_mair_A_09a) implements non-linear or non-metric versions of the classical linear multivariate analysis techniques (regression, analysis of variance, canonical analysis, discriminant analysis, principal component analysis). The non-linear versions are introduced as special cases of *Homogeneity Analysis*, which is better known as *Multiple Correspondence Analysis*.

In this section we present homogeneity analysis as a technique for
minimizing the loss function \@ref(eq:snmu) when the data are $n\times k_j$ *indicator matrices* $G_j$, with $j=1,\cdots,m$. This is a non-standard presentation, because usually homogeneity analysis is related to principal component analysis, and not to multidimensional scaling
(see, for example, @deleeuw_C_14 or @deleeuw_C_23). @hoffman_deleeuw_A_92

In homogeneity analysis the data are (or are coded as) $m$ *indicator matrices* $G_j$, where $G_j$ is $n\times k_j$.
Indicator matrices are binary matrices, with rows that add up to one or to zero. 
Thus each row has either a single element equal to one and the rest zeroes, or it has all elements equal to zero. Indicator matrices are
used to code our categorical variables. Rows corresponds with objects
(or individuals), columns with the categories (or levels) of a variable.
Element $g_{il}^j$ is one if object $i$ is in category $l$ of variable $j$, and all other elements in row $i$ are zero. If an object is *missing* on variable $j$ then the whole row is zero.

Homogeneity analysis makes joint maps in $p$ dimensions of objects
and categories, both represented as points. A joint map for variable $j$
has $n$ object points $x_i$ and $k_j$ category points $y^j_{il}$.
In a homogeneous solution the object points are close to the points of the categories that the objects score in, i.e, to those $y^j_{il}$ for which $g^j_{il}=1$. If there is only one variable then it is trivial to make a perfectly homogeneous map. We just make sure the object points coincide with their category points. But there are $j>1$ indicator matrices, corresponding with $m$ categorical variables, and there is only a single set of object scores. The solution is a compromise trying to achieve as much homogeneity as possible for all variables simultaneously.

In loss function \@ref(eq:snmu) applied to homogeneity analysis
the sets $\Delta_i^j$ are defined in such a way that $\hat d_{il}^j$ is zero if $i$ is in category $l$ of 
variable $j$. There are no constraints on the other $\hat d$'s in row $i$
of variable $j$. Thus for zero loss we want an object to coincide with all $m$ categories it is in. With this definition of the $\Delta_i^j$ we have
\begin{equation}
\min_{\hat d_i^j\in\Delta_i^j}\sum_{l=1}^{k_j}w_{il}^j(\hat d_{il}^j-d(x_i,y_l^j))^2=f_{ij}d_{ij}^2(X,Y),
(\#eq:homsnmu)
\end{equation}
where 
\begin{subequations}
\begin{align}
d_{ij}(X,Y)&:=\sum_{l=1}^{k_j}g_{il}^jd(x_i,y^j_l),(\#eq:dred)\\
f_{ij}&:=\sum_{l=1}^{k_j}w^j_{il}w^j_{il}.(\#eq:wred)
\end{align}
\end{subequations}
Note that the $w^j_{il}$ for which $g^j_{il}=0$ play no role in 
homogeneity analysis. In the usual implementations of homogeneity
analysis and multiple correspondence analysis 
$f_{ij}$ is either zero or one, depending on whether observation
$i$ on variable $j$ is missing or non-missing.

Using indicator matrices we can write loss function \@ref(eq:homsnmu) as
\begin{equation}
\sigma(X,Y_1,\cdots,Y_m)=
\sum_{j=1}^m\text{tr}\ (X-G_jY_j)'F_j(X-G_jY_j),
(\#eq:matsnmu)
\end{equation}
The $F_j$ are diagonal matrices with the $f_{ij}$ from \@ref(eq:wred) 
on the diagonal.

In homogeneity analysis we minimize \@ref(eq:matsnmu) using the
explicit normalization $X'F_\star X=I$, where $F_\star$ is the
sum of the $F_j$. The solution is given by the singular value
equations
\begin{subequations}
\begin{align}
X\Lambda&=F_\star^{-1}\sum_{j=1}^m F_jG_jY_j,(\#eq:homsvd1)\\
Y_j&=(G_j'F_jG_j)^{-1}G_j'F_jX,(\#eq:homsvd2)
\end{align}
\end{subequations}
where $\Lambda$ is a symmetric matrix of Lagrange multipliers. 

In homals (@gifi_B_80, @deleeuw_mair_A_09a) alternating least squares is used
to solve the equations \@ref(eq:homsvd1) and \@ref(eq:homsvd2). We start with
some initial $X$, then compute the corresponding $Y_j$ using \@ref(eq:homsvd2),
then for these new $Y_j$ we compute a new corresponding $X$ from \@ref(eq:homsvd1),
and so on. Computations are efficient, because only diagonal matrices need to 
be inverted and matrix multiplication with an indicator matrix is not really multiplication but simply selection of a particular row or column. Alternating least squares thus becomes *reciprocal averaging*.  Equation \@ref(eq:homsvd2) says that the optimal category
point is the weighted averages of the objects points in the category, and \@ref(eq:homsvd1)
says that, except for rescaling with the Lagrange multipliers, the optimal object
point is the weighted average of the category points that the object scores in.

Alternative methods of computation (and interpretation) are possible if we
substitute \@ref(eq:homsvd2) in \@ref(eq:homsvd1) to eliminate the $Y_j$
and obtain an equation in $X$ only. This gives
\begin{equation}
F_\star X\Lambda=\sum_{j=1}^m F_jG_j(G_j'F_jG_j)^{-1}G_j'F_jX,
(\#eq:geneigx)
\end{equation}
which is a generalized eigenvalue equation for $X$. If we substitute \@ref(eq:homsvd1)
in \@ref(eq:homsvd2) we obtain generalized eigenvalue equations for $Y$.
\begin{equation}
(G_j'F_jG_j)Y_j\Lambda=\sum_{h=1}^m G_j'F_jW_\star^{-1}F_hG_hY_h.
(\#eq:geneigy)
\end{equation}
If $k_\star$, the sum of the $k_j$, is not too large then finding the $p$ largest non-trivial eigenvalues with corresponding eigenvectors from \@ref(eq:geneigy) may be computationally efficient. The largest "trivial" eigenvalue is always equal to one, no matter what the $G_j$ and $W_j$ are, and we can safely ignore it. The trivial solution with all distances equal to zero mentioned in section \@ref(cat) corresponds with this largest eigenvalue.

Homogeneity analysis can be most convincingly introduced using the concept of a *star plot*. For variable $j$ we plot $k_j$ category points and $n$ object
points in a single joint plot. We then draw a line from each category point to the points of the objects in that category. This creates $k_j$ groups of
lines and points in $\mathbb{R}^p$, and each of these groups is called a *star*.
The sum of squares of the line lengths of a star is the loss of homogeneity for category $l$ of variable $j$, and the total sum of squares of all line lengths in the $k_j$ stars is the loss \@ref(eq:matsnmu) for variable $j$. Homogeneity analysis chooses $X$ and the $Y_j$ such that $X$ is normalized by $X'F_\star X=I$ and the stars are as small or as compact as possible, measured by the squared line lengths. For given $X$ the stars are as small as possible by choosing the category points $Y_j$ as the centroids of the object points in the category, as in equation
\@ref(eq:homsvd2). That explains the use of the word "star", because now the stars really look like stars. In graph theory a star is a tree with one internal node (the category point) and $k$ leaves (the object points). Thus, given the optimum choice of the $Y_j$ as centroids, we can also say that homogeneity analysis quantifies the $n$ objects in such a way that the resulting stars are as small as possible.

# Multidimensional Structuple Analysis MSA

The Guttman-Lingoes series of programs (@lingoes_73)
discusses, among many others, several techniques for analyzing
a number of indicator matrices. They have the acronyms
MSA-I, MSA-II, MSA-III, and MSA-IV, where MSA is short for
either Multidimensional Scalogram Analysis or Multidimensional
Structuple Analysis. Unfortunately the techniques are rather
poorly documented in the mainstream literature. I rely on @lingoes_68a,
@lingoes_68b, @lingoes_72, @lingoes_79, and the various
short program descriptions Lingoes published in Behavioural Science.
Unfortunately I currently have no access to @lingoes_73.

All MSA programs start their iterations with MAC-II. MAC stands for
Multivariate Analysis of Contingencies, and the technique implements
the equations from @guttman_41. In other words, MAC is homogeneity
analysis or multiple correspondence analysis. Thus the MSA
programs have the same starting configuration as our smacof
programs for categorical data.

The publications on MSA do not pay much attention to the existence
of trivial solutions and to the speed of convergence of the
iterations. 

## MSA-I

The most interesting member of the MSA sequence is MSA-I.

>The logic of MSA-I was worked out by Guttman as a creative reaction to a number of objections to other proposed solutions for multidimensional scalogram analysis raised by members of his course on multidimensional analysis during his visit to The University of Michigan (1964-1965). 
Some of the computational details and the programming of the technique were done by the author. (@lingoes_68b, p. 76)

The most complete description of MSA-I is probably @zvulun_78. There
are also some computational details in @lingoes_68b. So what is this
MSA-I model ?

Partition the object points corresponding to any category A into inner and outer points. Take any point not in A and find the closest point in A to that point. Such a closest point is called an *outer point* of category A. Go through all points not in A to find all outer points of A. The points of A that are not outer points of A are *inner points* of A. 
Category A is *contiguous* if each inner point of A is closer to an outer point of A than to any other outer point. Since the closest point in B to an inner point of A is by definition an outer point of B we have also contiguity if and only if each inner point of A is closer to some outer point of A than to any point outside A.

In MSA-I there are no category points, only object points. This makes
comparison with the partitioning by Voronoi regions complicated. In
the same way there is no obvious connection with the convex hulls of
the object points in a category. Separations and partitions can be quite
irregular and in the various small examples I have seen are mostly
done after the fact by hand.

The algorithm to optimize contiguity is described in @lingoes_68b. I will
try to reconstruct it.

## MSA-II

Unlike MSA-I, MSA-II, which seems to be mostly due to Lingoes, is
pretty straightforward. The model, as taken from @lingoes_68b, is that
there is a $\rho>0$ such that $g_{il}^j=1$ implies $d(x_i,y_l^j)\leq\rho$
and $g_{il}^j=1$ implies $d(x_i,y_l^j)\geq\rho$. Geometrically: 

* circles with center $x_i$ and radius $\rho$ contain all categories
object $i$ scores in, all other category points are outside the circle
* circles with center $y_l^j$ and radius $\rho$ contain all objects that score in category $l$ of variable $j$, all other object points are
outside the circle.

Computionally we interpret the $n\times k_\star$ binary supermatrix 
$(G_1\mid\cdots\mid G_m)$ as a matrix of similarities and apply
a non-metric MDS technique. The data consist of two tie-blocks,
the ones and the zeroes, and we use the primary approach to ties.
Observe there is no row-conditionality here and there is only a single
radius $\rho$.

The loss function for MSA-II is simply Kruskal's stress formula one,
implicitely normalized by the sum of all $n\times k_\star$
distances, with monotone regression replaced by rank images.

This use of rank images, by the way, is somewhat problematic.
There are $nm$ smallest distances, corresponding with the
elements of $G$ equal to one, and $n(k_\star-m)$ largest distances.
But how do we define the rank images within the two tie blocks ?
Lingoes ranks the distances within the tie blocks from small to large, which seems rather arbitrary. 

## MSA-III

MSA-III is closer to our smacofHO method. 

# The smacofHO Loss Function

The smacofHO technique solves the closely related problem in which we do not require, as in homogeneity analysis, that 
\begin{subequations}
\begin{equation}
\sum_{l=1}^{k_j}g^j_{il}\hat d^j_{il}=0
(\#eq:homcons)
\end{equation}
for all $i$ and $j$, but we impose the weaker condition that for all $i$ and $j$
\begin{equation}
\sum_{l=1}^{k_j}g^j_{il}\hat d^j_{il}\leq\hat d^j_{i\nu}
(\#eq:hocons)
\end{equation}
\end{subequations}
for all $\nu=1,\cdots,k_j$.
In homogeneity analysis the geometric interpretation of loss is that we
want objects to coincide with all categories they score in. The geometric interpretation of loss function \@ref(eq:snmu) is that we want 
objects to be closer to the categories they score in than to the categories
they do not score in. 

This can be formalized using the notion of *Voronoi regions*. The Voronoi region of category $l$ of variable $j$ is the polyhedral convex set of all points of $\mathbb{R}^p$ closer to category $l$ than to any other category of variable $j$. The plot of the the $k_j$ categories of variable $j$ defines $k_j$ Voronoi regions that
partition $\mathbb{R}^p$. For a wealth of information about Voronoi regions we refer to

Loss function \@ref(eq:snmu) with $\Delta$ defined by constraint\@ref(eq:hocons) vanishes if for each variable all $x_i$ are in the Voronoi regions of the categories they score in. This condition implies, by the way, that the interiors of the $k_j$ convex hulls of the $x_i$ in a given category are disjoint, and the point clouds can consequently be weakly separated by hyperplanes. Since the category points themselves are in their own Voronoi region the convex hulls of the stars are also disjoint.

The initial configuration for the iterations is computed using homogeneity analysis. In each iteration configuration updates are alternated with updates of the $\hat D_j$. 
The general majorization theory for MDS with restrictions (@deleeuw_heiser_C_80) calls for configuration updates
in two steps. In the first step we compute the Guttman transform of the current configuration, and in the second step we project the Guttman transform on the set of constrained configurations.

Minimizing loss \@ref(eq:snmu)  over the $\hat d_i^j$ is a monotone regression problem for a simple tree order. This is easily solved by using Kruskal's primary approach to ties (@kruskal_64a, @kruskal_64b, @deleeuw_A_77).

## Dual Formulation

Instead of requiring that $d^j_{il}\leq g\d^j_{i\nu}$ for all $\nu$ if $g^j_{il}=1$ we can also require that $d^j_{il}\leq d^j_{kl}$ if
$1=g^j_{il}<g^j_{kl}=0$. This means requiring monotonicity, using
the primary approach to ties, for each column of $G$. 

Geometrically for each category we want the $x_i$ in that category to be inside a sphere, and each $x_i$ not in that category outside the sphere. With the
provision that points on the sphere are both inside and outside.
There are positive real numbers $\rho^j_l$ such that
$d^j_{il}\leq\rho^j_l$ if $g^j_{il}=1$ and $d^j_{il}\geq\rho^j_l$if if $g^j_{il}=0$. This does not imply, by the way that the interiors of the
spheres are disjoint. 

If the spheres are not disjoint there cannot be any points in the 
interior of the intersection.

It is true, of course, that a sphere containing
all $x_i$ from a category also contains the convex hull of those
$x_i$. 

If the $\rho^j_l$ for variable $j$ are all equal then $d^j_{il}\leq\rho^j\leq d^j_{i\nu}$ if $1=g^j_{il}<g^j_{kl}=0$. Thus in that case our column-wise
sphere constraints imply our row-wise Voronoi constraints. 

It is also possible to combine the Voronoi and sphere constraints. We then
require that the points of a category are in a sphere which lies in the Voronoi
region of that category. The center of the sphere and the Voronoi point
of the category coincide.

## The Guttman Transform

The smacof iterations, or Guttman transforms, more or less ignore the fact that we are dealing with a rectangular matrix and use the weights to transform the problem into a symmetric one (as in @heiser_deleeuw_A_79).

The loss function is
\begin{equation}
\sigma(Z_1,\cdots,Z_m)=\sum_{j=1}^m\sum_{i=1}^{n_j}\sum_{k=1}^{n_j}w_{ik}^j(\hat d_{ik}^j-d_{ik}(Z_j))^2,
(\#eq:fullloss)
\end{equation}
with $n_j:=n+k_j$ and with $Z_j$ the $n_j\times p$ matrices that stack $X$ on top of
$Y_j$. The $w_{ik}^j$ are zero for the diagonal $n\times n$ and the diagonal
$k_j\times k_j$ block.

To compute the Guttman transform of $Z_j$ we have to solve 
the partitioned system
\begin{equation}
\begin{bmatrix}
R_W&-W\\
-W'&C_W
\end{bmatrix}
\begin{bmatrix}\tilde X\\\tilde Y\end{bmatrix}=
\begin{bmatrix}
R_B&-B\\
-B'&C_B
\end{bmatrix}
\begin{bmatrix}X\\Y\end{bmatrix}
(\#eq:guttman)
\end{equation}
Since we have to solve this system for each variable separately we forget about the index $j$ here. In 
\@ref(eq:guttman) $R_W$ and $C_W$ are the diagonal matrices with row and column sums of $-W$, while $R_B$ and $C_B$ are diagonal matrices with the row and columns sums of the $n\times k_j$ matrix $B$, which has elements
\begin{equation}
b_{il}=w_{il}\frac{\hat d_{il}}{d_{il}(X,Y_l)}.
(\#eq:bdef)
\end{equation}
Matrices $X$ and $Y$ are the two parts of the current $Z$ that we are updating,
while we solve for $\tilde X$ and $\tilde Y$, the two parts of the Guttman
transform. 

Define
\begin{equation}
\begin{bmatrix}
P\\Q
\end{bmatrix}:=
\begin{bmatrix}
R_B&-B\\
-B'&C_B
\end{bmatrix}
\begin{bmatrix}X\\Y\end{bmatrix}
(\#eq:gusolve)
\end{equation}
Now
$R_W\tilde X-W\tilde Y=P$ or $\tilde X=R_W^{-1}(P+W\tilde Y)$. Substitute this in $C_W\tilde Y-W'\tilde X=Q$
to get $C_W\tilde Y-W'R_W^{-1}(P+W\tilde Y)=Q$ or 
\begin{equation}
(C_W-W'R_W^{-1}W)\tilde Y=Q+W'R_W^{-1}P
(\#eq:solvefory)
\end{equation}
We solve equation \@ref(eq:solvefory) for $\tilde Y$ and then use $\tilde X=R_W^{-1}(P+W\tilde Y)$.
Note that $C_W-W'R_W^{-1}W$ is doubly-centered.
As in homogeneity analysis we hope that $k_\star$
is not to big, and we avoid generalized inverses of very large and very
sparse matrices.

## The smacof Projection

After computing the Guttman transforms $\tilde X_j$ and $\tilde Y_j$ we have to project them on the set of constrained configurations. 

First suppose the only constraint is $X_j=X$. We will discuss some additional (optional) constraints in a while. To project we must minimize
\begin{multline}
\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'R_j(X-\tilde X_j)-2\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'W_j(Y_j-\tilde Y_j)+\\
\sum_{j=1}^m\text{tr}\ (Y_j-\tilde Y_j)'C_j(Y_j-\tilde Y_j)
\end{multline}
where $R_j$ and $C_j$ are now the diagonal matrices of row and column sums of the $W_j$.

The stationary equations are
\begin{align}
Y_j&=\tilde Y_j-C_j^{-1}W_j'(X-\tilde X_j),(\#eq:stateq1)\\
X&=\{R_\star\}^{-1}\sum_{j=1}^m\left\{R_j\tilde X_j-W_j(Y_j-\tilde Y_j)\right\}.(\#eq:stateq2)
\end{align}
We solve these equations iteratively using alternating least squares.
This means using \@ref(eq:stateq1) to compute a new $Y$ for given $X$ and \@ref(eq:stateq2) to compute a new $X$ for given $Y$. We alternate these two updates until convergence.

Thus we have an iterative "inner" ALS process within the iterative
"outer" ALS process of alternating the Guttman transform/projection and the monotone regressions. More precisely the inner iterations are in the
projection phase of the Guttman update.

If there are further constraints on $X$, besides $X_j=X$, and if there
are constraints on $Y_j$ the updates in the projection phase must
be modified. 

### Rank Constraints

If ww choose to do we can require the $Y_j$ to have rank $r_j\leq\min(k_j,p)$, i.e. $Y_j=Q_jA_j'$
with $Q_j$ a $k_j\times r_j$ matrix and $A_j$ a $p\times r_j$ matrix.
The rank-constraint on $Y_j$ is taken from the Gifi system, where it serves to connect homogeneity analysis with forms of non-linear principal
component analysis. 

If $r=1$ then geometrically having all $y_l^j$ on a line through the origin implies that all Voronoi boundaries are hyperplanes perpendicular to that line, and consequently all Voronoi regions are bounded by two parallel hyperplanes (parallel lines if $p=2$). All objects scores must orthogonally project on the line in the interval corresponding with the category theyscore in. Note that the intervals on the line are actually the one-dimensional Voronoi regions of the line with the category points.

If $r=2$ and $p=3$, another case that may be practically relevant, then category points are in a hyperplane through the origin. The Voronoi regions in three-dimensional space are bounded by lines perpendicular to that plane, intersecting the plane at the two-dimensional Voronoi points for that plane. The object points must be in the correct polyhedral cylinder.

For each of the $m$ variables we can independently choose the ranks $r_j$ of the $Y_j$ and combine it with one of the three options for $X$, creating a large number of different analyses (in a given dimensionality $p$).

If there are rank constraints on one of more of the $Y_j$ then for those $j$ we have to minimize
$$
2\text{tr}\ A_j'\{Y_j'C_j-(X-\tilde X_j)'W_j\}Q_j+
\text{tr}\ A_j'Q_j'C_jQ_jA_j
$$
The stationary equations are
$$
\{Y_j'C_j-(X-\tilde X_j)'W_j\}Q_j=Q_j'C_jQ_jA_j,
$$
and 
$$
\{Y_j'C_j-(X-\tilde X_j)'W_j\}'A_j=C_jQ_jA_jA_j'
$$

### Centroid Constraints

If we require that $Y_j=(G_j'R_jG_j)^{-1}G_j'R_jX$ then this effectively eliminates the $Y_j$ as variables from the optimization problem and we only have to optimize over $X$. We must minimize
\begin{multline}
\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'R_j(X-\tilde X_j) -
\text{tr}\ (X-\tilde X_j)'W_j(H_jX-\tilde Y_j)+\\
\text{tr}\ (H_jX-\tilde Y_j)'C_j(H_jX-\tilde Y_j)
\end{multline}
Expanding
\begin{multline}
2\ X'R_\star X-2\ \text{tr}\ X'\sum_{j=1}^mR_j\tilde X_j-2\ \text{tr}\ X'\{\sum_{j=1}^mW_jH_j\}X + 2\text{tr}\ X'\sum_{j=1}^mW_j\tilde Y_j+\\
\text{tr}\ X'\{\sum_{j=1}^mH_j'C_jH_j\}X-2\text{tr}\ X'\{\sum_{j=1}^mH_j'C_j\tilde Y_j\}
\end{multline}

Substituting $Y_j=H_jX$ with $H_j:=(G_j'R_jG_j)^{-1}G_j'R_j$ in .. and simplifying gives
the stationary equations $P_\star X= Q_\star$ with
\begin{subequations}
\begin{align}
P_\star&:=\sum_{j=1}^m\{R_j-H_j'W_j'-W_jH_j+H_j'C_jH_j\},\\
Q_\star&:=\sum_{j=1}^m\{(R_j-H_j'W_j')\tilde X_j-(W_j-H_j'C_j)\tilde Y_j\}.
\end{align}
\end{subequations}
Thus the unnormalized solution for the object scores is $X=P^+_\star Q_\star$.

We want to avoid inversion of the matrix $P_\star$, which has order $n$. In fact we do not want to compute and store $P_\star$ at all. huppel

We avoid the inversion by using majorization. Suppose $\mu$ is such that $P_\star\lesssim\mu R_\star$ in  the Loewner sense. We would typically take $\mu$ as the largest eigenvalue of $R_\star^{-1}P_\star$.

Define 
$$
\omega(X):=\text{tr}\ X'P_\star X-2\ \text{tr}\ X'Q_\star
$$
Then, reculer pour mieux sauter, writing $\overline{X}$ for the current best 
$X$,
$$
\omega(X)=\text{tr}\ (\overline X+(X-\overline X))'P_\star(\overline X+(X-\overline X))-2\ \text{tr}\ (\overline X+(X-\overline X))'Q_\star
$$
Now
\begin{multline}
\omega(X)\leq\omega(\overline X)+
\mu\text{tr}\ (X-\overline X)'R_\star(X-\overline X)-2\ \text{tr}\ (X-\overline X)'(Q_\star-P_\star\overline X)
\end{multline}
The stationary equations in the unnormalized case have solution
$$
X=\overline X+\mu^{-1}R_\star^{-1}(Q_\star-P_\star\overline X)
$$
If we require the normalization $X'R_\star X=I$ then we must solve the Procrustus problem
$$
(\mu R_\star-P_\star)\overline X - Q_\star=R_\star X\Lambda
$$
with $\Lambda$ a symmetric matrix of Lagrange multipliers.


### Normalization Constraints

Besides $X_j=X$ we can also optionally require the normalization constraint $X'R_\star X=I$. In both cases the stationary equation \@ref(eq:stateq1) remains the same, while \@ref(eq:stateq1) becomes
$$
R_\star X\Lambda=P_\star,
$$
with
$$
P_\star:=\sum_{j=1}^m\left\{R_j\tilde X_j-W_j(Y_j-\tilde Y_j)\right\}
$$
and with $\Lambda$ a symmetric matrix of Lagrange multipliers. 
Using the symmetric square root, $\Lambda=(P'R_\star^{-1}P)^\frac12$ and thus 
$$
X=R_\star^{-1}P(P'R_\star^{-1}P)^{-\frac12}.
$$
Thus requiring normalized object scores only needs small modifications in the $X$ update step of the unnormalized update $R_\star^{-1}P$.

# Dual Formulation

# Convergence and Degeneracy

# Utilities

## Object Plot Function

## Category Plots Function

## Joint Plot Function

## Prediction Table

In the solution $(X,Y)$ we say that pair $(i,j)$ is a *hit* if
$$
d_{il}^j(X,Y)=\min_{\nu=1}^{k_j}d_{i\nu}^j(X,Y)
$$
or, in words, if object point $x_i$ is in the Voronoi region of the category point
corresponding to the category the object scores in.

# Examples

Since there are so many different analyses that can be done (choosing
the rank and normalization constraints), and since each analysis leads
to a large number of plots, presentation of results is a problem. We encourage readers to repeat the analyses and study the output
in more detail.

## Small

We start we a small artificial example, earlier used for illustrative purposes in @gifi_B_90, chapter 2. The data have $n=10$ objects and 
$m=3$ variables with `r c(3,3,2)` categories.

```{r smalldata, echo = FALSE}
small <-
structure(list(first = c("a", "b", "a", "a", "b", "c", "a", "a", 
"c", "a"), second = c("p", "q", "r", "p", "p", "p", "p", "p", 
"p", "p"), third = c("u", "v", "v", "u", "v", "v", "u", "v", 
"v", "v")), class = "data.frame", row.names = c("01", "02", "03", 
"04", "05", "06", "07", "08", "09", "10"))
small
```

### Homogeneity 

We first give the Voronoi plus star joint plots for a homogeneity
analysis of the data, using the function `smacofHomogeneityHO()`.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:smallhomals) ABOUT HERE**
:::
::::

```{r smallhomalscomp, cache = TRUE, echo = FALSE}
h0 <- smacofHomogeneityHO(small)
```


The solution is Voronoi homogeneous for variables one and three. For
variable two the star for category $p$ has objects in the Voronoi
region of category $r$, and is consequently not perfectly homogeneous.
We also see this in the prediction table from this analysis.
```{r smallhompred, echo = FALSE}
smacofPredictionTable(h0)
```
Note that variable $p$ is atypical, because eight of the ten objects
are in category $p$, while categories $q$ and $r$ only have a single object in them.

```{r smallanalysis00, echo = FALSE, cache = TRUE}
h00 <- smacofHO(small, verbose = FALSE, eps = 1e-10)
```

### Voronoi 

We next use the Homogeneity Analysis solution as intial estimate
for a smacofHO analysis without normalization or rank constraints.
Stress is `r h00$stress` after `r h00$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:smallplot00) ABOUT HERE**
:::
::::

As expected, variables one and three, which already have perfect fit, do not change. There is some change in variable two, in the right
direction, but it is not enough to improve the number of correct
predictions.

```{r smallpred00, echo = FALSE}
smacofPredictionTable(h00)
```

```{r smallanalysis10, echo = FALSE, cache = TRUE}
h10 <- smacofHO(small, verbose = FALSE, yform = 1, eps = 1e-10)
```

### Rank one

Bext, for this example, we constrain the $Y_j$ to be of rank one
and leave $X$ unnormalized. Stress is `r h10$stress` after `r h10$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:smallplot10) ABOUT HERE**
:::
::::


Variable three, which is binary, does not change. The plot for variable two changes for the better. To improve the fit the algorithm moves the
category points for categories $p$ and $r$ very close together. There is
still one prediction violation in variable two, but
if the category points of $p$ and $r$ coincide they have the same Voronoi
region and the prediction violation disappears. This may happen if we
continue iterating. The same is true for the prediction violation in
variable one, where object five in category $b$ is only marginally on the wrong side of the
boundary between $a$ and $b$.

```{r smacofpred10, echo = FALSE}
smacofPredictionTable(h10)
```

We do note that the constrained version does better than the 
unconstrained version. But this merely means that the constrained
version finds a better local minimum -- both analyses do not
find the global minimum, which we know is equal to zero.

### Centroids

The next analysis has unnormalized $X$ and centroid restrictions
on the $Y_j$.
```{r smallcomph20, echo = FALSE, cache = TRUE}
h20 <- smacofHC(small, itpar = list(itmax = 10000, eps = 1e-10, verbose = FALSE))
```

Stress is `r h20$snew` after `r h20$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:smallplot20) ABOUT HERE**
:::
::::

```{r smacofpred20, echo = FALSE}
smacofPredictionTable(h10)
```

### Circular

The final analysis for the small example has unnormalized $X$ and circular restrictions on the $Y_j$.
```{r smallcomph2s, echo = FALSE, cache = TRUE}
h2s <- smacofHS(small, verbose = FALSE)
```

Stress is `r h2s$stress` after `r h2s$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:smallploths) ABOUT HERE**
:::
::::



## Cetacea

```{r cetaceadata, echo = FALSE}
cetacea <-
structure(list(NECK = c(0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L), `FORM OF THE HEAD` = c(5L, 
4L, 4L, 2L, 1L, 3L, 2L, 1L, 5L, 3L, 3L, 2L, 2L, 0L, 2L, 2L, 1L, 
4L, 1L, 3L, 1L, 1L, 3L, 3L, 1L, 0L, 2L, 3L, 2L, 2L, 2L, 1L, 2L, 
3L, 2L, 1L), `SIZE OF THE HEAD` = c(0L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 
1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), BEAK = c(0L, 
0L, 0L, 2L, 1L, 0L, 2L, 0L, 0L, 0L, 0L, 2L, 3L, 0L, 2L, 3L, 1L, 
0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 2L, 2L, 2L, 2L, 3L, 
2L, 2L, 2L), `DORSAL FIN` = c(0L, 3L, 3L, 2L, 2L, 0L, 2L, 0L, 
0L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 0L, 3L, 2L, 0L, 3L, 0L, 2L, 2L, 
1L, 0L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L), FLIPPERS = c(1L, 
0L, 3L, 0L, 2L, 1L, 2L, 3L, 1L, 3L, 3L, 0L, 1L, 0L, 2L, 1L, 2L, 
3L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 3L, 2L, 2L, 2L, 2L, 1L, 
0L, 2L, 0L), `SET OF TEETH` = c(2L, 4L, 4L, 0L, 1L, 1L, 1L, 3L, 
2L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 4L, 0L, 0L, 2L, 1L, 1L, 1L, 
1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L), FEEDING = c(3L, 
3L, 3L, 0L, 0L, 1L, 1L, 3L, 3L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 
3L, 0L, 0L, 3L, 1L, 1L, 2L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 
0L, 1L, 0L), `BLOW HOLE` = c(3L, 3L, 3L, 2L, 1L, 1L, 1L, 3L, 
3L, 1L, 1L, 2L, 2L, 0L, 1L, 0L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 1L, 
1L, 0L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L), COLOR = c(4L, 
0L, 3L, 0L, 4L, 2L, 0L, 3L, 1L, 3L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
3L, 0L, 3L, 0L, 3L, 1L, 3L, 0L, 0L, 1L, 1L, 2L, 0L, 3L, 0L, 1L, 
0L, 0L, 3L), `CERVICAL VERTEBRAE` = c(1L, 0L, 0L, 1L, 1L, 0L, 
1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 
1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L), `LACRYMAL AND JUGAL BONES` = c(-1L, 
-1L, -1L, 1L, 1L, 1L, 1L, -1L, -1L, 1L, 1L, 1L, 0L, -1L, 1L, 
0L, 1L, -1L, 1L, 1L, -1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 
1L, 1L, 0L, 1L, 1L, 1L), HABITAT = c(2L, 4L, 4L, 2L, 2L, 2L, 
1L, 4L, 2L, 1L, 4L, 4L, 0L, 4L, 2L, 0L, 1L, 4L, 4L, 2L, 2L, 4L, 
3L, 2L, 4L, 1L, 0L, 4L, 3L, 3L, 4L, 1L, 0L, 2L, 4L, 4L), `LONGITUDINAL FURROWS ON THE THROAT` = c(0L, 
2L, 2L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
2L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 1L), `HEAD BONES` = c(0L, 0L, 0L, 2L, 1L, 2L, 1L, 0L, 
0L, 1L, 1L, 2L, -1L, 3L, 1L, -1L, 1L, 0L, 2L, 2L, 0L, 1L, 1L, 
1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, -1L, 2L, 1L, 2L)), class = "data.frame", row.names = c("Bowhead whales", 
"Rorquals", "Blue whale", "Giant bottle-nosed whales", "Commerson's Dolphins", 
"White whales", "Common dolphins", "Grey whales", "Right whales", 
"Pilot whales", "Risso's dolphins", "Bottle-nosed whales", "Amazon dolphins", 
"Pygmy sperm whales", "White-sided dolphins", "Chinese river dolphins", 
"Right whale dolphins", "Humpback whales", "Sowerby's whales", 
"Narwhals", "Pygmy right whales", "Finless black porpoises", 
"Irawady dolphins", "Killer whales", "Common porpoises", "Sperm whales", 
"Gangetic dolphins", "False killer whales", "Guyanian river dolphins", 
"Cameroun's dolphins", "Spotted dolphins", "Rough toothed dolphins", 
"La Plata's dolphins", "Shepherd's beaked whales", "Bottle-nosed dolphins", 
"Goosebeak whales"))
```

Our first real example has $m=15$ variables and $n=36$ objects. The
objects are genera of whales, dolphins, and porpoises . The variables are morphological, osteological, and behavioral descriptors,
all categorical with a small number of categories. They are (with the number of categories)
```{r cetaceancat, echo = FALSE}
matrix(apply(cetacea, 2, function(x) length(unique(x))), 15, 1,
       dimnames =  list(names(cetacea), NULL))
```
In order to be able to interpret the plots, we also give the 36 genera.
```{r cetaceagens, echo = FALSE}
as.matrix(row.names(cetacea))
```
The data matrix has been constructed by @vescia_85. Chapter 1 of the
book edited by @marcotorchino_proth_janssen_85 has the data, and a number of sub-chapters in which different data analysts
apply various techniques to these data and discuss the results. Among the contenders were MSA-I (@guttman_85) and homals (@vanderburg_85).

### Homogeneity 

We start with a homogeneity analysis.
```{r cetahomcomp, cache = TRUE, echo = FALSE}
hcethom <- smacofHomogeneityHO(cetacea)
```
:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:cetahomplot) ABOUT HERE**
:::
::::

The number of correct predictions per variable, out of a possible 36, is
```{r}
hcethompre <- smacofPredictionTable(hcethom)
print(colSums(hcethompre, na.rm = TRUE))
```
Thus we predict correctly in `r sum(hcethompre)/(15 * 36) * 100` percent of the cases.

### Voronoi

```{r cetacomp00, echo = FALSE, cache = TRUE}
hcetho <- smacofHO(cetacea, verbose = FALSE, itmax = 10000)
```

Stress is `r hcetho$stress` after `r hcetho$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ceta00plot) ABOUT HERE**
:::
::::

The number of correct predictions per variable is
```{r echo = FALSE}
hcethopre <- smacofPredictionTable(hcetho)
print(colSums(hcethopre, na.rm = TRUE))
```
Thus we predict correctly in `r round(sum(hcethopre)/(15 * 36) * 100)` percent of the cases.


### Voronoi, Normed

```{r cetacomp01, echo = FALSE, cache = TRUE}
hcetno <- smacofHO(cetacea, verbose = FALSE, itmax = 10000, xnorm = TRUE)
```

Stress is `r hcetno$stress` after `r hcetno$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ceta01plot) ABOUT HERE**
:::
::::

```{r cetanorm, eval = FALSE, cache = TRUE}
hcetnopre <- smacofPredictionTable(hcetno)
print(sum(hcetnopre, na.rm = TRUE))
```

There is an alternative analysis available, based on the same model, which uses the smacofPC program. The indicator matrix can be 
converted into $n(k_\star-m)$ tetrads, for the cetacea example
that is `r 36 * (58 - 15)` tetrads. These tetrads are used in
an MDS problem of order `r 36 + 58`, using a matrix of weights
in which only the 36 by 58 off-diagonal matrix is non-zero.

## Senate

```{r senatedata, echo = FALSE}
data(senate, package = "homals")
```

### Homogeneity

```{r senatehomcomp, cache = TRUE, echo= FALSE}
hhom <- smacofHomogeneityHO(senate[, 2:21])
```

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:senatehomplot) ABOUT HERE**
:::
::::

```{r}
par(mfrow = c(1,3))
smacofJointPlotsHO(hhom, jvar = c(6, 8, 10), objects = TRUE, voronoi = TRUE, stars = TRUE, labels = as.character(senate[, 1]), xcex = .5, clabels = TRUE)
```

```{r senhhompre, echo = FALSE}
hhompre <- smacofPredictionTable(hhom)
print(colSums(hhompre, na.rm = TRUE))
```
Thus we predict correctly in `r sum(hhompre, na.rm = TRUE)/(20 * 100) * 100` percent of the cases.

### Voronoi

```{r senatehocomp, cache = TRUE, echo = FALSE}
hho <- smacofHO(senate[, 2:21], verbose = FALSE, itmax = 10000)
```
Stress is `r hho$stress` after `r hho$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:senatehoplot) ABOUT HERE**
:::
::::

```{r}
par(mfrow = c(1,3))
smacofJointPlotsHO(hho, jvar = c(6, 8, 10), objects = TRUE, voronoi = TRUE, stars = TRUE, labels = as.character(senate[, 1]), xcex = .5, clabels = TRUE)
```

```{r senhhopre, echo = FALSE}
hhopre <- smacofPredictionTable(hho)
print(colSums(hhopre, na.rm = TRUE))
```
Thus we predict correctly in `r sum(hhopre, na.rm = TRUE)/(20 * 100) * 100` percent of the cases.

### Centroid

```{r senatecencomp, cache = TRUE, echo = FALSE}
hosencen <- smacofHC(senate[, 2:21], itpar = list(itmax = 10000, eps = 1e-10, verbose = FALSE))
```
Stress is `r hosencen$snew` after `r hosencen$itel` iterations.

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:senatehoplot) ABOUT HERE**
:::
::::

```{r senhocenpre, echo = FALSE}
hosencenpre <- smacofPredictionTable(hosencen)
print(colSums(hosencenpre, na.rm = TRUE))
```
Thus we predict correctly in `r sum(hosencenpre, na.rm = TRUE)/(20 * 100) * 100` percent of the cases.

```{r senatecennorm, cache = TRUE, echo = FALSE}
hosencennorm <- smacofHC(senate[, 2:21], itpar = list(itmax = 10000, eps = 1e-10, verbose = FALSE))
```
Stress is `r hosencennorm$snew` after `r hosencennorm$itel` iterations.

## GALO

```{r galodata, echo = FALSE}
data(galo, package="homals")
```

```{r galohomcomp, cache = TRUE}
hgalohom <- smacofHomogeneityHO(galo[, 1:4])
```

```{r}
smacofObjectsPlotHO(hgalohom, cex = .5)
hgalohompre <- smacofPredictionTable(hgalohom)
print(colSums(hgalohompre, na.rm = TRUE))
```
```{r}
par(mfrow = c(1, 2))
smacofJointPlotsHO(hgalohom, jvar = 1:2, objects = TRUE, voronoi = TRUE, xcex = .5, ycex = .5, clabels = 0)
```

```{r galohocomp, cache = TRUE}
hgaloho <- smacofHO(galo[, 1:4], verbose = FALSE, itmax = 10000)
```

```{r}
smacofObjectsPlotHO(hgaloho, cex = .5)
hgalohopre <- smacofPredictionTable(hgaloho)
print(colSums(hgalohopre, na.rm = TRUE))
```

```{r}
par(mfrow = c(2, 2))
smacofJointPlotsHO(hgaloho, objects = TRUE, voronoi = TRUE, xcex = .5, ycex = .5, clabels = 0)
```


# Generalizations

1. Fuzzy Indicators
2. Voronoi with clouds
3. Circles with varying radius
4. Disjoint circles

# Figures

## Small

```{r smallhomals, echo = FALSE, fig.align = "center", fig.cap = "Small example, Homogeneity Analysis"}
par(mfrow = c(1, 3))
smacofJointPlotsHO(h0, objects = TRUE, stars = TRUE, voronoi = TRUE, xlabels = 3)
```
```{r smallplot00, echo = FALSE, fig.align = "center", fig.cap = "Small example, smacofHO unrestricted"}
par(mfrow = c(1, 3))
smacofJointPlotsHO(h00, objects = TRUE, stars = TRUE, voronoi = TRUE, xlabels = 3)
```

```{r smallplot10, echo = FALSE, fig.align = "center", fig.cap = "Small example, smacofHO rank one"}
par(mfrow = c(1, 3))
smacofJointPlotsHO(h10, objects = TRUE, stars = TRUE, voronoi = TRUE, xlabels = 3)
```

```{r smallplot20, echo = FALSE, fig.align = "center", fig.cap = "Small example, smacofHC centroid"}
par(mfrow = c(1, 3))
smacofJointPlotsHC(h20, objects = TRUE, stars = TRUE, voronoi = TRUE, xlabels = 1)
```

```{r smallploths, echo = FALSE, fig.align = "center", fig.cap = "Small example, smacofHS circular"}
par(mfrow = c(1, 3))
smacofObjectsCirclePlot(h2s)
```
$$\pagebreak$$


## Cetacea

```{r cetahomplot, fig.align = "center", fig.cap = "Cetacea Homogeneity Analysis", echo = FALSE}
smacofObjectsPlotHO(hcethom, cex = .5, xlabels = 1)
```

```{r ceta00plot, fig.align = "center", fig.cap = "Cetacea smacofHO unrestricted", echo = FALSE}
smacofObjectsPlotHO(hcetho, cex = .5, xlabels = 1)
```

```{r ceta01plot, fig.align = "center", fig.cap = "Cetacea smacofHO normed", echo = FALSE}
smacofObjectsPlotHO(hcetno, cex = .5, xlabels = 1)
```

$$\pagebreak$$

## Senate

```{r senatehomplot, fig.align = "center", fig.cap = "Senate Homogeneity Analysis", echo = FALSE}
smacofObjectsPlotHO(hhom, cex = .5, labels = as.character(senate[, 1]))
```
```{r senatehoplot, fig.align = "center", fig.cap = "Senate smacofHO Unrestricted", echo = FALSE}
smacofObjectsPlotHO(hho, cex = .5, labels = as.character(senate[, 1]))
```
```{r senatecenplot, fig.align = "center", fig.cap = "Senate smacofHC Centroid", echo = FALSE}
smacofObjectsPlotHO(hosencen, cex = .5, labels = as.character(senate[, 1]))
```
```{r senatecennormplot, fig.align = "center", fig.cap = "Senate smacofHC Centroid", echo = FALSE}
smacofObjectsPlotHO(hosencennorm, cex = .5, labels = as.character(senate[, 1]))
```

$$\pagebreak$$


# References
