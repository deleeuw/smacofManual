---
title: |
    | Smacof at 50
    | A Manual
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started February 21 2024, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 4
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 4
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: TBD
editor_options: 
  markdown: 
    wrap: 72
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(splines, quietly = TRUE))
suppressPackageStartupMessages(library(splines2, quietly = TRUE))
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(scatterplot3d, quietly = TRUE))
suppressPackageStartupMessages(library(smacofRR, quietly = TRUE))
```

```{r load code, echo = FALSE}
makeBspline <-
  function(x,
           degree,
           knots = c(.2, .5, .9),
           Boundary.knots = c(0, 1),
           ordinal = FALSE) {
    b <-
      bSpline(
        x,
        knots = knots,
        degree = degree,
        Boundary.knots = Boundary.knots,
        intercept = TRUE
      )
    m <- ncol(b)
    n <- length(knots)
    if (ordinal) {
      b <- smacofCumulateBasis(b)
      jmin = 2
    } else {
      jmin = 1
    }
    plot(x,
         b[, jmin],
         type = "l",
         lwd = 2,
         col = "RED")
    for (j in (jmin + 1):m) {
      lines(x, b[, j], lwd = 2, col = "RED")
    }
    if (n > 0) {
      for (k in 1:n) {
        abline(v = knots[k])
      }
    }
  }

smacofCumulateBasis <- function(basis) {
  return(t(apply(basis, 1, function(x)
    rev(cumsum(
      rev(x)
    )))))
}
```

**Note:** This is a working paper which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/smacofCode>.

\sectionbreak

# Introduction

In *Multidimensional Scaling (MDS)* the data consists of information
about the similarity or dissimilarity between pairs of objects selected
from a finite set $\mathcal{O}=\{o_1,\cdots,o_n\}$.

In *metric MDS* we have numerical dissimilarity measures and we want to
map the objects $o_i$ into $n$ points $x_i$ of some metric space in such
a way that the distances between the points approximate the
dissimilarities between the objects. In *smacof*, our framework for MDS
theory, algorithms, and computer programs, the metric space is
$\mathbb{R}^p$, the space of all $p$-tuples of real numbers, and in the
code documented in this manual we assume the distance is the usual
Euclidean distance.

In *non-metric MDS* the information about the dissimilarities is
incomplete. It is usually *ordinal*, i.e. it tells us in some way or
another that some dissimilarties are larger or smaller than others.
Somewhere between metric and non-metric MDS is MDS with *missing data*,
in which some dissimilarities are known numbers while others are
unknown. MDS with missing data is a form of *distance matrix completion*
(@fang_oleary_12).

## Kruskal's Stress

In the pioneering papers @kruskal_64a and @kruskal_64b the MDS problem
was formulated for the first time as minimization of an explicit loss
function, which measures the quality of the approximation of the
dissimilarities by the distances. The loss function in least squares
metric Euclidean MDS is called *raw stress* or *Kruskal's raw stress*
and is defined as \begin{equation}
\sigma(X):=\frac12\sum w_{ij}(\delta_{ij}-d_{ij}(X))^2.
(\#eq:stressdef)
\end{equation}

The symbol $:=$ is used for definitions. In definition
\@ref(eq:stressdef) the $w_{ij}$ are known non-negative *weights*, the
$\delta_{ij}$ are the known non-negative *dissimilarities* between
objects $o_i$ and $o_j$, and the $d_{ij}(X)$ are the *distances* between
the corresponding points $x_i$ and $x_j$. The summation is over all
$\binom{n}{2}$ pairs $(i,j)$ with $j>i$, i.e. over elements below the
diagonal of the matrices $W$ and $\Delta$. From now on we use "metric
MDS" to mean Least Squares Metric Euclidean MDS.

The $n\times p$ matrix $X$, which has the coordinates $x_i$ of the $n$
points as its rows, is called the *configuration*, where $p$ is the
*dimension* of the Euclidean space in which we make the map. Thus
\begin{equation}
d_{ij}(X)=\sqrt{\sum_{s=1}^p(x_{is}-x_{js})^2}.
(\#eq:ddef)
\end{equation} The metric MDS problem (of dimension $p$, for given $W$
and $\Delta$) is the minimization of \@ref(eq:stressdef) over the
$n\times p$ configurations $X$.

The weights $w_{ij}$ can be used to quantify information about the
precision or importance of the corresponding dissimilarities. Some of
the weights may be zero, which can be used to code *missing data*. If
all weights are positive we have *complete data*. If we have complete
data, and all weights are equal to one, we have *unweighted* metric MDS.
Weights were only introduced in MDS in @deleeuw_C_77, the pioneering
papers by Shepard, Kruskal, and Guttman only consider the unweighted
case.

We assume throughout that the weights are *irreducible* (@deleeuw_C_77).
This means there is no partitioning of the index set
$I_n:=\{1,2,\cdots,n\}$ into subsets for which all between-subset
weights are zero. A reducible metric MDS problems decomposes into a
number of smaller independent metric MDS problems, so the irreducibility
assumption causes no real loss of generality.

The fact that the summation in \@ref(eq:stressdef) is over all $j<i$
indicates that the diagonal elements of $\Delta$ are not used (they are
assumed to be zero) and the elements above the diagonal are not used as
well (they are assumed to be equal to the corresponding elements below
the diagonal). The somewhat mysterious factor $\frac12$ in definition
\@ref(eq:stressdef) is there because it simplifies some of the formulas
in later sections of this paper.

Kruskal was not really interested in metric MDS and the "raw" loss
function \@ref(eq:stressdef). His papers are really about non-metric
MDS, by which we mean least squares non-metric Euclidean MDS. Non-metric
MDS differs from metric MDS because we have incomplete information about
the dissimilarities. As we have seen, that if some dissimilarities are
missing metric MDS can handle this by using zero weights. In some
situations, however, we only know the rank order of the non-missing
dissimilarities. We do not know, or we refuse to use, their actual
numeric values. Or, to put it differently, even if we have numerical
dissimilarities we are looking for a *transformation* of the non-missing
dissimilarities, where the transformation is chosen from a set of
admissible transformations (for instance from all linear or monotone
transformations). If the dissimilarities are non-numerical, for example
rank orders or partitionings, we choose from the set of admissible
*quantifications*.

In non-metric MDS the loss function becomes \begin{equation}
\sigma(X,\hat D):=\frac12\sum w_{ij}(\hat d_{ij}-d_{ij}(X))^2,
(\#eq:rawstressdef)
\end{equation} where $\hat D$ are now the quantified or transformed
dissimilarities. In MDS parlance they are also called *pseudo-distances*
or *disparities*. Loss function \@ref(eq:rawstressdef) must be minimized
over both configurations and disparities, with the condition that the
disparities $\hat D$ are an admissible transformation of the
dissimilarities $\Delta$. In Kruskal's non-metric MDS this means
requiring monotonicity. In this paper we will consider various other
choices for the set of admissible transformations. We will use the
symbol $\mathfrak{D}$ for the set of admissible transformations

The most familiar sets of admissible transformations (linear,
polynomial, monotone) define convex cones with apex at the origin. This
means that if $\hat D\in\mathfrak{D}$ then so is $\lambda\hat D$ for all
$\lambda\geq 0$. But consequently minimizing \@ref(eq:rawstressdef) over
all $\hat D\in\mathfrak{D}$ and over all configurations has the trivial
solution $\hat D=0$ and $X=0$, corresponding with the global minimum
$\sigma(X,\hat D)=0$. We need additional constraints to rule out this
trivial solution, and in non-metric MDS this is done by choosing a
*normalization* that keeps the solution away from zero.

Kruskal's solution is to normalize the raw stress by defining
\begin{equation}
\sigma(X,\Delta):=\frac{\sum w_{ij}(\hat d_{ij}-d_{ij}(X))^2}{\sum w_{ij}d_{ij}^2(X)}.
(\#eq:nstressdef)
\end{equation} In fact in Kruskal's formulation there are no weights,
and he actually takes the square root of \@ref(eq:nstressdef) to define
*Kruskal's stress*. The non-metric Euclidean MDS problem is to minimize
loss function \@ref(eq:nstressdef) over all $n\times p$ configurations
$X$ and all admissible disparities $\Delta$.

## Normalization

Normalization in non-metric MDS has been discussed in detail in
@kruskal_carroll_69 and @deleeuw_U_75a. In the terminology of
@deleeuw_U_75a there are *explicit* and *implicit* normalizations.

In implicit normalization we minimize either \begin{equation}
\sigma(X,\hat D):=\frac{\sum  w_{ij}(\hat d_{ij} -d_{ij}(X))^2}{\sum   w_{ij}^{\ }\hat d_{ij}^2}
(\#eq:implicit1)
\end{equation} or \begin{equation}
\sigma(X,\hat D):=\frac{\sum   w_{ij}(\hat d_{ij}-d_{ij}(X))^2}{\sum   w_{ij}^{\ }d_{ij}^2(X) }
(\#eq:implicit2)
\end{equation} @kruskal_64a chooses definition \@ref(eq:implicit2) and
calls the explicitly normalized loss function *normalized stress*. In
fact, he takes the square root, which does not change the minimization
problem, and only considers the unweighted case. Note that we overload
the symbol $\sigma$ to denote any one of the least squares loss
functions. It will always be clear from the text which $\sigma$ we are
talking about.

In explicit normalization we minimize the raw stress $\sigma(X,\hat D)$
from \@ref(eq:rawstressdef), but we add the constraint \begin{equation}
\sum   w_{ij}^{\ }d_{ij}^2(X)=1,
(\#eq:explicit1)
\end{equation} or the constraint \begin{equation}
\sum   w_{ij}^{\ }\hat d_{ij}^2=1.
(\#eq:explicit2)
\end{equation} @kruskal_carroll_69 and @deleeuw_E_19d show that these
four normalizations all lead to essentially the same solution for $X$
and $\hat D$, up to scale factors dictated by the choice of
normalization. It is also possible to normalize both $X$ and $\hat D$,
either explicitly or implicitly, and again this will give the same
solutions, suitably normalized. These invariance results assume the
admissible transformations form a closed cone with apex at the origin,
i.e. if $\hat D$ is admissible and $\lambda\geq 0$ then $\lambda\hat D$
is admissible as well. The matrices of Euclidean distances $D(X)$ form a
similar closed cone as well. The LSNE-MDS problem is to find an element
of the $\hat D$ cone and an element of the $D(X)$ cone where the angle
between the two is a small as possible.

In the R version of smacof (@deleeuw_mair_A_09c,
@mair_groenen_deleeuw_A_22) we use explicit normalization
\@ref(eq:explicit2). This is supported by the result, also due to
@deleeuw_U_75a, that projection on the intersection of the cone of
disparities and the sphere defined by \@ref(eq:explicit2) is equivalent
to first projecting on the cone and then normalizing the projection (see
also @bauschke_bui_wang_18).

In our version of non-metric MDS we need more flexibility. For
algorithmic reasons that will become clear later on, we will go with the
other explicit normalization \@ref(eq:explicit1) and minimize $\sigma$
from \@ref(eq:rawstressdef) over normalized $X$ and unnormalized
$\hat D$. For the final results the choice between \@ref(eq:explicit1)
and \@ref(eq:explicit2) should not make a difference.

## Some thoughts on ALS

I will take this opportunity to clear up some misunderstandings and
confusions that have haunted the early development of non-metric MDS.

### The Single-Phase approach

In @kruskal_64a defines \begin{equation}
\sigma(X):=\min_{\hat D\in\mathfrak{D}}\ \sigma(\hat D,X)=\sigma(X,\hat D(X)),
(\#eq:project)
\end{equation} where $\sigma(\hat D,X)$ is defined by
\@ref(eq:implicit2). where the minimum is over admissible
transformations. In definition \@ref(eq:project) \begin{equation}
\hat D(X):=\mathop{\text{argmin}}_{\hat D\in\mathfrak{D}}\sigma(X, \hat D).
(\#eq:optscal)
\end{equation} Normalized stress defined by \@ref(eq:project) is now a
function of $X$ only. Under some conditions, which are true in Kruskal's
definition of non-metric MDS, \begin{equation}
\mathcal{D}\sigma(X)=\mathcal{D}_1\sigma(X,\hat D(X)),
(\#eq:partials)
\end{equation} where $\mathcal{D}\sigma(X)$ are the derivatives of
$\sigma$ from \@ref(eq:project) and $\mathcal{D}_1\sigma(X,\hat D(X))$
are the partial derivatives of $\sigma$ from \@ref(eq:implicit2) with
respect to $X$. Thus the partials of $\sigma$ from \@ref(eq:project) can
be computed by evaluating the partials of $\sigma$ from
\@ref(eq:implicit2) with respect to $X$ at $(X,\hat D(X))$. This has
created much confusion in the past. The non-metric MDS problem is now to
minimize $\sigma$ from \@ref(eq:project), which is a function of $X$
alone.

@guttman_68 calls this the *single-phase approach*. A variation of
Kruskal's single-phase approach defines $$
\sigma(X)=\sum   w_{ij}(d_{ij}^\#(X)-d_{ij}(X))^2
$$ where the $d_{ij}^\#(X)$ are *Guttman's rank images*, i.e. the
permutation of the $d_{ij}(X)$ that makes them monotone with the
$\delta_{ij}$ (@guttman_68). Or, alternatively, define $$
\sigma(X):=\sum   w_{ij}(d_{ij}^\%(X)-d_{ij}(X))^2
$$ where the $\hat d_{ij}^\%(X)$ are *Shepard's rank images*, i.e. the
permutation of the $\delta_{ij}$ that makes them monotone with the
$d_{ij}(X)$ (@shepard_62a, @shepard_62b, @deleeuw_E_17e).

Minimizing the Shepard and Guttman single-phase loss functions is
computationally more complicated than Kruskal's *monotone regression*
approach, mostly because the rank-image transformations are not
differentiable, and there is no analog of \@ref(eq:partials) and of the
equivalence of the different implicit and explicit normalizations.

### The Two-Phase Approach

The *two-phase approach* or *alternating least squares (ALS)* approach
alternates minimization of $\sigma(\hat D,X)$ over $X$ for our current
best estimate of $\hat D$ with minimization of $\sigma(\hat D,X)$ over
$\Delta\in\mathfrak{D}$ for our current best value of $X$. Thus an
update from iteration $k$ to iteration $k+1$ looks like \begin{align}
\hat D^{(k)}&=\mathop{\text{argmin}}_{\hat D\in\mathfrak{D}}\sigma(\hat D,X^{(k)}),(\#eq:step1)\\
X^{(k+1)}&=\mathop{\text{argmin}}_X\sigma(\hat D^{(k)},X).(\#eq:step2)
\end{align} This ALS approach to MDS was in the air since the early
(unsuccessful) attempts around 1968 of Young and De Leeuw to combine
Torgerson's classic metric MDS method with Kruskal's monotone regression
transformation. All previous implementations of non-metric smacof use
the two-phase approach, and we will do the same in this paper.

As formulated, however, there are some problems with the ALS algorithm.
Step \@ref(eq:step1) is easy to carry out, using monotone regression.
Step \@ref(eq:step2) means solving a metric scaling problem, which is an
iterative proces that requires an infinite number of iterations. Thus,
in the usual implementations, step \@ref(eq:step1) is combined with one
of more iterations of a convergent iterative procedure for metric MDS,
such as smacof. If we take only one of these *inner iterations* the
algorithm becomes indistinguishable from Kruskal's single-phase method.
This has also created much confusion in the past.

In the usual implementations of the ALS approach we solve the first
subproblem \@ref(eq:step1) exactly, while we take only a single step
towards the solution for given $\hat D$ in the second phase
\@ref(eq:step2). If we have an infinite iterative procedure to compute
the optimal $\hat D\in\mathfrak{D}$ for given $X$, then a more balanced
approach would be to take several inner iterations in the first phase
and several inner iterations in the second phase. How many of each,
nobody knows. In our current implementation of smacof we take several
inner iteration steps in the first phase and a single inner iteration
step in the second phase.

# Smacof Notation and Terminology

We discuss some standard MDS notation, first introduced in
@deleeuw_C_77. This notation is useful for the second phase of the ALS
algorithm, in which solve the metric MDS problem of we minimizing
unnormalized $\sigma(X,\hat D)$ over $X$ for fixed $\hat D$. We will
discuss the first ALS phase later in the paper.

Start with the unit vectors $e_i$ of length $n$. They have a non-zero
element equal to one in position $i$, all other elements are zero. Think
of the $e_i$ as the columns of the identity matrix.

Using the $e_i$ we define for all $i\not= j$ the matrices
\begin{equation}
A_{ij}:=(e_i-e_j)(e_i-e_j)'.
\end{equation} The $A_{ij}$ are of order $n$, symmetric,
doubly-centered, and of rank one. They have four non-zero elements.
Elements $(i,i)$ and $(j,j)$ are equal to $+1$, elements $(i,j)$ and
$(j,i)$ are $-1$.

The importance of $A_{ij}$ in MDS comes from the equation
\begin{equation}
d_{ij}^2(X)=\text{tr}\ X'A_{ij}X.
(\#eq:dfroma)
\end{equation} In addition we use the fact that the $A_{ij}$ form a
basis for the $binom{n}{2}$-dimensional linear space of all
doubly-centered symmetric matrices.

Expanding the square in the definition of stress gives \begin{equation}
\sigma(X)=\frac12\{\sum   w_k\delta_k^2-2\ \sum   w_k\delta_kd_k(X)+\sum   w_kd_k^2(X)\}.
(\#eq:expand)
\end{equation} It is convenient to have notation for the three separate
components of stress from equation \@ref(eq:expand). Define
\begin{align}
\eta_{\hat D}^2&=\sum   w_{ij}\hat d_{ij}^2,(\#eq:condef)\\
\rho(X)&=\sum   w_{ij}\hat d_{ij}d_{ij}(X),(\#eq:rhodef)\\
\eta^2(X)&=\sum   w_{ij}d_{ij}(X)^2.(\#eq:etadef)
\end{align} which lead to \begin{equation}
\sigma(X)=\frac12\left\{\eta_{\hat D}^2-2\rho(X)+\eta^2(X)\right\}.
(\#eq:stressshort)
\end{equation} We also need \begin{equation}
\lambda(X)=\frac{\rho(X)}{\eta(X)}.
(\#eq:lambdadef)
\end{equation}

Using the $A_{ij}$ makes it possible to give matrix expressions for
$\rho$ and $\eta^2$. First \begin{equation}
\eta^2(X)=\text{tr}\ X'VX,
(\#eq:etamat)
\end{equation} with \begin{equation}
V:=\sum   w_{ij}A_{ij}.
(\#eq:vdef)
\end{equation} In the same way \begin{equation}
\rho(X)=\text{tr}\ X'B(X)X,
(\#eq:rhomat)
\end{equation} with \begin{equation}
B(X):=\sum   w_{ij}r_{ij}(X)A_{ij},
(\#eq:bdef)
\end{equation} with \begin{equation}
r_{ij}(X):=\begin{cases}\frac{\delta_{ij}}{d_{ij}(X)}&\text{ if }d_{ij}(X)>0,\\
0&\text{ if }d_{ij}(X)=0.
\end{cases}
\end{equation} Note that $B$ is a function from the set of $n\times p$
configurations into the set of symmetric doubly-dentered matrices of
order $n$. All matrices of the form $\sum x_{ij}A_{ij}$, where summation
is over all pairs $(i,j)$ with $j<i$, are symmetric and doubly-centered.
They have $-x_{ij}$ as off-diagonal elements while the diagonal elements
$(i,i)$ are $\sum_{j=1}^nx_{ij}$.

Because $B(X)$ and $V$ are non-negative linear combinations of the
$A_{ij}$ they are both positive semi-definite. Because $W$ is assumed to
be irreducible the matrix $V$ has rank $n-1$, with only vectors
proportional to the vector $e$ with all elements equal to one in its
null-space (@deleeuw_C_77).

Summarizing the results so far we have \begin{equation}
\sigma(X)=\frac12\{\eta_{\hat D}^2-\text{tr}\ X'B(X)X+\text{tr}\ X'VX\}.
(\#eq:sigmat)
\end{equation}

Next we define the *Guttman transform* of a configuration $X$, for given
$W$ and $\Delta$, as \begin{equation}
G(X)=V^+B(X)X,
(\#eq:gudef)
\end{equation} with $V^+$ the Moore-Penrose inverse of $V$. In our
computations we use $$
V^+=(V+\frac{1}{n}ee')^{-1}-\frac{1}{n}ee'
$$ Also note that in the unweighted case with complete data $V=nJ$,
where $J$ is the centering matrix $I-\frac{1}{n}ee'$, and thus
$V^+=\frac{1}{n}J$. The Guttman transform is then simply
$G(X)=n^{-1}B(X)X$.

# Properties of Smacof Loss

## Derivatives

The Euclidean distance function $d_{ij}$ from ... is not differentiable
at configurations $X$ with $x_i=x_j$. If $d_{ij}(X)>0$ then $$
\mathcal{D}\sigma(X)=\frac{1}{d_{ij}(X)}A_{ij}X
$$ If $d_{ij}(X)=0$ then $$
D_+d_{ij}(X,Y)=\lim_{\epsilon\downarrow 0}\frac{d_{ij}(X+\epsilon Y)-d_{ij}(X)}{\epsilon}=d_{ij}(Y)
$$ which is non-linear in $Y$, showing non-differentiability.

$$
D_+\sigma(X,Y)=\text{tr}\ Y'(V-B(X))X+\sum\{w_{ij}\delta_{ij}d_{ij}(Y)\mid d_{ij}(X)=0\}
$$ This form of the directional derivative is used by @deleeuw_A_84f to
show that two independent necessary conditions for a local minimum are
$(V-B(X))X=0$ and $d_{ij}(X)>0$ for all $(i,j)$ with
$w_{ij}\delta_{ij}>0$. \### Gradient

$$
\mathcal{D}\sigma(X)=(V-B(X))X
$$ At a stationary point $B(X)X=VX$ or $V^+B(X)X=X$. Thus a necessary
condition for a local minimum is that $V^+B(X)$ has at least $p$
eigenvalues equal to one. @deleeuw_U_14b has shown that if
$V^+B(X)\lesssim I$ then actually $X$ is a global minimizer of stress.

### Hessian

The results on the Hessian of stress are largely unpublished. So we
summarize them here in this manual, so they'll be even more unpublished.

$$
H_{st}(X):=\sum w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left\{\frac{A_{ij}x_sx_t'A_{ij}}{d_{ij}^2(X)}\right\}
$$

$$
H_{st}(X)=\sum   w_{ij}\frac{\delta_{ij}}{d_{ij}^3(X)}(x_{is}-x_{js})(x_{it}-x_{jt})A_{ij}
$$ $$
\mathcal{D}_{st}\sigma(X)=\begin{cases}H_{st}(X)&\text{ if }s\not= t,\\
V-B(X)+H_{st}&\text{ if }s= t.
\end{cases}
$$ If $I_p$ is the identity matrix of order $p$, and $\otimes$ is the
Kronecker product, then $$
\mathcal{D}^2\sigma(X)=I_p\otimes(V - B(X))+ H(X)
$$ $$
\sum_{s=1}^p\sum_{t=1}^p y_s'H_{st}y_t=\sum w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left\{\frac{(\text{tr} \ Y'A_{ij}X)^2}{d_{ij}^2(X)}\right\}\leq\sum w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\text{tr}\ Y'A_{ij}Y=\text{tr}\ Y'B(X)Y.
$$ Thus $$
0\lesssim H\lesssim I_p\otimes B(X),
$$ and $$
I_p\otimes (V-B(X))\lesssim\mathcal{D}^2\sigma(X)\lesssim I_p\otimes V
$$ At a local minimum of $\sigma$ $$
0\lesssim\mathcal{D}^2\sigma(X)\lesssim I_p\otimes V
$$ In comparing the lower bounds on $\mathcal{D}^2\sigma(X)$ in ... and
... @deleeuw_U_14b shows that $V-B(X)\gtrsim 0$ is sufficient for a
*global* minimum of stress (but far from necessary).

Also $$
\sum_{t=1}^p H_{st}y_t=\sum w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left\{\frac{\text{tr}\ Y'A_{ij}X }{d_{ij}^2(X)}\right\}A_{ij}x_s
$$ If $Y=X$ then $H(X)y=(I_p\otimes B(X))x$ and thus
$$\mathcal{D}^2\sigma(X)x=(I_p\otimes V)x.$$ In the unweighted case this
means that $X$ is an eigenvector of $\mathcal{D}^2\sigma(X)$ with
eigenvalue $n$. Inequalities ... show that this is actually the largest
eigenvalue. Or $(I_p\otimes V)^+\mathcal{D}^2\sigma(X)\lesssim I$.

If $Y=XT$ with $T$ anti-symmetric then $\text{tr}\ Y'A_{ij}X=0$ then
thus $H(X)y=0$. Thus $$
\sum_{t=1}^p\mathcal{D}_{st}\sigma(X)y_t=(V-B(X))y_t
$$ which is zero if $\mathcal{D}\sigma(X)$ is zero. Thus at a stationary
point of stress $\mathcal{D}^\sigma(X)$ has $\frac12p(p-1)$ zero
eigenvalues.

There are several ways to think of the Hessian. The simplest one
(perhaps) is as an $np\times np$ symmetric matrix (corresponding to

column-major R vector of length $\frac12 np(np+1)$). This is what we
would use for a straightforward version of Newton-Raphson.

It is more elegant, however, to think of $H$ as a symmetric super-matrix
of order $p$, with as elements $n\times n$ matrices. And, for some
purposes, such as the pseudo-confidence ellipsoids in @deleeuw_E_17q, as
a super-matrix of order $n$ with as elements $p\times p$ matrices. Both
the super-matrix interpretations lead to four-dimensional arrays, the
first a $p\times p\times n\times n$ array, the second an
$n\times n\times p\times p$ array. The different interpretations lead to
different ways to store the Hessian in memory, and to different ways to
retrieve its elements. Of course we can write routines to transform from
one interpretation to another.

## Lagrangian

In our implementation of the smacof algorithm we minimize stress over
configurations with $\eta(X)=1$, or, equivalently,
$\sum w_{ij}d_{ij}^2(X)=1$. This means we do not look for $X$ with
$\mathcal{D}\sigma(X)=(V-B(X))X=0$, but we look for solutions of $$
(V-B(X))X-\lambda VX=0,\\
\text{tr}\ X'VX= 1.
$$ At the solution $$
\lambda=1-\rho(X)
$$ and $$
X = \frac{\Gamma(X)}{\eta(\Gamma(X))}
$$ Also it is necessary for a local minimum that $$
\Gamma(X)=\rho(X)X
$$ Because the Guttman transform is homogeneous of degree zero this
implies $$
\Gamma(\Gamma(X)=\Gamma(X),
$$ so although $X$ is not a fixed point of the Guttman transform,
$\Gamma(X)$ is.

The second order necessary condition is that $$
H(X)\gtrsim I_p\otimes (\rho(X)V - B(X))
$$ is positive

### Kuhn-Tucker Points

bozo

# Smacof Algorithm

## First Phase: Update Configuration

### Introduction to Majorization

Majorization, more recently better known as MM (@lange_16), is a general
approach for the construction of minimization algorithms. There is also
minorization, which leads to maximization algorithms, which explains the
MM acronym: minorization for maximization and majorization for
minimization.

Before the MM principle was formulated as a general approach to
algorithm construction there were some important predecessors. Major
classes of MM algorithms avant la lettre were the *EM Algorithm* for
maximum likelihood estimation of @dempster_laird_rubin_77, the *Smacof
Algorithm* for MDS of @deleeuw_C_77, the *Generalized Weiszfeldt Method*
of @vosz_eckhardt_80, and the *Quadratic Approximation Method* of
@boehning_lindsay_88. The first formulation of the general majorization
principle seems to be @deleeuw_C_94c.

Let's start with a brief introduction to majorization. Minimize a real
valued function $\sigma$ over $x\in\mathbb{S}$, where $\mathbb{S}$ is
some subset of $\mathbb{R}^n$. There are obvious extensions of
majorization to functions defined on more general spaces, with values in
any partially ordered set, but we do not need that level of generality
in this manual. Also majorization applied to $\sigma$ is minorization
applied to $-\sigma$, so concentrating on majorization-minimization and
ignoring minorization-maximization causes no loss of generality

Suppose there is a real-valued function $\eta$ on
$\mathbb{S}\otimes\mathbb{S}$ such that

```{=tex}
\begin{align}
\sigma(x)&\leq\eta(x,y)\qquad\forall x,y\in\mathbb{S},(\#eq:maj1)\\
\sigma(x)&=\eta(x,x)\qquad\forall x\in\mathbb{S}.(\#eq:maj2)
\end{align}
```
The function $\eta$ is called a *majorization scheme* for $\sigma$ on
$S$. A majorization scheme is *strict* if $\sigma(x)<\eta(x,y)$ for all
$x,y\in S$ withj $x\not=y$.

Define \begin{equation}
x^{(k+1)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\eta(x,x^{(k)}),
(\#eq:majalg)
\end{equation} assuming that $\eta(\bullet,y)$ attains its (not
necessarily unique) minimum over $x\in\mathbb{S}$ for each $y$. If
$x^{(k)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\eta(x,x^{(k)})$ we
stop.

If we do not stop, then by update rule \@ref(eq:majalg) \begin{equation}
\sigma(x^{(k+1)})\leq\eta(x^{(k+1)},x^{(k)}),
\end{equation} by majorization property \@ref(eq:maj1) \begin{equation}
\eta(x^{(k+1)},x^{(k)})\leq\eta(x^{(k)},x^{(k)}).
\end{equation} and by majorization property \@ref(eq:maj1)
\begin{equation}
\eta(x^{(k)},x^{(k)})=\sigma(x^{(k)}).
\end{equation}

If the minimum in \@ref(eq:majalg) is attained for a unique $x$ then
$\eta(x^{(k+1)},x^{(k)})<\eta(x^{(k)},x^{(k)})$. If the majorization
scheme is strict then $\sigma(x^{(k+1)})<\eta(x^{(k+1)},x^{(k)})$. Under
either of these two additional conditions
$\sigma(x^{(k+1)})<\sigma(x^{(k)})$, which means that the majorization
algorithm is a monotone descent algorithm, and if $\sigma$ is bounded
below on $\mathbb{S}$ then the sequence $\sigma(x^{(k)})$ converges.

Note that we only use the order relation to prove convergence of the
sequence of function values. To prove convergence of the $x^{(k)}$ we
need stronger compactness and continuity assumptions to apply the
general theory of @zangwill_69a. For such a proof the argmin in update
formula \@ref(eq:majalg) can be generalized to
$x^{(k+1)}=\phi(x^{(k)})$, where $\phi$ maps $\mathbb{S}$ into
$\mathbb{S}$ such that $\eta(\phi(x),x)\leq\sigma(x)$ for all $x$.

We give a small illustration in which we minimize $\sigma$ with
$\sigma(x)=\sqrt{x}-\log{x}$ over $x>0$. Obviously we do not need
majorization here, because solving $\mathcal{D}\sigma(x)=0$ immediately
gives $x=4$ as the solution we are looking for.

To arrive at this solution using majorization we start with
\begin{equation}
\sqrt{x}\leq\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}},
(\#eq:sqrtmaj)
\end{equation} which is true because a differentiable concave function
such as the square root is majorized by its tangent everywhere.
Inequality \@ref(eq:sqrtmaj) implies \begin{equation}
\sigma(x)\leq\eta(x,y):=\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}}-\log{x}.
(\#eq:examplemaj)
\end{equation} Note that $\eta(\bullet,y)$ is convex in its first
argument for each $y$. We have $\mathcal{D}_1\eta(x,y)=0$ if and only if
$x=2\sqrt{y}$ and thus the majorization algorithm is \begin{equation}
x^{(k+1)}=2\sqrt{x^{(k)}}
(\#eq:examplealg)
\end{equation} The sequence $x^{(k)}$ converges monotonically to the
fixed point $x=2\sqrt{x}$, i.e. to $x=4$. If $x^{(0)}<4$ the sequence is
increasing, if $x^{(0)}<4$ it is decreasing. Also, by l'Hôpital,
\begin{equation}
\lim_{x\rightarrow 4}\frac{2\sqrt{x}-4}{x-4}=\frac12
(\#eq:hopi1)
\end{equation} and thus convergence to the minimizer is linear with
asymptotic convergence rate $\frac12$. By another application of
l'Hôpital \begin{equation}
\lim_{x\rightarrow 4}\frac{\sigma(2\sqrt{x)})-\sigma(4)}{\sigma(x)-\sigma(4)}=\frac14,
(\#eq:hopi2)
\end{equation} and convergence to the minimum is linear with asymptotic
convergence rate $\frac14$. Linear convergence to the minimizer is
typical for majorization algorithms, as is the twice-as-fast linear
convergence to the minimum value.

This small example is also of interest, because we minimize a *DC
function*, the difference of two convex functions. In our example the
convex functions are minus the square root and minus the logarithm.
Algorithms for minimizing DC functions define other important subclasses
of MM algorithms, the *DC Algorithm* of Tao Pham Dinh (see @lethi_tao_18
for a recent overview), the *Concave-Convex Procedure* of
@yuille_rangarajan_03, and the *Half-Quadratic Method* of Donald Geman
(see @nikolova_ng_05 for a recent overview). For each of these methods
there is a huge literature, with surprisingly little non-overlapping
literatures. The first phase of the smacof algorithm, in which we
improve the configuration for given disparities, is DC, concave-convex,
and half-quadratic.

In the table below we show convergence of \@ref(eq:examplealg) starting
at $x=1.5$. The first column show how far $x^{(k)}$ deviates from the
minimizer (i.e. from 4), the second shows how far$\sigma(x^{(k)})$
deviates from the minimum (i.e. from $2-\log 4$). We clearly see the
convergence rates $\frac12$ and $\frac14$ in action.

```{r majiter, echo = FALSE}
x <- 1.5
f <- sqrt(x) - log(x)
f0 <- 2 - log(4)
for (i in  1:15) {
  cat("itel ", formatC(i, digits = 0, width = 2, format = "d"),
      formatC(4 - x, digits = 10, format = "f"),
      formatC(f - f0, digits = 10, format = "f"),
      "\n")
  x <- 2 * sqrt(x)
  f <- sqrt(x) - log(x)
}
```

The first three iterations are shown in the figure below. The vertical
lines indicate the value of $x$, function is in red, and the first three
majorizations are in blue.

```{r majplot, fig.align = "center", echo = FALSE, cache = TRUE}
x <- 100:500/100
y <- sqrt(x) - log(x)
plot(x, y, type = "l", lwd = 3, col = "RED")
g <- function(x, y) {
return(sqrt(y)+ (x - y) / (2 * sqrt(y)) - log(x))
}
x1 <- 1.5
abline(v = x1)
z1 <- g(x, x1)
lines(x, z1, col = "BLUE")
x2 <- 2 * sqrt(x1)
abline(v = x2)
z2 <- g(x, x2)
lines(x, z2, col = "BLUE")
x3 <- 2 * sqrt(x2)
abline(v = x3)
z3 <- g(x, x3)
lines(x, z3, col = "BLUE")
```

### Majorizing Stress

## Second Phase: Update Transformation

### Spline Basis Details

*Splines* of degree $d$ on a closed interval $[a,b]$ are piecewise polynomials of degree $d$. The endpoints of the interval are the *boundary knots*. In the interval there are a number of *inner knots*. For smacof we suppose the inner knots are distinct. There is a polynomial piece between all consecutive
pairs of knots. Although the pieces can be parts of different polynomials
splines are required to have a certain degree of smoothness. In fact
at the interior knots a spline has $d - 1$ continuous derivatives. A
spline of degree zero is a step function, stepping to a different level
at each knot. A spline of degree one is piecewise linear, where the 
line segments are joined continuously at the inner knots. A spline of
degree two is piecewise quadratic and continuously differentiable at the
knots. And so on. There is no limit on the number of inner knots and
on the degree of the spline, although the number of interior knots
must be greater than or equal to the degree minus one. The flexibility
of the spline (as opposed to the rigidity of a polynomial on $[a,b]$
of the same degree) comes from the number and placement of the interior knots, not so much from the degree of the spline.

### B-splines

### Berstein PolynomiaLS


```{r bsplinekable, echo = FALSE}
x <- matrix(c(0:4, 1:5, rep(4,5), c(6, 8, 10, 12, 14), 5:9), 5, 5)
kable(x, format = "pipe", digits = 0, col.names = c("degree", "order", "ninner", "nknots", "span"), align = 'c', caption = "B spline parameters")
```

$$
\sum_i B_{i,k}(x)=1
$$

M-splines

$$
M_{i,k}(x)=\frac{k+1}{t_{i+k+1}-t_i}B_{i,k}(X)
$$ then $$
\int M_{i,k}(x)dx=1
$$

I-splines $$
I_{i,k+1}(z)=\int_{-\infty}^zM_{i,k}(x)dx
$$

When is a B-spline increasing ? $$
\mathcal{D}B_{i,k}(x)=
$$ Thus if $$
\mathcal{D}\sum_{i=1}^{d+m}\alpha_iB_{i,k}(x)=
$$

It is sufficient that $\alpha_i\leq\alpha_{i+1}$

Integral, I-splines

### Ordinal MDS

### Interval and Ratio MDS

### Cyclic Coordinate Decent

In the non-linear least squares (NNLS) problem the data are an
$n\times p$ matrix $X$, a vector $y$ with $n$ elements, and a positive
semi-definite diagonal matrix $W$. We want to minimize $$
\sigma(\beta):=\frac12(X\beta-y)'W(X\beta-y)
$$ over $\beta\geq 0$. In data analysis and statistics the problem is
often solved by *active set methods*, implemented in R for example by
NNLS (@mullen_vanstokkum_23) and FNNLS (@bro_dejong_97). Active set
methods are finitely convergent dual methods. While iterating the
intermediate solutions are not feasible (i.e. non-negative). In fact in
dual methods we reach feasibility and optimality at the same time. Also
the number of iterations, although theoretically finite, can be very
large.

In each smacof iteration we need an NNLS solution. Especially in the
early iterations the solution does not have to be very precise. Also the
solution from the previous NNLS problem will generally provide a very
good starting value for the next iteration (each NNLS problem has a "hot
start"). And finally, we would like all internediate solutions to be
feasible. These considerations have lead us to using *cyclic coordinate
descent* (CCD).

Suppose the current best feasible solution in CCD iteration $k$ is
$\beta^{(k)}$. The next CCD iteration changes each of the $p$
coordinates of $\beta^{(k)}$ in turn, maintaining feasibility, while
keeping the other $p-1$ coordinates fixed at their current values. Thus
within a CCD iteration $k$ we create intermediate solutions
$\beta^{(k,1)},\cdots,\beta^{(k,p)}$, where each of the intermediate
solutions $\beta^{(k,r)}$ differs from the previous one
$\beta^{(k,r-1)}$ in a single coordinate. For consistency we define
$\beta^{(k,0)}:=\beta^{(k)}$. After the iteration is finished we set
$\beta^{(k+1)}=\beta^{(k,p)}$.

Note that in smacof each iteration modifies the coordinates in the order
$1,\cdots,p$, which explains why the method is called "cyclic". There
are variations of CCD in which the order within an iteration is random
or greedy (choose the coordinate which gives the largest improvement) or
zig-zag $1,\cdots,p,p-1,\cdots,1$. We have not tried out these
alternatives in smacf, but we may in the future.

The effect of changing a single coordinate on the loss function is $$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$ where $e_j$ is the unit vector corresponding with the coordinate we
are changing, $g(\beta):=\mathcal{D}\sigma(\beta)=X'Wr(\beta)$ is the
gradient at $\beta$, and $r(\beta):=X\beta-y$ is the residual. Also
$S:=X'WX$. Note that if $s_{jj}=0$ then also $g_j(\beta)=0$ and thus
$\sigma(\beta+\epsilon e_j)=\sigma(\beta)$. In each CCD cycle we simply
skip updating coordinate $j$.

If $s_{jj}>0$ then $\sigma(\beta+\epsilon e_j)$ is a strictly convex
quadratic in $\epsilon$, which we must minimize under the constraint
$\beta_j+\epsilon\geq 0$ or $\epsilon\geq-\beta_j$. Define
$\hat\epsilon$ to be the solution of this constrained minimization
problem.

The quadratic ... has its minimum at $$
\tilde\epsilon=-\frac{g_j(\beta)}{s_{jj}}
$$ If $\beta+\tilde\epsilon$ is feasible then it is the update we are
looking for. Thus $\hat\epsilon=\tilde\epsilon$. If
$\beta+\tilde\epsilon<0$ then the contrained minimum is attained at the
boundary, i.e. $\hat\epsilon=-\beta_j$ and the updated $\beta_j$ is
zero. Thus, in summary, $\hat\epsilon=\max(\tilde\epsilon,-\beta_j)$.

One of the nice things about CCD is that $$
r(\hat\beta)=r(\beta)+\hat\epsilon x_j
$$ $$
g(\hat\beta)=g(\beta)+\hat\epsilon s_j
$$

It follows that $\hat\epsilon=0$ if and only if either $\beta_j=0$ and
$g_j(\beta)\geq 0$ or if $g_j(beta)=0$ and $beta_j>0$.

If $g_j(\beta)<0$ then $\tilde\epsilon>0$, and thus $\hat\epsilon>0$ and
$\sigma(\hat\beta)<\sigma(\beta)$. Thus we must have $g_j(\beta)\geq 0$.

If $\beta_j>0$ and $g_j(\beta)\not=0$ then there is an $\epsilon$ such
that $\sigma(\beta+\epsilon e_j)<\sigma(\beta)$. Thus if $\beta_j>0$ we
must have $g_j(\beta)=0$.

In summary at the minimum of $\sigma$ over $\beta\geq 0$ we must have
$\beta_j\geq 0$, $g_j(\beta)\geq 0$, and $\beta_jg_j(\beta)=0$ for all
$j$ (*complementary slackness*).

$$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$ where $S:=X'WX$.

Now suppose we minimize $\sigma$ over $\beta\geq 0$.

Our best solution so far is $\beta^{(k)}\geq 0$. Minimize
$\sigma(\beta^{(k)}+\epsilon e_1)$ over $\epsilon$ on the condition that
$\beta^{(k)}_1+\epsilon\geq 0$ or $\epsilon\leq-\beta^{(k)}_1$. If
$s_{11}=0$ then also $g_1(\beta)=0$ and we set
$\beta^{(k+1,1)}=\beta^{(k,1)}$. If $s_{11}>0$ we compute $$
\tilde\epsilon=-g_1(\beta)/s_{11}
$$ If $$
\beta^{(k)}_1+\tilde\epsilon\geq 0
$$ then $$
\beta^{(k+1,1)}=\beta^{(k)}_1+\tilde\epsilon
$$ If $$
\beta^{(k)}_1+\tilde\epsilon<0
$$ we set $$
\beta^{(k+1,1)}=0.
$$

# Smacof Program

### Front-end

The front-end for both smacofRR and smacofRC is written in R. The
analysis is started in the user's working directory by the command
smacofRR(foo) or smacofRC(foo), where foo is a user-chosen name (without
quotes).

Two text files need to be present in the working directory. The first is
fooParameters.txt, where of course you substitute the user-chosen name
for foo. The second file is fooDelta.txt, which has the dissimilarities
below the diagonal in row-major order.

The parameter file has key-value format. Here is an example.

```{r parfile, eval = FALSE}
nobj   9
ndim   3
init   2
width   10
precision   6
haveweights   0
itmax  1000
epsi  10
verbose  1
ditmax  5
depsi  6
dverbose  0
kitmax  5
kepsi  6
kverbose  0
degree  3
haveknots  3
ninner  5
ordinal  1
anchor 0
intercept  1
```

The parameter file is read first, using the R function read.table().
There is one key-value pair on each line, at the start of the line. The order of the lines does not matter. There can be additional comments or other text on
each line after the value field, as long as that text is space-separated
from the value field. Additional key-value lines with non-existing
parameters can be added at will.

Values of the parameters are put the local environment using R function
assign(), which means they are available to R throughout the smacof run.
Of course if we choose smacofRC the front-end needs to pass them to C
using C(), but they will be available again for the back-end. 

The Delta file, and any subsequent optional input files, are read with
the R function scan(). Values are separated by spaces. They can be on a
single line, or laid out as a lower-triangular matrix, or whatever. The
function scan() only stops reading if it reaches the end-of-file.

We'll now discuss the parameters one by one. Note that all parameters
are integers. The first two are obvious: *nobj* is the number of objects
and *ndim* the number of dimensions. These two parameters have no
default or recommended values, because they deyermined by the data. All
other parameters in our example parameter file are set to reasonable
values in our example parameter file. But the whole idea is to
experiment with various combinations of parameter values, so
"reasonable" is weaker than "recommended" and "recommended" is weaker
than "default".

The *init* parameter can have values 1, 2, or 3. If *init* equals 1 the
program reads an initial configuration from the file fooXinit.txt in the
working directory. The file has *nobj* \* *ndim* numbers, the initial
configuration, in row-major format. If *init* = 2 then the classical
Torgerson initial estimate will be computed. If *init* = 3 a random
initial estimate will be used.

*width* and *precision* are parameters for the output of the values of
stress during iterations.

*haveweights* is either zero or one. If zero there are no weights, which
is equivalent to all weights equal to one. If one then we will read a
file fooWeights.txt, which has the lower-diagonal $\frac12 n(n-1)$
weights in row-major order.

As explained in previous sections there are three iterative running in
smacof. There are two inner iterations: one for the configuration for
fixed disparities, and one for the disparities for fixed configuration.
The two inner iteration loops are nested in one outer iteration loop.
Each of the iterations has three parameters: one for the maximum number
of iterations, one for the stopping criterion, and one for the verbosity
of the iteration output. For the outer loop the parameters are *itmax*,
*ieps*, and *verbose*. For the inner configuration loop they are
*kitmax*, *keps*, and *kverbose*. And the inner transformation loop they
are *ditmax*, *deps*, and *dverbose*. If the verbose parameter is one,
then each iteration prints out the stress before and the stress after
update. If verbose is zero, nothing is printed. The stopping parameters
check if the change in stress in an iteration is less than epsilon,
where epsilon is 10\^-ieps, 10\^-keps, or 10\^-deps.

The final five parameters are used to define the nature of the spline
space for the transformations. *degree* is the degree of the piecewise
polynomials. The *haveknots* parameter can be 0, 1, 2, or 3. If it is
zero, there are no inner knots and we use the Bernstein polynomial
basis. If *haveknots* is one, the inner knots are read in from
fooKnots.txt in the usual way. If *haveknots* is two the knots are
equally spaced between zero and one, and if it is three the knots are
equally spaced on the precentile scale (so that the number of data
points between knots is approximately the same). The *ninner* parameter
determines the number of knots in the case that *haveknots* is either
two or three. If *haveknots* is zero, then *ninner* should be zero, if
*haveknots* is one it should be equal to the number of knots in
fooKnots.txt.

The three final spline parameters are *ordinal*, *anchor*, and *intercept*. If *ordinal* is one the fitted spline is constrained to be monotone. If *intercept* is zero then the first spline coefficient is constrained to be zero (which
means the first column is deleted from the basis). This means that the spline
is constrained to be zero at the lower boundary knot. If *intercept* is one
there is no such constraint, and the spline can be anything at the lower boundary
(subject to monotonicity of *ordinal* is one). If *anchor* is one then the
boundary knots are set to zero and the maximum dissimilarity, if *anchor* is
zero the boundary knots are the minimum and maximum dissimilarity. Thus if *intercept* is zero and  *anchor* is one the spline goes through the origin.

The computations in the frontend are straightforward. We first transform
the dissimilarities linearly so that the smallest becomes zero and the
largest becomes one. This is not strictly necessary but it makes the
spline computations slightly easier.

Initial Estimates for $X$ Spline Basis

### Engine

ALS First Phase Second Phase

### Back-end

The back-end consists of a number of R functions that have the list
returned by smacofRR or smacofRC as an argument. They can be used to 
make plots, compute derivatives, convert matrices to an easily
printable format, do sensitivity analysis, and so on. The philosophy
is that in the backend the main computing is finished and we
just create different representations of the results.

#### Plotting

There are two main plot functions in the backend: smacofShepardPlot()
and smacofConfigurationPlot(). A smacofShepardPlot has the
dissimilarities (un-normalized) on the horizontal axes and
it has the distances and the disparities on the vertical axis.
It draws the spline, and shows where the fitted disparities are
on the spline. It also plots the (delta, dist) pairs as points,
to show how far they deviate from the spline. Optionally 
smacofShepardPlot() can draw vertical lines at the inner knots
(argument knotlines = TRUE), and optionally it can connect 
the (delta, dhat) points on the spline to the (delta, dist)
points with lines (argument fitlines = TRUE). 

It must be emphasized that smacofShepardPlot() draws the spline
over the whole interval, which is either (deltamin, deltamax)
if anchor = 0 and (0, deltamax) if anchor = 1. It does
this by recomputing the spline at a large number of
uniformly spaced points in the interval, where the
number of points is given by the smacofShepardPlot()
parameter resolution. Thus we do not use only the
data points (delta,dhat) and then let the R plot function 
interpolate linearly. That can be misleading. It is
especially misleading if degree is zero (step function)
or if there are consecutive inner knots with no data values
between the knots. Degree zero is handled by the special 
purpose step function plotting routine smacofPlotStepFunction(),
which makes sure the spline is drawn as a horizontal segment
from one knot to the next knot. In addition smacofShepardPlot()
can set some base R plot parameters such as col, cex, lwd, and pch
(see the R documentation).

The smacofConfigurationPlot() function is much simpler than
smacofShepardPlot(). It sets pch, col, and cex. It uses
the smacofRR/RC labels parameter to decide how to label
the points in the configuration. If labels = 1 it reads
a character vector of labels from fooLabels.txt, where
foo is of course the name of the run. If labels = 2
the points are numbered, if labels = 3 plotting uses
the pch symbol for all points. If the dimension $p$
is larger than two, smacofConfigurationPlot() uses
the parameters dim1 and dim2 to select the dimensions
to plot.

#### Writing

#### Checking

#### Derivatives

#### Sensitivity

Perturbation regions 

Parametric Bootstrap 

Jacknife

# Examples


## ekman

The @ekman_54 color circle example has been used in many, if not most, multidimensional scaling textbooks and review articles. This is due, no doubt,
to its astonishing good two-dimensional fit and its easy interpretability.

```{r ekmanresults, echo = FALSE, cache = TRUE}
source("results/ekmanResults.R")
```

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT TABLE \@ref(tab:ekmankable) ABOUT HERE**
:::
::::

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmancubic) ABOUT HERE**
:::
::::

:::: {.greybox data-latex=""}
::: {.center data-latex=""}
**INSERT FIGURE \@ref(fig:ekmanordinal) ABOUT HERE**
:::
::::

## gruijter

The two previous examples had a good fit. We use the data from @degruijter_67
for an example with a rather bad fit in two dimensions and a rather good fit in three dimensions.
Dissimilarity judgments between nine Dutch political parties were
collected in 1966 by the complete method of triads. Results were
averaged over 100 students. Averaging over a heterogeneous population
will cause regression to the mean, and thus dissimilarities will tend
to be more equal than they should be on the basis of the individual 
results. This means there should be a large difference in fit between
the "ratio" and the "interval" options
(i.e. between the interval parameter equal to one or zero).


## morse

```{r morseresults, echo = FALSE, cache = TRUE}
source("results/morseResults.R")
```

# Tables


```{r ekmankable, echo = FALSE, cache = TRUE}
x <- matrix(c(
0, 3, 0, 1, 0, 1, 2, ekman030$snew, ekman030$itel,
5, 3, 3, 1, 0, 1, 2, ekman533$snew, ekman533$itel,
50, 0, 3, 1, 0, 1, 2, ekman5003$snew, ekman5003$itel, 
50, 0, 2, 1, 0, 1, 2, ekman5002$snew, ekman5002$itel), 4, 9, byrow = TRUE)
knitr::kable(x, format = "pipe", digits = 10, col.names = c("ninner", "degree", "haveknots", "ordinal", "intercept", "anchor", "ndim", "stress", "itel"), align = 'c',
caption = "Analyses of the ekman example")
```
******
```{r gruijterkable, echo = FALSE, cache = TRUE, eval = FALSE}
x <- matrix(c(
0, 1, 0, 1, 0, 1, 2, gruijter0100$snew, gruijter0100$itel,
0, 1, 0, 1, 1, 1, 2, gruijter0101$snew, gruijter0101$itel,
5, 3, 3, 1, 0, 1, 2, gruijter5332$snew, gruijter5332$itel, 
5, 3, 3, 1, 0, 1, 3, gruijter5333$snew, gruijter5333$itel), 4, 9, byrow = TRUE)
knitr::kable(x, format = "pipe", digits = 10, col.names = c("ninner", "degree", "haveknots", "ordinal", "intercept", "anchor", "ndim", "stress", "itel"), align = 'c', caption = "Analyses of the gruijter example")
```
******

# Figures


```{r ekmancubic, fig.align = "center", fig.cap = "Ekman example, cubic analysis", echo = FALSE, cache = TRUE}
par(mfrow = c(1,2))
smacofShepardPlot(ekman030, main = "Cubic Polynomial")
smacofShepardPlot(ekman533, main = "Cubic Spline")
```

```{r ekmanordinal, fig.align = "center", fig.cap = "Ekman example, nonmetric analysis", echo = FALSE, cache = TRUE}
par(mfrow = c(1,2))
smacofShepardPlot(ekman5003, main = "Equal Percentile Intervals")
smacofShepardPlot(ekman5002, main = "Equal Delta Intervals")
```


# References
