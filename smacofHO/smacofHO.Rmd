---
title: |
    | Smacof at 50: A Manual
    | Part 8: Homogeneity Analysis with Smacof
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started April 13 2024, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex 
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 3
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 3
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: smacofVO
---
```{r loadpackages, echo = FALSE}
#suppressPackageStartupMessages (library (foo, quietly = TRUE))
```

```{r load code, echo = FALSE}
#dyn.load("foo.so")
#source("janUtil.R")
```


**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw> in the repositories smacofCode, smacofManual,
and smacofExamples.


# Simultaneous Non-Metric Unfolding{#snmu}

* There are $m$ *variables*. 
* Variable $j$ has $k_j>1$ *categories*.
* There are $n$ *objects*.
* Each object defines a *partial order* over the categories of each variable.

The simultaneous non-metric unfolding problem is to minimize the stress loss function
\begin{equation}
\sigma(X,Y_1,\cdots,Y_m):=\sum_{j=1}^m\sum_{i=1}^n\min_{\hat d_i^j\in\Delta_i^j}\sum_{l=1}^{k_j}w_{il}^j(\hat d_{il}^j-d(x_i,y_l^j))^2
(\#eq:snmu)
\end{equation}
Note that for each object and variable there are different sets of transformations $\Delta_j$
and for each variable there different matrices of *category scores* $Y_j$, but there is only a single matrix of *object scores* $X$. Also note that index $j$, for variables, is sometimes used as a subscript and sometimes as a superscript, depending on what looks best.

If the $\Delta_i^j$ contain the zero vector, then the unconstrained minimum of \@ref(eq:snmu) is zero. Collapsing all $x_i$ and all $y_l^j$ into a single point makes all distances zero, and thus makes stress zero. In fact, it is easy to see that a minimum of zero stress is also possible in the situation where the $\Delta_i^j$ contain the set of all constant vectors (or all non-negative constant vectors). Collapse all
$x_i$ into a single point, and place all $y_l^j$ for variable $j$ on a sphere around this point. There can be different spheres for different variables. This makes all $d(x_i,y_l^j)$ equal and thus makes stress zero. Some constraints on $X$ and/or the $Y_j$ are needed to prevent these trivial solutions.

In the context of non-metric unfolding ... 

# Homogeneity Analysis{#hom}

The Gifi System (@gifi_B_90, @michailidis_deleeuw_A_98, @deleeuw_mair_A_09a) implements non-linear or non-metric versions of the classical linear multivariate analysis techniques (regression, analysis of variance, canonical analysis, discriminant analysis, principal component analysis). The non-linear versions are introduced as special cases of *Homogeneity Analysis*, which is better known under the name  *Multiple Correspondence Analysis*.

In this section we present homogeneity analysis as a technique for
minimizing the loss function \@ref(eq:snmu) when the data are $n\times k_j$ *indicator matrices* $G_j$, with $j=1,\cdots,M$. This is a non-standard
presentation, because usually homogeneity analysis is related to
principal component analysis, and not to multidimensional scaling
(see, for example, @deleeuw_C_14 or @deleeuw_C_23).
Indicator matrices are binary matrices, with rows that add up to one or to zero. 
Thus each row has either a single elements equal to one and the rest zeroes,
or all elements equal to zero. Indicator matrices are
used to code categorical variables. Rows corresponds with objects
(or individuals), columns with the categories (or levels) of a variable.
Element $g_{il}^j$ is one if object $i$ is in category $l$ of variable $j$,
and all other elements in row $i$ are zero. If an object is *missing* on
variable $j$ then the whole row is zero.

Homogeneity analysis makes joint maps in $p$ dimensions of objects
and categories, both represented as points. A joint map for variable $j$
has $n$ object points $x_i$ and $k_j$ category points $y^j_{il}$.
In a homogeneous solution the object points are close to the points of the categories that the objects score in, i.e, to those $y^j_{il}$ for which $g^j_{il}=1$.
If there is only one variable then it is trivial to make a perfectly 
homogeneous map. We just make sure the object points coincide with
their category points. But there are $j>1$ indicator matrices, corresponding with $m$ categorical variables, and there is only a single set of object scores. The solution is a compromise trying to achieve as much homogeneity as possible for all variables simultaneously.

In loss function \@ref(eq:snmu) applied to homogeneity analysis
the sets $\Delta_i^j$ are defined in such a way that $\hat d_{il}^j$ is zero if $i$ is in category $l$ of 
variable $j$. There are no constraints on the other $\hat d$'s in row $i$
of variable $j$. Thus for zero loss we want an object to coincide with all $m$ categories it is in. With this definition of the $\Delta_i^j$ we have
\begin{equation}
\min_{\hat d_i^j\in\Delta_i^j}\sum_{l=1}^{k_j}w_{il}^j(\hat d_{il}^j-d(x_i,y_l^j))^2=f_{ij}d_{ij}^2(X,Y),
(\#eq:homsnmu)
\end{equation}
where 
\begin{subequations}
\begin{align}
d_{ij}(X,Y)&:=\sum_{l=1}^{k_j}g_{il}^jd(x_i,y^j_l),(\#eq:dred)\\
f_{ij}&:=\sum_{l=1}^{k_j}w^j_{il}w^j_{il}.(\#eq:wred)
\end{align}
\end{subequations}
Note that the $w^j_{il}$ for which $g^j_{il}=0$ play no role in 
homogeneity analysis. In the usual implementations of homogeneity
analysis and multiple correspondence analysis 
$f_{ij}$ is either zero or one, depending on whether observation
$i$ on variable $j$ is missing or non-missing.

Using indicator matrices we can write loss function \@ref(eq:homsnmu) as
\begin{equation}
\sigma(X,Y_1,\cdots,Y_m)=
\sum_{j=1}^m\text{tr}\ (X-G_jY_j)'F_j(X-G_jY_j),
(\#eq:matsnmu)
\end{equation}
The $F_j$ are diagonal matrices with the $f_{ij}$ from \@ref(eq:wred) 
on the diagonal.

In homogeneity analysis we minimize \@ref(eq:matsnmu) using the
explicit normalization $X'F_\star X=I$, where $F_\star$ is the
sum of the $F_j$. The solution is given by the singular value
equations
\begin{subequations}
\begin{align}
X\Lambda&=F_\star^{-1}\sum_{j=1}^m F_jG_jY_j,(\#eq:homsvd1)\\
Y_j&=(G_j'F_jG_j)^{-1}G_j'F_jX,(\#eq:homsvd2)
\end{align}
\end{subequations}
where $\Lambda$ is a symmetric matrix of Lagrange multipliers. 

In homals (@gifi_B_80, @deleeuw_mair_A_09a) alternating least squares is used
to solve the equations \@ref(eq:homsvd1) and \@ref(eq:homsvd2). We start with
some initial $X$, then compute the corresponding $Y_j$ using \@ref(eq:homsvd2),
then for these new $Y_j$ we compute a new corresponding $X$ from \@ref(eq:homsvd1),
and so on. Computations are efficient, because only diagonal matrices need to 
be inverted and matrix multiplication with an indicator matrix is not really multiplication but simply selection of a particular row or column. Alternating least squares thus becomes *reciprocal averaging*.  Equation \@ref(eq:homsvd2) says that the optimal category
point is the weighted averages of the objects points in the category, and \@ref(eq:homsvd1)
says that, except for rescaling with the Lagrange multipliers, the optimal object
point is the weighted average of the category points that the object scores in.

Alternative methods of computation (and interpretation) are possible if we
substitute \@ref(eq:homsvd2) in \@ref(eq:homsvd1) to eliminate the $Y_j$
and obtain an equation in $X$ only. This gives
\begin{equation}
F_\star X\Lambda=\sum_{j=1}^m F_jG_j(G_j'F_jG_j)^{-1}G_j'F_jX,
(\#eq:geneigx)
\end{equation}
which is a generalized eigenvalue equation for $X$. If we substitute \@ref(eq:homsvd1)
in \@ref(eq:homsvd2) we obtain generalized eigenvalue equations for $Y$.
\begin{equation}
(G_j'F_jG_j)Y_j\Lambda=\sum_{h=1}^m G_j'F_jW_\star^{-1}F_hG_hY_h.
(\#eq:geneigy)
\end{equation}
If $k_\star$, the sum of the $k_j$, is not too large then finding the $p$ largest non-trivial eigenvalues with corresponding eigenvectors from \@ref(eq:geneigy) may be computationally efficient. The largest "trivial" eigenvalue is always equal to one, no matter what the $G_j$ and $W_j$ are, and we can safely ignore it. The trivial solution with all distances equal to zero mentioned in section \@ref(snmu) corresponds with this largest eigenvalue.

Homogeneity analysis can be most convincingly introduced using the concept of a *star plot*. For variable $j$ we plot $k_j$ category points and $n$ object
points in a single joint plot. We then draw a line from each category point to the object points of the objects in that category. This creates $k_j$ groups of
lines and points in $\mathbb{R}^p$, and each of these groups is called a *star*.
The sum of squares of the line lengths of a star is the loss of homogeneity for category $l$ of variable $j$, and the total sum of squares of all line lengths in the $k_j$ stars is the loss \@ref(eq:matsnmu) for variable $j$. Homogeneity analysis chooses $X$ and the $Y_j$ such that $X$ is normalized by $X'F_\star X=I$ and the stars are as small or as compact as possible, measured by the squared line lengths. For given $X$ the stars are as small as possible by choosing the category points $Y_j$ as the centroids of the object points in the category, as in equation
\@ref(eq:homsvd2). That explains the use of the word "star", because now the stars really look like stars. In graph theory a star is a tree with one internal node (the category point) and $k$ leaves (the object points). Thus, given the optimum choice of the $Y_j$ as centroids, we can also say that homogeneity analysis quantifies the $n$ objects in such a way that the resulting stars are as small as possible. 

# The smacofHO Loss Function

The smacofHO technique solves the closely related problem in which we do not require,
as in homogeneity analysis, that 
\begin{subequations}
\begin{equation}
\sum_{l=1}^{k_j}g^j_{il}\hat d^j_{il}=0
\end{equation}
for all $i$ and $j$, but we impose the weaker condition that for all $i$ and $j$
\begin{equation}
\sum_{l=1}^{k_j}g^j_{il}\hat d^j_{il}\leq\hat d^j_{i\nu}
\end{equation}
\end{subequations}
for all $\nu=1,\cdots,k_j$.
In homogeneity analysis the geometric interpretation of loss is that we
want objects to coincide with all categories they score in. The geometric interpretation of loss function ... is that we want 
objects to be closer to the categories they score in than to the categories
they do not score in. 

This can be formalized using the notion of *Voronoi regions*. The Voronoi region of
category $l$ of variable $j$ is the polyhedral convex set of all points of $\mathbb{R}^p$ closer to category $l$ than to any other category of variable $j$. The plot of the the $k_j$ categories of variable $j$ defines $k_j$ Voronoi regions that
partition $\mathbb{R}^p$. For a wealth of information about Voronoi regions we refer to


Loss function ... with $\Delta$ defined by ... vanishes if for each variable all $x_i$ are in the Voronoi regions of the categories they score in. This condition implies, by the way, that the interiors of the $k_j$ convex hulls of the $x_i$ in a given category are disjoint, and the point clouds can consequently be weakly separated by hyperplanes. Since the category points themselves are in their own Voronoi region the convex hulls of the stars are also disjoint.

The general majorization theory for MDS with restrictions (@deleeuw_heiser_C_80)
calls for updates
in two steps. In the first step we compute the Guttman transform of the current configuration, and in the second step we project the Guttman transform on the
set of constrained configurations.

Configuration updates are alternated with updates of the $\hat D_j$. Initial: homals.

Minimizing loss ... over the $\hat d_i^j$ is a monotone regression problem for a simple tree
order. This is easily solved by using Kruskal's primary approach to ties
(@kruskal_64a, @kruskal_64b, @deleeuw_A_77).

## The Guttman Transform

The smacof iterations, or Guttman transforms, more or less ignore the fact that we are dealing with a rectangular matrix and use the weights to transform the problem into a symmetric one (as in @heiser_deleeuw_A_79).

The loss function is
$$
\sigma(Z_1,\cdots,Z_m)=\sum_{j=1}^m\sum_{i=1}^{n_j}\sum_{k=1}^{n_j}w_{ik}^j(\hat d_{ik}^j-d_{ik}(Z_j))^2,
$$
with $n_j:=n+k_j$ and with $Z_j$ the $n_j\times p$ matrices that stack $X$ on top of
$Y_j$. The $w_{ik}^j$ are zero for the diagonal $n\times n$ and the diagonal
$k_j\times k_j$ block.

To compute the Guttman transform of $Z_j$ we have to solve 
the partitioned system
\begin{equation}
\begin{bmatrix}
R_W&-W\\
-W'&C_W
\end{bmatrix}
\begin{bmatrix}\tilde X\\\tilde Y\end{bmatrix}=
\begin{bmatrix}
R_B&-B\\
-B'&C_B
\end{bmatrix}
\begin{bmatrix}X\\Y\end{bmatrix}
(\#eq:guttman)
\end{equation}
Since we have to solve this system for each variable separately we forget about the index $j$ here. In 
\@ref(eq:guttman) $R_W$ and $C_W$ are the diagonal matrices with row and column sums of $-W$, while $R_B$ and $C_B$ are diagonal matrices with the row and columns sums of the $n\times k_j$ matrix $B$, which has elements
\begin{equation}
b_{il}=w_{il}\frac{\hat d_{il}}{d(x_i,y_l)}.
(\#eq:bdef)
\end{equation}
Matrices $X$ and $Y$ are the two parts of the current $Z$ that we are updating,
while we solve for $\tilde X$ and $\tilde Y$, the two parts of the Guttman
transform. 

Define
\begin{equation}
\begin{bmatrix}
P\\Q
\end{bmatrix}:=
\begin{bmatrix}
R_B&-B\\
-B'&C_B
\end{bmatrix}
\begin{bmatrix}X\\Y\end{bmatrix}
(\#eq:gusolve)
\end{equation}
Now
$R_W\tilde X-W\tilde Y=P$ or $\tilde X=R_W^{-1}(P+W\tilde Y)$. Substitute this in $C_W\tilde Y-W'\tilde X=Q$
to get $C_W\tilde Y-W'R_W^{-1}(P+W\tilde Y)=Q$ or 
\begin{equation}
(C_W-W'R_W^{-1}W)\tilde Y=Q+W'R_W^{-1}P
(\#eq:solvefory)
\end{equation}
We solve equation \@ref(eq:solvefory) for $\tilde Y$ and then use $\tilde X=R_W^{-1}(P+W\tilde Y)$.
Note that $C_W-W'R_W^{-1}W$ is doubly-centered.
As in homogeneity analysis we hope that $k_\star$
is not to big, and we avoid generalized inverses of very large and very
sparse matrices.


## Nine analyses

There are three options for updating the $Y_j$. 

0. No further constraints on the $Y_j$.
1. The $Y_j$ are centroids, i.e. $Y_j=(G_j'W_jG_j)^{-1}G_j'W_jX$.
2. The $Y_j$ have rank one, i.e. $Y_j=q_ja_j'$.

There are two options for updating $X$. Note that $X$ is always
constrained to be the same for all variables, i.e. $X_j=X$.

0. No further constraints on $X$.
1. $X$ is normalized by $\text{tr}\ X'W_\star X=1$.
2. $X$ is normalized by $X'W_\star X=I$.

Note that the centroid constraint on the $Y_j$ and the normalization
constraint on $X$ are inspired by homogeneity analysis. The rank-one
constraint on $Y_j$ is taken from the Gifi system, where it serves
to make homogeneity analysis into a form of non-linear principal
component analysis. 

There are far more than nine possible analyses. For each of the
$m$ variables we can independently choose each of the three options for the
$Y_j$ and combine it with one of the three options for $X$, making a total
of $3^{m+1}$ different analyses (in a given dimensionality $p$).

## Categories unconstrained (yform = 0)

Suppose there are no constraints on the $Y_j$. 
After computing the Guttman transforms $\tilde X_j$ and $\tilde Y_j$  
we have to project them on the constrained configurations, in this
case on the set $X_j=X$. More explicitly we must minimize
\begin{multline}
\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'R_j(X-\tilde X_j)-2\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'W_j(Y_j-\tilde Y_j)+\\
\sum_{j=1}^m\text{tr}\ (Y_j-\tilde Y_j)'C_j(Y_j-\tilde Y_j)
\end{multline}
where $R_j$ and $C_j$ are now the diagonal matrices of row and column sums of the
$-W_j$.

The stationary equations are
\begin{align}
Y_j&=\tilde Y_j-C_j^{-1}W_j'(X-\tilde X_j),(\#eq:stateq1)\\
X&=\{R_\star\}^{-1}\sum_{j=1}^m\left\{R_j\tilde X_j-W_j(Y_j-\tilde Y_j)\right\}.(\#eq:stateq2)
\end{align}
We solve these equations iteratively using alternating least squares. This 
means using \@ref(eq:stateq1) to compute a new $Y$ for given $X$ and \@ref(eq:stateq2) to compute a new $X$ for given $Y$. We alternate these two updates until convergence.

### Normalization of X

We can impose the weak normalization constraint $\text{tr}\ X'R_\star X=1$ or the
strong normalization constraint $X'R_\star X=I$. In both cases the stationary
equation \@ref(eq:stateq1) remains the same, while \@ref(eq:stateq1) becomes
$$
R_\star X\Lambda=P,
$$
with
$$
P:=\sum_{j=1}^m\left\{R_j\tilde X_j-W_j(Y_j-\tilde Y_j)\right\}
$$
and with $\Lambda$ a matrix of Lagrange multipliers. For weak normalization $\Lambda$
is scalar, and
$$
X=\frac{1}{\text{tr}\ P'R_\star^{-1}P}R_\star^{-1}P
$$
For string normalization $\Lambda$ is symmetric. Using the symmetric square root, $\Lambda=(P'R_\star^{-1}P)^\frac12$ and thus 
$$
X=R_\star^{-1}P(P'R_\star^{-1}P)^{-\frac12}.
$$
Thus requiring normalized object scores only needs small modifications in the $X$ update step of the unnormalized update $R_\star^{-1}P$.

## Centroid Constraints on Y

If we require that $Y_j=(G_j'R_jG_j)^{-1}G_j'R_jX$ then this effectively eliminates the $Y_j$ as variables from the optimization problem and we only have to optimize over $X$.

\begin{equation}
\left\{\begin{bmatrix}
\tilde X_j\\
\tilde Y_j
\end{bmatrix}
-
\begin{bmatrix}
I\\
H_j
\end{bmatrix}X\right\}'
\begin{bmatrix}
R_j&-W_j\\
-W_j'&C_j
\end{bmatrix}
\left\{\begin{bmatrix}
\tilde X_j\\
\tilde Y_j
\end{bmatrix}
-
\begin{bmatrix}
I\\
H_j
\end{bmatrix}X\right\}
\end{equation}

where $H_j:=(G_j'R_jG_j)^{-1}G_j'R_j$. Thus $H_j$ is $k_j\times n$.

$$
X'\{R_j-H_j'R_j'-R_jH_j+H_j'C_jH_j\}X-2X'\{(R_j-H_j'W_j')\tilde X_j+(-R_j+H_j'C_j)\tilde Y_j\}
$$
\begin{subequations}
\begin{align}
P&:=\sum_{j=1}^m\{R_j-H_j'W_j'-W_jH_j+H_j'C_jH_j\},\\
Q&:=\sum_{j=1}^m\{(R_j-H_j'W_j')\tilde X_j+(-W_j+H_j'C_j)\tilde Y_j\}.
\end{align}
\end{subequations}

Un-normalized $X=P^+Q$. Too big. Majorization. $P\leq\lambda W_\star$

$$\text{tr}\ (\tilde X+(X-\tilde X))'P(\tilde X+(X-\tilde X))-2\text{tr}\ (\tilde X+(X-\tilde X))'Q$$
$$
\text{tr}\ (X-\tilde X)'P(X-\tilde X)-2\text{tr}\ (X-\tilde X)'Q\leq
\lambda\text{tr}\ (X-\tilde X)'R_\star(X-\tilde X)-2\text{tr}\ (X-\tilde X)'Q
$$
$$
\lambda W_\star(X-\tilde X)=Q
$$
$X=\tilde X+\lambda^{-1}R_\star^{-1}Q$

### Normalization

It is tempting to normalize using $X'PX$ but that would make comparison with
the results for unconstrained and rank-one $Y$ more complicated. Thus we continue
to use $X'W_\star X$ and use the majorization .. The stationary equations are
$$
(Q+\lambda R_\star\tilde X)=R_\star X\Lambda
$$
which we solve in the usual way for weak and strong normalizations.

## Rank Constraints on Y

As in homals it is possible in smacofHO to impose rank-one restrictions on some or all of the $Y_j$. This means we require
$$
y_l^j=\alpha_{l}^jy_l^{\ }{}
$$
Geometrically having all $y_l^j$ on a line through the origin implies that
all voronoi boundaries are hyperplanes perpendicular to that line, and consequently all voronoi regions are bounded by two parallel hyperplanes.

\begin{multline}
\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'R_j(X-\tilde X_j)-2\sum_{j=1}^m\text{tr}\ (X-\tilde X_j)'W_j(y_ja_j'-\tilde Y_j)+\\
\sum_{j=1}^m\text{tr}\ (y_ja_j'-\tilde Y_j)'C_j(y_ja_j'-\tilde Y_j)
\end{multline}

$$
-2a_j'\{(X-\tilde X_j)'W_j+\tilde Y_j'C_j\}y_j+a_j'a_jy_j'C_jy_j
$$

### Normalization

# Convergence and Degeneracy

# Utilities

## Object Plot Function

## Category Plots Function

## Joint Plot Function

## Prediction Table

In the solution $(X,Y)$ we say that pair $(i,j)$ is a *hit* if
$$
d_{il}^j(X,Y)=\min_{\nu=1}^{k_j}d_{i\nu}^j(X,Y)
$$
or, in words, if object point $x_i$ is in the Voronoi region of the category point
corresponding to the category the object scores in.

# Examples

Presenting examples runs into some difficulties. 

## Cetacea

## Senate

## GALO


# Generalizations

1. Fuzzy Indicators
2. Voronoi with general sites

# References
