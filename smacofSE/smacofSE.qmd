---
title: |
    | Smacof at 50: A Manual
    | Part zz: smacofSE: Initial Estimates
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
---

# Introduction

The best way to avoid unwanted (i.e. non-global) local minima in smacof is to start the iterations with an excellent initial configuration Even if the iterations converge to a non-global local minimum, at least we can be sure than the solution has a lower stress than the initial configuration, which was supposedly already excellent.

In this chapter we implement two fairly elaborate initial
estimates for metric smacof, which can of course also be 
used in the alternating least squares algorithm for non-linear
and non-metric smacof. Both initial estimates basically
iteratively minimize alternative loss functions, respectively called sstress and strain (see @deleeuw_heiser_C_82 for a
definition and comparison of these loss functions).

The R functions implementing the minimization of these
loss functions can be interpreted (and can actually be used) as
alternative MDS methods. It may seem somewhat peculiar to
start an iterative MDS technique with an initial estimate 
computed with another iterative MDS technique. But it is
not exactly unprecedented, since traditionally MDS programs
compute their initial configurations by using Classical
MDS (which uses iterative eigenvalue-eigenvector methods).

Our initial configuration techniques have a great deal of
flexibility, because we do not necessarily iterate to
convergence in the initial phase. We use the possibility
in monotone iterative algorithms to merely improve instead
of completely solve.

# Strain

In the standard versions of smacof we initialize by
using classical multidimensional scaling, i.e. the 
method proposed in @torgerson_58 and @gower_66.

Classical Scaling is based on the Euclidean embedding theorem of @schoenberg_35 and @young_householder_38. A real hollow symmetric matrix $\Delta$ of order $n$, with non-negative entries, is a Euclidean distance matrix if and only if the matrix
$B:=-\frac12 J\Delta^{(2)}J$ is positive semi-definite. 
Here $\Delta^{(2)}$ is the elementwise square of $\Delta$ and $J=I-n^{-1}ee'$ is the centering matrix. Moreover if $B$ is positive semi-definite then the embedding dimension is the rank of $B$. Classical scaling computes $B$ and its eigen-decomposition. It then
constructs the initial estimate $X$ for $p$-dimensional MDS by using the $p$% largest eigenvalues $\Lambda$ of $B$, and the corresponding eigenvectors in $K$,  by using
$X=K\Lambda^{-1/2}$.

Classical Scaling, as an MDS technique, has one major advantage over other forms
of MDS, and three rather annoying disadvantages. The advantage is that it minimizes
the strain loss function, defined as
$$
\sigma(X)=\text{tr}\ J(\Delta^{(2)}-D^{(2)}(X))J(\Delta^{(2)}-D^{(2)}(X))J)
$${#eq-strain}
with $D^{(2)}(X)$ the squared Euclidean distances. In this matrix form strain was first discussed by
by @deleeuw_heiser_C_82, although equivalent versions, using different notation, are already in @gower_66 and @mardia_78. Not only does Classical Scaling minimize strain,
but it actually computes its global minimum, and if $\lambda_k(B)>\lambda_{k+1}(B)$
that global minimum is unique (up to rotation and translation).

If $\Delta$ is Euclidean then @gower_66 shows that 
$$
d_{ij}^2(X_1)\leq d_{ij}^2(X_2)\leq\cdots\leq d_{ij}^2(X_r)=\delta_{ij}^2,
$$
where $X_p$ is the $p$-dimensional Classical Scaling 
solution and $r$ is the rank of $B$. Thus squared dissimilarities are approximated from below (see also @deleeuw_meulman_C_86). 

The first disadvantage of Classical Scaling is that the $B$-matrix may only
have $q<p$ positive eigenvalues. If that is the case the global minimum of strain
has only $q$ non-zero dimensions (and $p-q$ zero dimensions).
In practice this disadvantage
is usually not a very serious one, because having fewer than $p$ positive
eigenvalues suggests a horrendously bad fit so that maybe MDS is not a good technique choice
in the first place. Alternatively, and more optimistically, one can also compute an additive constant to force positive semi-definiteness of $B$ (@cailliez_83).

The second disadvantage is that classical scaling, unlike smacof, has no provision to include data weights $w_{ij}$ for each of the dissimilarities $\delta_{ij}$. In particular this means that there cannot be missing dissimilarities, or, more precisely,
something additional has to be done if there are missing data. One obvious
possibility is to use alternating least squares to minimize strain, alternating 
a step to impute the missing dissimilarities and a step to compute the
optimal configuration. The current smacof program in R imputes the
missing dissimilarities by replacing them with the average dissimilarity,
which is computationally inexpensive but not very satisfactory. This second
disadvantage also means there is not straightforward version of Classical
Scaling for Multidimensional Unfolding, in which all within-set dissimilarities
are missing.

And finally the squaring and double-centering of the dissimilarities may not
be such a good idea from the statistical point of view. Squaring will
emphasize large errors, and can easily lead to outliers. Double-centering
introduces dependencies between different observations, because the
pseudo scalar products in $B$ are linear combinations of multiple
(squared) dissimilarities.

Nevertheless the smacof project includes a version of Classical MDS
that can handle missing data (but not general weights). A similar non-metric version of strain was discussed by @trosset_98.

We formulate loss as
$$
\sigma(X,\theta)=\text{tr}\ (B(\theta)-XX')^2
$${#eq-strainmissing}
where
$$
B(\theta)=-\frac12J(\Delta_0^2+\sum \theta_kE_k)J
$${#eq-btheta}
Here $\Delta_0^2$ is the matrix with dissimilarities, with elements equal to zero for missing data.
The $E_k$ code missing data, and are for the form $e_ie_j'+e_je_i'$. Thus they are zero, except for
elements $(i,j)$ and $(j,i)$, which are one. Strain ... must be minimized over configurations
$X$ and over $\theta\geq 0$.

We use alternating least squares. Minimizing over $X$ for fixed $\theta$ is classical MDS, minimizing
over $\theta$ for fixed $X$ is a non-negative least squares problem. In an iteration the function
smacofTorgersonWithMissing() does not actually minimize over $X$ for given $\theta$, but it uses
majorization to make a number of steps in the right direction. The same applies to the non-negative
least squares problem of minimizing over $\theta$ for given $X$.

If there are no missing data the program just performs classical MDS.

# Sstress

In some early reports (@deleeuw_R_68e, @deleeuw_R_68g) I have proposed using 
what I call the "maximum sum principle", not just for metric MDS, but for various 
other metric and nonmetric techniques as well. Around the same time guttman_68 proposed a 
similar initial configuration. In metric MDS the idea is to maximize
$$
\rho(X):=\sum w_{ij}\delta_{ij}^2d_{ij}^2(X)
$$
over some compact set of configurations. General considerations suggest that $\rho$ will tend to be
large if $\Delta$ and $D(X)$ are numerically similar, or similarly ordered.

Now $\rho(X)=\text{tr}\ X'BX$ where
$B$ is now defined as
$$
B:=\sum w_{ij}\delta_{ij}^2A_{ij},
$$
i.e. $B$ has off-diagonal elements $-w_{ij}\delta_{ij}^2$, with the diagonal filled in such that rows and columns add up to zero.
It follows that $B$ is symmetric, double-centered, and positive semi-definite. 

No matter how attractive $\rho$ is, we still have to decide how to bound $X$ and how to introduce multi-dimensionality. A naive 
choice would be $\text{tr}\ X'X=1$,  but that gives a solution $X$ of rank one with all
columns equal to the eigenvector corresponding with the largest eigenvalue of $B$.
@deleeuw_R_70b seems to suggest maximizing $\rho$ over $X'X=I$, which leads 
to choosing $X$ as the eigenvectors corresponding with the $p$ largest
eigenvalues of $B$. But that result is of limited usefulness, because the
MDS problem does not specify anywhere that the configuration $X$ must be 
orthonormal.

Guttman says his initial configuration maximizes 
$$
\lambda:=\sum_{s=1}^p\frac{x_s'Bx_s}{x_s'x_s}
$$
But that cannot be correct. In the first place it would mean that the $x_s$ can be scaled independently and arbitrarily, in the second place the maximum of $\lambda$ is just $p$ times the largest eigenvalue of $B$ and all $x_s$ are proportional to the corresponding 
eigenvector. Thus Guttman's derivation is wrong. 

Both De Leeuw and Guttman seem to arrive, in mysterious ways, at the "solution" $X=K\Lambda^\frac12$, where $K$ and $\Lambda$
are the $p$ dominant eigenvectors and eigenvalues of $B$. 

# Sstress

The maximum sum initial configuration is related to the problem of minimizing the MDS loss function sstress, defined by @takane_young_deleeuw_A_77, as
$$
\sigma(X)=\sum w_{ij}(\delta_{ij}^2-d_{ij}^2(X))^2.
$$
If we expand ... we find
$$
\sigma(X)=\sum w_{ij}(\delta_{ij}^2-d_{ij}^2(X))^2=K-2\rho(X)+\sum w_{ij}d_{ij}^4(X).
$$
By using the homogeneity of the distance function we can show that minimizing sstress is equivalent to maximizing $\rho(X)$ of ... over all $X$ in the compact set $\sum w_{ij}d_{ij}^4(X)=1$.  But, unlike the maximum sum approach, this does not lead to a simple eigenvalue-eigenvector problem. It can be solved, however, by iteratively solving a sequence of related eigenvalue-eigenvector 
problems. @deleeuw_groenen_pietersz_E_16m


$$
\sigma(X)=\sum w_{ij}(\delta_{ij}-d_{ij}(X))^2=
\sum \frac{w_{ij}}{(\delta_{ij}+d_{ij}(X))^2}
(\delta_{ij}^2-d_{ij}^2(X))^2\approx\frac14\sum \frac{w_{ij}}{\delta_{ij}^2}
(\delta_{ij}^2-d_{ij}^2(X))^2
$$
Now consider the problem of minimizing sstress. 
$$
\sigma(X)=\sum w_{ij}(\delta_{ij}^2-d_{ij}^2(X))^2.
$$
Define $A_{ij}$ as usual, $C=XX'$, $a_{ij}=\text{vec}(A_{ij})$, an d $c=\text{vec}(C)$. Then $d_{ij}^2(X)=\text{tr}\ A_{ij}C=a_{ij}'c$, and thus
$$
\sigma(C)=K-2\text{tr}\ BC+c'Vc
$$
$$
B=\sum w_{ij}\delta_{ij}^2A_{ij}
$$
$$
V=\sum w_{ij}a_{ij}a_{ij}'
$$
Let $\lambda$ the largest eigenvalue of $V$. Now write $C=\tilde C+(C-\tilde C)$.
Then
$$
\sigma(C)\leq\sigma(\tilde C)-2\text{tr}\ (B-\tilde CV)(C-\tilde C)+\lambda_+\text{tr}\ (C-\tilde C)^2
$$
Thus we can majorize by minimizing
$$
\text{tr}\ (C-(\tilde C-\lambda_+^{-1}(B-\tilde CV)))^2
$$
over $C=XX'$.

The largest eigenvalue of $V$ is also the largest eigenvalue of
the matrix with elements $A'WA$, which is a non-negative matrix.


# Part 2

$$
\sigma(X)=\text{tr}\ (C-XX')^2
$$
to be minimized over $n\times p$ matrices $X$.

Use the short SVD $X=K\Lambda L'$ and minimize over $K'K=I$, 
$L'L=LL=I$, and $\Lambda$ diagonal.
$$
\sigma(K,L,\Lambda)=\text{tr}\ (C-K\Lambda^2 K')^2=\text{tr}\ C^2-2\text{tr}\ \Lambda^2K'CK+\text{tr}\ \Lambda^4.
$$
The minimum over $\Lambda^2$ is attained for $\Lambda^2=\text{diag}\ K'CK$.

Now maximize $\text{tr}\ \Lambda^2K'CK$ over $K'K=I$. If $C\gtrsim 0$ this
is convex in $K$, and thus we have the minorization at $U'U=I$
$$
\text{tr}\ \Lambda^2K'CK\geq\text{tr}\ \Lambda^2U'CU+2\text{tr}\ \Lambda^2U'C(K-U)
$$
Thus it suffices to increase $\text{tr}\ K'CU\Lambda^2$, which we can
actually maximize by orthogonal Procrustus and increase by Gram-Schmidt
same reasoning as in homals).

# References
